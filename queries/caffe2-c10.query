hello! could you please help me write a crate-description for the rust crate "caffe2-c10" containing the following symbols? not all of the symbols are defined in this crate, but they are used somehow (please do not explicitly list the symbols, or write anything other than a simple description.  no description header is necessary. however, please be descriptive and as thorough as possible without making too many assumptions) :EXPECT
Error
NDEBUG
THROW
ALREADY
Attempts
C
CHECK
CreateDirectoryA
Creates
Default
Don
Drop
ERROR
EXISTS
Error
GetLastError
Like
NULL
On
Option
RemoveDirectoryA
Returns
Self
String
TEMP
TEMPDIR
TMP
TMPDIR
TORCH
TempDir
TempFile
The
WIN
We
Windows
XXXXXX
kRandomPattern
Default
DefaultTensorOptions
Defined
Device
Layout
Like
Make
TensorOptions
TypeMeta
kCPU
kStrided
Debug
Default
EXPECT
Error
LeftRight
MyException
THROW
TRUE
We
A
ADInplaceOrView
ALIASES
APIs
ATen
AUTOCAST
AUTOGRAD
Alias
Alisa
All
Although
At
AutoDispatchBelowADInplaceOrView
Autocast
AutocastCPU
AutocastCUDA
Autocasting
Autograd
AutogradCPU
AutogradCUDA
AutogradHPU
AutogradMLC
AutogradNestedTensor
AutogradOther
AutogradPrivateUse
AutogradXLA
AutogradXPU
BACKENDS
BC
Backend
BackendSelect
Batched
BatchedTensorImpl
But
CATCHALL
COMPOSITE
CPU
CPUGeneratorImpl
CPUTensorId
CUDA
CUDAGeneratorImpl
CUDATensorId
Caffe
CatchAll
Check
Clone
CompositeExplicitAutograd
CompositeImplicitAutograd
Conjugate
Copy
Cuda
Currently
CustomRNGKeyId
DISPATCH
DefaultBackend
Define
DeviceType
DifferentiableViewMeta
Dispatch
DispatchKey
DispatchKeySet
DispatchKeys
Display
Don
Dream
Due
EXPLICIT
EndOfAliasKeys
EndOfBackendKeys
Eq
FIN
FPGA
For
Formatter
FuncTorchBatched
FuncTorchDynamicLayerBackMode
FuncTorchDynamicLayerFrontMode
FuncTorchGradWrapper
FuncTorchPython
FuncTorchVmapMode
Generator
GenericMode
GenericWrapper
H
HIP
HPU
HTTP
Habana
Hash
Hasher
Here
Higher
ID
IDs
If
In
Intel
Into
It
K
KEY
Keep
Key
Keys
MKLDNN
MLC
MLCompute
MSNPU
Masquerading
Meta
Metal
MkldnnCPU
NB
NOT
NOTE
Named
NestedTensor
Note
NumDispatchKeys
ONLY
Ord
Out
PR
PRIVATEUSE
PRNG
PartialEq
PartialOrd
PreAutograd
Private
PrivateUse
PyTorch
QuantizedCPU
QuantizedCUDA
QuantizedXPU
RNGs
RealDispatchKey
RegisterCPU
RegisterCUDA
RegisterCompositeExplicitAutograd
RegisterCompositeImplicitAutograd
RegisterMkldnnCPU
RegisterQuantizedCPU
RegisterQuantizedCUDA
RegisterSparseCPU
RegisterSparseCUDA
Registering
Result
See
Semantically
Sentinel
Similar
SparseCPU
SparseCUDA
SparseCsrCPU
SparseCsrCUDA
SparseHIP
SparseXPU
TENSOR
TESTING
TLS
TODO
TYPE
TensorImpl
The
There
These
They
This
Thus
To
TorchTensor
Torchdispatch
TorchkCPU
Tracer
Traditional
UNDEFINED
UNKNOWN
Undefined
Use
Variable
VariableType
VariableTypeId
VmapMode
Vulkan
WRAPPERS
We
When
XLA
XPU
Xilinx
You
ZERO
`AutoDispatchBelowADInplaceOrView`
`DispatchKey`
`VariableType`
toString
~~~~~~~UNDEFINED
Arc
CHECK
COMMS
DEBUG
DebugInfoBaseInterface
DebugInfoGuard
DebugInfoKind
Default
Drop
Expected
INFERENCE
INFO
Internal
MOBILE
Nested
PARAM
PRODUCER
PROFILER
Peek
Pop
Push
RUNTIME
STATE
Self
TEST
TLS
TORCH
Thread
ThreadLocalDebugInfo
ThreadLocalStateGuard
Used
Users
Drop
Helper
Here
In
Index
IndexMut
IntoIter
IntoIterator
Item
Iterator
No
None
Option
Output
Pod
SBO
Self
SmallBuffer
SmallBufferIterator
SmallBufferMut
SmallVector
Some
This
`IntoIterator`
`SmallBufferMut`
`SmallBuffer`
Ignoring
Issues
NB
Option
Reads
TORCH
WARN
BFloat
Explicit
Implement
Note
Sign
A
AArch
ALIGNMENT
AND
ANDROID
ARM
ASAN
AVX
AllocError
Allocator
AllocatorRegisterer
Arc
C
CHECK
CPU
CPUAllocator
CPUCachingAllocator
Caching
Cannot
DataPtr
Default
DefaultCPUAllocator
DefaultMobileCPUAllocator
Delete
DeleterFnPtr
Device
DeviceType
EVERY
Error
FLAGS
Fill
G
Get
GetAllocator
GetCurrentNUMANode
GetDefaultMobileCPUAllocator
GetThreadLocalAllocationPlanner
GetThreadLocalCachingAllocator
GetThreadLocalProfilingAllocator
Global
HashMap
Here
INFO
LOG
Layout
Let
MOBILE
MS
MSC
Making
Malloc
Memory
MemoryDeleter
Mobile
NEON
NOLINTNEXTLINE
NUMA
NUMAMove
NaN
New
NonNull
Number
Option
Post
PostGuardBytes
Pre
PreGuardBytes
ProfiledCPUMemoryReporter
QNNPACK
RawMutex
ReportAndDelete
Result
SIMD
Set
SetAllocator
Sets
StaticRuntime
THROW
TLS
TODO
The
There
This
UNLIKELY
Use
VER
VLOG
WARNING
We
XNNPACK
gAlignment
kJunkPattern
memoryProfilingEnabled
profiledCPUMemoryReporter
reportMemoryUsageToProfiler
ASSERT
Active
C
CHECK
CUDA
CUDAGraph
CUDAGraphsC
CUDAGraphsUtils
CaptureId
CaptureStatus
Cuda
CudaStreamCaptureMode
CudaStreamCaptureModeGuard
Display
Drop
Formatter
GTE
Graphs
INTERNAL
Invalidated
MempoolId
None
Protects
RAII
Result
Self
Some
TORCH
Unknown
Use
Utils
VERSION
WARN
cudaStreamCaptureMode
cudaStreamCaptureStatus
cudaStreamCaptureStatusActive
cudaStreamCaptureStatusInvalidated
cudaStreamCaptureStatusNone
cudaStreamIsCapturing
cudaThreadExchangeStreamCaptureMode
getCurrentCudaStream
ARCH
ASSERT
C
CPU
CUDA
CUDACC
CUDAMathCompat
Cuda
DECL
DEVICE
HIP
HIPCC
ICE
INTERNAL
Jetson
MATH
PR
PyTorch
RTC
Segfaults
TORCH
This
ANDROID
API
APPLE
Action
Also
AtomicBool
Block
C
COND
Cannot
Check
Constructor
Currently
DFL
DIR
DISABLE
Defaults
Drop
FATAL
Failed
FatalSignalHandler
Flag
Global
GotSIGHUP
GotSIGINT
HANDLER
HANDLERS
INITIALIZER
If
Installs
Intercept
LOG
Leaky
Linux
MUTEX
Mutex
NONE
NR
Normal
ONSTACK
Our
PID
PTHREAD
PThreadCond
PThreadMutex
Previous
RESTART
Restart
Return
SA
SIG
SIGABRT
SIGBUS
SIGFPE
SIGHUP
SIGILL
SIGINFO
SIGINT
SIGNAL
SIGSEGV
SIGUSR
STOP
SUPPORTS
SYS
Self
Set
Setup
SigAction
SignalHandler
SignalHandlerAction
Since
Specify
TODO
The
This
Thread
UNKNOWN
We
callPreviousSignalHandler
currentTid
fatalSignalHandler
fatalSignalHandlerPostProcess
fatalSignalHandlerStatic
fatalSignalHandlersInstallationMutex
fatalSignalHandlersInstalled
fatalSignalName
fatalSignalReceived
fatalSignum
getInstance
getPreviousSigaction
getSignalName
handleSignal
hookedUpCount
hookupHandler
installFatalSignalHandlers
kSignalHandlers
needsLock
previousSighup
previousSigint
previousSigusr
procDir
sighupCount
sigintCount
stacktraceSignalHandler
stacktraceSignalHandlerStatic
unhookHandler
uninstallFatalSignalHandlers
writingCond
writingMutex
CHECK
Engine
FBGEMM
K
KFBGEMM
KQNNPACK
Keep
NO
NoQEngine
QENGINE
QEngine
QNNPACK
Quantized
String
TORCH
Unrecognized
kFBGEMM
kNoQEngine
kQNNPACK
A
APIs
ASSERT
An
Are
B
Because
C
CHECK
Caffe
Concat
Construct
Cuda
CudaStream
DEFAULT
Device
DeviceIndex
DeviceType
Display
Enqueues
Every
Feel
Formatter
H
HIP
HOWEVER
Hash
Hasher
Hazard
INTERNAL
If
In
Just
Known
Make
NB
NOT
Not
Note
OK
Of
OpenCL
PartialEq
Python
Re
Remove
Result
Return
See
Self
Stream
StreamDefault
StreamGuard
StreamId
StreamIds
StreamUnsafe
Streams
TORCH
The
There
This
UNSAFE
Unfortunately
Unsafely
VirtualGuardImpl
Wait
Without
`StreamIndex`
cudaStream
hipStream
isValidDeviceType
queryStream
synchronizeStream
xFFFFFFFFFFFFull
xFFull
A
ATen
Acquire
Box
By
C
CHECK
CPU
CPUGeneratorImpl
Clone
CloneImpl
Common
Cuda
CurrentSeed
Currently
DEFAULT
Device
DispatchKeySet
FFFFFFFFFFFFF
FIXME
Figure
For
Generator
GeneratorImpl
GeneratorImplAdapter
GeneratorImplInterface
Generators
GetState
Gets
LinkedListLink
Look
NOLINTNEXTLINE
NOT
Note
Number
O
Often
Option
PRNG
Please
Pseudo
PyObject
RDONLY
RNG
Random
RawMutex
SEED
See
Seed
Self
SetCurrentSeed
SetState
Such
THCRandom
THRandom
TODO
TORCH
TensorImpl
TensorImplAdapter
The
Unable
VAL
WIN
Windows
You
randDev
randValue
readBytes
readURandomLong
ASSERT
Check
CheckPythonGil
DEP
DO
DeadlockDetection
Drop
For
GIL
Holding
INTERNAL
If
NO
NOT
PYTHON
Please
PyGILState
PyTorch
Python
PythonGILHooks
PythonGILHooksInterface
PythonGILHooksRegisterer
Returns
Self
SetPythonGILHooks
TORCH
The
This
WITHOUT
You
ANDROID
API
APIs
Android
C
Convoluted
Currently
Define
GLIBCXX
It
Just
LT
NDK
TODO
U
Using
Various
ADDRESS
ALWAYS
AND
ANDROID
ANONYMOUS
APPLE
ARCH
ASAN
ASSERT
ASSIGN
ATTRIBUTE
Ampere
Annotate
As
BLOCK
BLOCKS
BOUNDS
Buck
C
CMake
CONCATENATE
CONSTEXPR
COPY
COPYABLE
COUNTER
CPP
CPU
CRT
CUDA
CUDACC
CUSTOM
Clang
Cuda
Currently
DEVICE
DISABLE
Designates
Detect
Disable
Dzd
ENABLED
EXCEPT
EXPAND
EXTERNAL
Eigen
Exhibits
FALLBACK
FALLTHROUGH
FILE
FUNCTION
File
For
Functions
GCC
GENERATED
GNUC
GNUG
GPU
Geforce
HAS
HCC
HIP
HIPCC
HOST
Hence
However
ICL
IMPL
IMPORT
INLINE
IOS
IPHONE
IS
In
Instead
It
KERNEL
LAUNCH
LIKELY
LINE
Line
Loops
LossCTC
MACRO
MACROS
MAX
MIN
MOBILE
MSC
MSVC
Macros
Main
Message
NB
NDEBUG
NOINLINE
NOT
NOTE
Not
Note
ONLY
OS
PER
PLATFORM
POD
PR
PYL
Portable
ROCM
Reduce
Release
SANITIZE
SIMULATOR
SIZE
SM
STRINGIZE
SYCL
Some
Suppose
TARGET
THREADS
TODO
TRIVIALLY
Technically
Test
The
These
This
Those
Turing
UID
UNLIKELY
USING
Unfortunately
VAR
VARIABLE
VER
VERSION
WARP
WIDE
WIN
Warn
Warning
We
Windows
Workaround
You
Z
tLMQS
ARCH
Allow
C
CONSTEXPR
CUDA
CUDACC
Cuda
Disallow
Display
Eq
FUNCSIG
FUNCTION
Formatter
GCC
GNUC
H
Hash
Hasher
ID
Idea
Invalid
MSC
Make
NVCC
Option
Ord
Ordering
PRETTY
PartialEq
PartialOrd
Result
SUPPORTS
Self
Some
TODO
TYPENAME
There
To
TypeIndex
VER
We
Windows
typeId
underlyingId
Backend
CHECK
CSR
Default
Display
Formatter
K
Layout
MKLDNN
Mkldnn
MkldnnCPU
NumOptions
Result
SPARSE
STRIDED
Sparse
SparseCPU
SparseCUDA
SparseCsr
SparseCsrCPU
SparseCsrCUDA
SparseHIP
SparseXPU
Strided
TORCH
Unknown
kMkldnn
kSparse
kSparseCsr
kStrided
AND
LEAN
MEAN
NOCLIPBOARD
NOGDI
NOKERNEL
NOMB
NOMCX
NOMINMAX
NOMSG
NOSERVICE
NOSOUND
NOUSER
WIN
A
API
Allocator
B
Box
C
DLManagedTensor
Default
Deref
How
Implementing
In
It
Lifetime
Note
Self
So
Some
Target
The
Under
UniqueVoidPtr
deleteNothing
Add
After
Allocate
Allocation
AllocationPlan
AllocationPlanner
Also
Box
CHECK
CPUProfilingAllocator
Check
Construct
Drop
Either
Enf
Eq
Erase
EventType
Example
Find
First
FlatHashMap
Free
Freeing
From
Given
Got
Greedily
HashMap
If
Including
Insert
Invalid
Iterate
Lifetime
Map
Maps
MemBlock
MemEvent
Memory
Merge
Merging
NB
NOLINTNEXTLINE
NOT
Nesting
Number
Option
Ord
Ordering
PartialEq
PartialOrd
Profile
ProfilingAllocator
Recorded
Records
Remove
SAFE
Second
See
Self
Similarly
Skip
Some
Sort
Step
Such
THREAD
TORCH
Tensor
This
Thus
Total
Upon
Usage
Validate
WARN
When
WithProfileAllocationGuard
WithProfileAllocationsGuard
WithProfilingAllocatorGuard
WithValidateAllocationPlanGuard
Y
As
AtomicBool
C
CaffeTaskThread
Check
Condvar
Copy
Decrement
Deliberately
Destruct
Drop
ERROR
Entry
Exception
If
InThreadPool
Increment
LOG
M
NUMABind
Need
No
NumAvailable
Option
RawMutex
RefCell
Run
SegQueue
Self
Set
Size
Task
TaskThreadPool
TaskThreadPoolBase
TaskThreadPoolBaseInterface
The
This
Thread
ThreadPool
ThreadPoolRegistry
ThreadPoolTaskElement
Update
Wait
defaultNumThreads
setThreadName
ASSERT
Comparison
Constructor
DispatchKey
DispatchKeySet
EXPECT
Error
FALSE
FULL
Increment
LT
NumDispatchKeys
Ops
THROW
TRUE
Undefined
highestPriorityTypeId
ABI
ANDROID
APPLE
B
C
DEMANGLE
Demangling
EMSCRIPTEN
GXX
HAS
IPHONE
Itanium
More
NOLINTNEXTLINE
NOTE
OS
RTTI
Returns
SIMULATOR
String
TARGET
This
Type
Utility
WIN
Z
A
ASSERT
An
Are
C
CHECK
COMPILE
CPU
CUDA
Check
Compiler
Constructs
Could
Cuda
DEBUG
DEVICE
DFS
Default
Device
DeviceIndex
DeviceType
DeviceTypeName
Display
Eq
Expected
FPGA
Formatter
From
Further
GLIBCXX
GPU
H
HIP
HPU
Hash
Hasher
Hazard
IDEEP
INTERNAL
If
Invalid
LANG
LIMIT
MAX
MKLDNN
MLC
MSB
MSNPU
MSVC
Make
Meta
Note
ONLY
OPENCL
OPENGL
Option
PartialEq
QUANTIFIERS
REGEX
RELEASE
Removing
Represents
Result
Return
Returns
STATE
Same
Self
Sets
String
TIME
TORCH
TYPES
Technically
Test
The
This
Type
Vulkan
We
When
XLA
XPU
Z
`DeviceType`
`Device`
zA
CRC
ConstexprCrc
Crc
Jones
MyTestString
These
ASSERT
ATen
All
Allows
Although
Ambiguous
Below
Best
By
C
CHECK
CONTIGUOUS
Channels
ChannelsLast
Contiguous
Display
Due
Ensure
FORMAT
Fallback
Formatter
H
HC
Hardcoded
Helper
INTERNAL
If
It
LEGACY
Last
MEMORY
Many
Memory
MemoryFormat
NC
NCHW
NOT
NOTE
No
Note
Please
Possible
Preserve
PyTorch
Regardless
Result
So
TODO
TORCH
Tensor
That
The
This
Two
Unknown
W
WAR
We
Without
CHECK
DEFAULT
DeviceIndex
DeviceType
DeviceTypeName
Drop
Event
EventFlag
InlineEvent
Move
Option
PYTORCH
Self
Stream
TORCH
destroyEvent
queryEvent
A
Activate
As
C
CPU
Command
CommandLineOpts
CopyFrom
Debug
Equivalent
If
MAX
NOT
NUMA
Resize
ResizeLike
ShrinkTo
Since
StructOpt
Tensor
The
This
Use
WARNING
BITS
Before
BitScanForward
BitSet
Bitset
BitsetType
Call
Debug
Eq
Func
IX
It
M
MSC
NUM
PartialEq
Return
Self
The
This
VER
You
ACCESSOR
BFloat
Bool
C
CHECK
CTOR
Complex
ComplexDouble
ComplexFloat
Complexf
DEFINE
Default
Double
HAS
IMPLICIT
Int
It
Long
MSVC
Neg
Output
PrimInt
Real
Scalar
ScalarTag
ScalarType
ScalarValue
Self
Specialized
Support
SymbolicVariable
TO
TODO
TORCH
Tag
Tensor
Unknown
Unlike
Value
We
includeBool
isBoolean
isComplex
isFloatingPoint
isIntegral
A
API
ASSERT
Also
Although
At
Atomic
Avoid
Because
Better
But
C
CHECK
COMPILE
CUDA
CUDAGuard
Check
Confused?
Construct
Could
Creates
Cuda
CudaStream
CudaStreamGuard
CudaStreamUnchecked
CudaStreams
Currently
DEFAULT
Default
Device
DeviceGuard
DeviceIndex
DeviceType
Did
Display
Don
Drop
EXT
Eq
Explicit
Externally
Formatter
GPU
GPUS
GPUs
Get
Global
HIGH
HIP
Hazard
Helper
How
However
Hypothetically
ID
IDs
IDs?
INTERNAL
Id
IdType
If
Implicit
In
Increase
Init
Initializes
Inits
Internal
It
LOW
LeakyStreamInternals
MAX
MSB
Multiple
NB
Non
Note
Number
OS
POD
PartialEq
Populates
PyTorch
Result
Return
Returns
Reversibly
See
Self
Set
Stream
StreamGuard
StreamId
StreamIdType
Streams
Switches
TIME
TORCH
That
The
There
These
This
Thread
UB
UNCHECKED
UNSAFE
Unchecked
Unexpected
Unpack
Unrecognized
Value
Warning
We
When
Where
Yes
You
cuStream
cuStreams
cudaDeviceGetStreamPriorityRange
cudaError
cudaErrorNotReady
cudaStream
cudaStreamCreateWithPriority
cudaStreamDestroy
cudaStreamGetPriority
cudaStreamNonBlocking
cudaStreamQuery
cudaStreamSynchronize
cudaSuccess
fromInternals
getStreamFromPool
getStreamId
initCudaStreamsOnce
initDeviceStreamState
initGlobalStreamState
isHighPriority
kDefaultFlags
kHighPriority
kLowPriority
kStreamTypeBits
kStreamsPerPool
kStreamsPerPoolBits
makeStreamId
setCurrentCudaStream
streamIdIndex
streamIdType
A
ANONYMOUS
API
ARGS
Args
C
CHECK
CLASS
CREATOR
DEBUG
DECLARE
DEFAULT
DEFINE
DefaultCreator
DerivedType
EXPORT
FALLBACK
FILE
HashMap
Higher
However
IMPORT
IN
Key
KeyStrRepr
KeyType
Make
NB
Note
ONCE
ONLY
ObjectPtrType
ObjectType
Option
Overwriting
PREFERRED
PRIORITY
PtrType
REGISTER
REGISTRY
RawMutex
Register
Registerer
Registry
RegistryCreator
RegistryName
RegistryPriority
Returns
SOURCE
Same
Self
Semantically
Simple
SrcType
String
TYPED
The
This
Used
VA
VARIABLE
WARNING
WITH
WITHOUT
Windows
Yangqing
You
BFloat
BinaryToFloat
EXPECT
Exponent
F
FFF
FFFFF
FLOAT
GetParam
LE
Mantissa
NOLINTNEXTLINE
S
TRUE
TestParam
The
This
inInf
inNaN
xFF
ANONYMOUS
ARGS
ATen
C
CHECK
COMPILE
CopyBytes
CopyBytesFunction
CopyBytesFunctionRegisterer
Cuda
DEVICE
Device
DeviceType
DeviceTypeName
Don
Duplicate
First
Implementations
MAX
No
Self
TIME
TYPES
VA
VARIABLE
WARNING
We
fromType
toType
ABI
ADDRESS
API
Address
As
BACKTRACE
Backtrace
CXX
CaptureStackBackTrace
DEFERRED
DWORD
Default
Drop
ELF
EX
FBCODE
FLAG
FROM
FileName
For
FrameInformation
GET
GLIBCXX
Get
GetCurrentProcess
GetModuleFileNameA
GetModuleHandleEx
HANDLE
HMODULE
IMAGEHLP
INFO
If
In
Initialize
LEN
LIBCPP
LIBCXX
LINE
LOADS
LPCTSTR
LineNumber
MAX
MODULE
MSC
MaxNameLen
NAME
NOTE
NULL
Name
Option
PSYMBOL
REFCOUNT
Reference
SUPPORTS
SYM
SYMBOL
SYMOPT
Self
SizeOfStruct
Skip
Some
StackTrace
String
Support
SymCleanup
SymFromAddr
SymGetLineFromAddr
SymGetOptions
SymInitialize
SymSetOptions
SymbolHelper
TCHAR
TODO
TRUE
The
They
This
Toggles
ULONG
UNCHANGED
USHORT
Unfortunately
Unknown
VER
VERSION
WIN
We
When
Windows
Z
`CaptureStackBackTrace`
`SymFromAddr`
`SymGetLineFromAddr
getInstance
toString
A
ATen
AutogradCPU
AutogradCUDA
AutogradHPU
AutogradMLC
AutogradXLA
AutogradXPU
BACKEND
Backend
CHECK
CPU
CUDA
CUDNN
Cuda
DeviceType
DispatchKey
FPGA
From
HIP
HPU
ID
Into
MKL
MLC
MSNPU
Metal
MkldnnCPU
NB
NOT
NumOptions
QuantizedCPU
QuantizedCUDA
QuantizedXPU
Self
SparseCPU
SparseCUDA
SparseCsrCPU
SparseCsrCUDA
SparseHIP
SparseXLA
SparseXPU
String
TORCH
The
This
Type
UNKNOWN
Undefined
Unknown
Unrecognized
Vulkan
XLA
XPU
CLANGTIDY
Callable
Drop
Engaged
ExitFunction
F
False
Fn
Fp
Interface
Keeps
Mostly
Rhs
ScopeExit
Self
Avoid
Big
Can
EXPECT
Finally
Give
Grow
Option
SizesAndStrides
Small
Wall
Wself
bigCopyFrom
bigMoveFrom
bigTarget
checkBig
checkData
checkSmall
copiedBig
copiedEmpty
copiedSmall
makeBig
makeSmall
movedBig
movedEmpty
movedSmall
selfMove
smallCopyFrom
smallMoveFrom
smallTarget
ANY
ASSERT
But
CUDA
Cuda
Device
DeviceIndex
DeviceType
EXPECT
FakeGuardImpl
HIP
If
Index
InlineDeviceGuard
InlineOptionalDeviceGuard
MaybeTestGuard
Optional
THROW
Test
TestDeviceType
TestGuard
TestGuardImpl
getDeviceIndex
setDeviceIndex
ALL
ALUs
AND
ASSERT
AT
But
C
CASE
CAST
CHECK
COMPLEX
CPU
CUDA
Cast
Casting
ComplexFloat
Converting
D
DEFINE
DISPATCH
Dest
Dynamic
ERROR
EXCEPT
Especially
FETCH
FORALL
Fetch
For
From
Further
GPU
HALF
IS
If
IsComplex
KERNEL
MaybeReal
Most
NOTE
Note
NumPy
Partial
PyTorch
Remove
SCALAR
STORE
ScalarType
Some
Src
TODO
TORCH
TYPES
To
Trigger
Type
TypeCast
UNCASTABLE
UNSUPPORTED
Unexpected
WITH
Base
Checks
Concatenates
Condition
Convenience
Counts
Creates
Default
Direct
Enable
Error
Example
Examples
Filters
Find
Func
Head
HeadTypes
In
Index
IndexSequence
Indices
It
Items
Like
List
Mapper
Maps
MyClass
Returns
Reverses
Shave
Successful
TODO
Tail
TailLists
Take
Transforms
Tried
Ts
Tuple
Type
TypeList
TypeLists
Types
`Ts
AFFINE
ATen
CHANNEL
CHECK
COMPILE
FLOAT
K
Keep
NUM
PER
Please
QPARAMS
QSCHEMES
QScheme
Quantizer
Quantizers
SYMMETRIC
String
TENSOR
TIME
TORCH
This
Unrecognized
kPerChannelAffine
kPerChannelAffineFloatQParams
kPerChannelSymmetric
kPerTensorAffine
kPerTensorSymmetric
AT
CHECK
CONTAINER
Check
CheckNotNull
CheckNotNullCommon
DCHECK
Debug
ERROR
FATAL
FEWIV
FILE
FOR
GE
GLOG
GT
Helpers
IF
INFO
INSTANTIATE
IS
If
Iter
LE
LINE
LOG
LT
Log
LogMessageFatal
LoggerVoidify
Logging
Macro
MessageLogger
Must
NDEBUG
NE
NOTNULL
NULL
ON
OP
Optimized
Otherwise
Output
PREFIX
PrintSequence
Return
SEVERITY
Self
Support
THRESHOLD
The
These
This
Two
Types
VLOG
W
WARNING
When
Write
APIs
As
At
Available
But
CPUCachingAllocator
CPUCachingAllocatorInterface
Cache
Caches
CachingAllocator
Checks
DISCLAIMER
Drop
Error
FlatHashMap
For
Furthermore
GetThreadLocalCachingAllocator
HashMap
If
In
Instantiate
Invariants
It
Memory
NB
NOLINTNEXTLINE
No
Note
OS
Otherwise
Reason
See
Self
Since
SmallVec
There
Therefore
This
Thus
Try
Usage
Use
We
What
When
Why?
WithCPUCachingAllocatorGuard
A
ATen
Backend
C
CPU
Calls
Cuda
CudaEvent
DEFAULT
Device
DeviceGuard
DeviceGuardImplInterface
DeviceGuardImpls
DeviceIndex
DeviceType
Does
E
Each
Event
EventFlag
For
Getters
HIP
If
In
Increments
InlineDeviceGuard
InlineEvent
Move
Option
PYTORCH
Querying
Returns
See
Self
Stream
The
These
This
VirtualGuardImpl
When
Y
recordOnce
A
ASSERT
AT
Allocator
An
Box
Can
DataPtr
Deepcopy
Device
DeviceType
For
INTERNAL
Identifies
LinkedListLink
NB
Option
Python
Return
Returns
Self
So
Storage
StorageImpl
StorageImplAdapter
TODO
TORCH
Technically
This
Torch
UniqueStorageShareExternalPointer
UseByteSize
VC
Version
We
You
CHECK
Dimension
INDEX
Option
TORCH
WrapDimMinimal
Option
ScalarType
ScalarTypeToTypeMeta
TODO
TypeMeta
fromScalarType
isScalarType
toScalarType
typeMetaToScalarType
A
C
CUSTOM
DEFINE
EXPECT
Foo
LOCAL
PREFER
STORAGE
THREAD
TLS
TRUE
ThreadLocal
~A
ANDROID
APPLE
C
GLIBC
HAS
NP
PREREQ
PTHREAD
SETNAME
String
kMaxThreadName
ASSERT
Cuda
Device
DeviceIndex
DeviceType
FakeGuardImpl
InlineMultiStreamGuard
InlineOptionalStreamGuard
InlineStreamGuard
MultiTestGuard
OptionalInlineStreamGuard
OptionalTestGuard
Stream
StreamId
TestDeviceType
TestGuard
TestGuardImpl
UNSAFE
getCurrentStreamIdFor
getDeviceIndex
resetStreams
setDeviceIndex
Allow
CRC
ConstexprCrc
Crc
Eq
From
H
Hash
Hasher
ID
Jones
Ord
PartialEq
PartialOrd
Self
TABLE
underlyingId
xFF
A
API
An
Besides
Box
But
CUDA
Constructor
Create
CudaStream
CudaStreamGuard
Default
Design
Device
DeviceGuard
DeviceGuardImplInterface
DeviceIndex
DeviceType
Even
For
From
Furthermore
If
In
Initialize
InlineDeviceGuard
InlineOptionalDeviceGuard
Morally
NOT
Note
On
Option
OptionalDeviceGuard
OptionalDeviceGuards
RAII
Returns
Self
Set
Sets
So
Stream
StreamGuard
TODO
The
This
Uninitialized
Unlike
VirtualGuardImpl
Whither
setDevice
ADDR
ASSERT
AT
Avoid
BIND
Bind
C
CHECK
Check
Could
ENABLE
F
FLAGS
Get
However
IsNUMAEnabled
MF
MOBILE
MOVE
MPOL
Move
NODE
NULL
NUMA
STRICT
TORCH
This
UL
Unable
VLOG
VLOGs
AND
ASSERT
BEEN
C
CANNOT
CHECK
Could
DEBUG
Default
Drop
HAS
HERE
INLINE
INTERNAL
IntArrayRef
LIKELY
MAX
Move
NOLINTNEXTLINE
Note
ONLY
OR
OVERWRITE
OVERWRITTEN
Packed
Resize
SIZE
SIZES
STRIDES
Self
Shift
Size
SizesAndStrides
SizesAndStridesUnion
SizesConstIterator
SizesIterator
SmallVector
StridesConstIterator
StridesIterator
TODO
TORCH
Tensor
TensorImpl
The
They
This
UNLIKELY
WOULD
We
Zero
allocateOutOfLineStorage
bytesToCopy
bytesToZero
copyDataInline
copyDataOutline
freeOutOfLineStorage
inlineStorage
isGrowing
isInline
newSize
newSizes
oldSize
outOfLineStorage
resizeOutOfLineStorage
resizeSlowPath
storageBytes
tempStorage
AT
Additional
Basically
Because
C
CONTAINER
CUDACC
FILE
FOR
For
GLOG
GOOGLE
INSTANTIATE
In
LINE
LOG
Log
LogMessage
MINIMAL
Note
STRIP
Some
Types
Using
A
ASSERT
AT
Allocator
AllocatorRegisterer
Also
An
Be
Box
By
COMPILE
CPU
Choice
Compare
Cuda
DEBUG
DEVICE
DataPtr
DataPtrs
Debug
DebugInfoBaseInterface
DebugInfoKind
Default
Deref
Device
DeviceType
Grep
However
INTERNAL
If
In
InefficientStdFunctionContext
It
MAX
Masquerading
MemoryProfilingEnabled
MemoryReportingInfoBase
MemoryReportingInfoBaseInterface
NB
NOT
Negative
Note
ONLY
Only
Option
PROFILER
ReportMemoryUsage
STATE
Self
Set
SetAllocator
So
TIME
TORCH
TYPES
Target
The
This
ThreadLocalDebugInfo
Thrust
ThrustAllocator
Under
UniqueVoidPtr
Unsafely
We
What
deleteInefficientStdFunctionContext
memoryProfilingEnabled
reportMemoryUsage
API
BUILD
C
COMPILE
CUDA
CUDAMacros
EXPORT
Export
GNUC
GPUS
GPUs
IMPORT
LIB
LIBS
MAIN
MAX
SHARED
See
TIME
The
This
WIN
We
C
CHECK
CUDA
CUDATest
Cuda
Just
This
cudaGetDevice
cudaGetDeviceCount
AnotherBar
Bar
C
CLASS
Cannot
Create
DEFAULT
Default
EXPECT
FALLBACK
Foo
FooRegistry
FooWithPriority
GXX
INFO
LOG
NE
NOLINTNEXTLINE
Non
Note
PREFERRED
PRIORITY
REGISTER
REGISTRY
RTTI
RegisterFooBarFallback
RegisterFooBarPreferred
RegisterFooDefault
RegisterFooDefaultAgain
Registry
Self
SetTerminate
THROW
TRUE
WITH
A
ALWAYS
ANONYMOUS
API
ARGS
ASSERT
ASSERTM
AT
ATen
Accepts
Add
An
Args
Assuming
Brian
C
CHECK
CHECK`
CONST
CUDA
CUDACC
Caffe
Callers
Calling
CompileTimeEmptyString
Could
Currently
DEBUG
DEPRECATED
Debug
Deprecated
Deprecation
Dispatched
Don
Drop
ERROR
EXPAND
EnforceFiniteError
Error
ErrorData
Errors
Exception
Exceptions
ExitException
Expected
FAILED
FILE
Fold
GIL
GXX
GetFetchStackTrace
Gets
HIPCC
Here
IF
IMPLEMENTED
INDEX
INTERNAL
If
In
IndexError
IndexErrors
Issue
It
JIT
LINE
LOG
Like
Logging
MESSAGE
MESSAGES
MSG
MSVC
MUST
Macros
Most
NB
NDEBUG
NOT
NOTE
NotImplementedError
NotImplementedErrors
Note
O
ON
ONCE
ONLY
OR
On
OnnxfiBackendSystemError
Onnxifi
Optimized
Option
Prints
Private
Provides
PyTorch
Python
RAII
RETHROW
RTTI
Report
Returns
RuntimeError
SAFE
STRINGIZE
STRIP
See
Self
Sets
Since
Some
SourceLocation
String
StripBasename
Supposed
THROW
TODO
TORCH
TYPE
The
These
This
ThreadWarningHandler
To
TypeError
TypeErrors
UNLIKELY
Usage
Use
Used
User
VA
VALUE
VARIABLE
ValueError
ValueErrors
Vaughan
Verbatim
WARN
WARNING
WITH
WORKAROUND
WarnAlways
Warning
WarningHandler
WarningHandlerInterface
Warnings
We
When
With
`SourceLocation`
`TORCH
condMsg
getBaseHandler
setWarnAlways
torchCheckFail
torchCheckMsgImpl
torchInternalAssertFail
userMsg
warnAlways
API
APIs
ASAN
ASSERT
Alternatively
BLOCKING
Based
C
CHECK
CUDA
CUDAFunctions
Call
Clear
Cuda
DEVICES
DeviceIndex
Did
Download
ENABLED
Error
Found
GPU
GPUs
Got
INTERNAL
If
In
It
LAUNCH
NB
NVCC
NVIDIA
No
NumCudaDevices
Oblige
Please
PyTorch
Setting
TORCH
The
This
Too
URL
Unexpected
VISIBLE
Version
WARN
We
Zero
\nCUDA
\nFor
cudaDeviceSynchronize
cudaDriverGetVersion
cudaError
cudaErrorInitializationError
cudaErrorInsufficientDriver
cudaErrorMemoryAllocation
cudaErrorNoDevice
cudaErrorUnknown
cudaGetDevice
cudaGetDeviceCount
cudaGetErrorString
cudaGetLastError
cudaSetDevice
cudaSuccess
Args
C
CHECK
CallAndMessage
CanonicalizeStrTypes
CompileTimeEmptyString
Container
Convert
Debug
Display
For
Formatter
Into
Messing
NOLINTNEXTLINE
Obtains
Replace
Represents
Result
Returns
SourceLocation
Specializations
String
StringUtil
TORCH
W
Write
isPrint
kSeparator
lenFrom
lenTo
numReplaced
ASSERT
FALSE
IFDIR
NE
S
TRUE
TempDir
TempFile
WIN
A
AND
ASSERT
AT
ATen
Allow
As
AtomicBool
Below
Blob
Box
C
CHECK
COMPLEX
CONSTEXPR
Copy
CopyNotAllowed
Create
DEBUG
DEFINE
DataType
Default
Delete
Destructor
Disallow
Display
Do
E
EXPORT
Eq
Explicit
FORALL
For
Formatter
GCC
Get
H
HOST
Hash
Hasher
However
ID
IF
INSTANCE
INTERNAL
If
Implementation
In
It
KNOWN
LIKELY
Linux
MAX
META
METADATA
MOBILE
MSC
MSVC
Make
MaxTypeIndex
Maximum
Mechanism
Mutex
MyType
NOLINTNEXTLINE
NOT
New
NewNotDefault
NumScalarTypes
ONLY
Option
Ord
Ordering
Other
Otherwise
PartialEq
PartialOrd
PickCopy
PickDelete
PickNew
PickPlacementDelete
PickPlacementNew
Placement
PlacementDelete
PlacementNew
PlacementNewNotDefault
Please
QINTS
Register
Result
Returns
Right
SCALAR
SFINAE
SIZE
ScalarType
Scalartype
Self
Some
String
TODO
TORCH
TYPE
TYPES
Tensor
The
There
This
ThrowRuntimeTypeLogicError
To
Type
TypeIdentifier
TypeIndex
TypeMeta
TypeMetaData
Typed
Types
UINT
Undefined
Uninitialized
Unrecognized
Unsupported
VER
WITH
We
Windows
Wpragmas
Wundefined
Wunknown
You
addTypeMetaData
deleteFn
isScalarType
newFn
nextTypeIndex
placementDelete
placementNew
scalarTypeItemSizes
typeId
typeMeta
typeMetaData
typeMetaDataInstances
typeMetaDatas
underlyingId
~T
ADD
AND
ArrayRef
CALLER
DCHECK
DEATH
Display
ERROR
EXPECT
EnforceEqWithCaller
Eq
Error
FAIL
FAILURE
FLAGS
Formatter
GE
GT
GTEST
HAS
Isn
It
Join
LE
LOG
LT
Message
NE
Noncopyable
PRINT
PartialEq
Result
Self
TEST
THAT
Temporary
This
WITH
WRAP
We
errStr
kFalse
kTrue
AND
AT
BFloat
Bool
But
Byte
C
CASE
CHECK
COMPLEX
CONSTANT
Char
Complex
ComplexDouble
ComplexFloat
Complexf
CppTypeToScalarType
Cuda
DEFINE
Differing
Dispatch
Display
Don
Double
ELEMENTSIZE
ENUM
FORALL
Float
For
Formatter
If
Int
Long
NB
NUM
NumOptions
NumPy
Order
Please
QINTS
QInt
QUInt
Result
SCALAR
SCALARTYPE
SIGNED
SPECIALIZE
ScalarType
ScalarTypeToCPPType
ScalarTypeToCType
ScalarTypes
Scalars
Short
TODO
TORCH
TYPES
These
This
Treat
UNKNOWN
Undefined
Unknown
WITH
We
includeBool
isComplexType
isFloatingType
isIntegral
isIntegralType
isQIntType
isSignedType
promoteTypes
promoteTypesLookup
toString
toUnderlying
ADD
ANY
ASSERT
CHECK
DEBUG
Does
EXPECT
Error
Expected
FAILURE
Functor
INTERNAL
NDEBUG
NO
NOLINTNEXTLINE
ONLY
R
RETHROW
STREQ
THROW
TORCH
This
WARN
While
Y
assertionArgumentCounter
expectThrowsEq
expectedMessage
failCheck
failInternalAssert
getAssertionArgument
A
ANONYMOUS
API
All
An
Another
BACKEND
Backend
Better
C
CHECK
COMPILE
CPU
CRTP
CUDA
Create
Cuda
D
DEFAULT
DEVICE
DISABLE
DISABLING
DataPtr
Destroys
DevType
Device
DeviceCount
DeviceGuard
DeviceGuardImpl
DeviceGuardImplInterface
DeviceGuardImplRegistrar
DeviceIndex
DeviceType
Does
Don
EVENT
Each
Ensure
Event
EventFlag
Every
ExchangeDevice
ExchangeStream
FB
FOR
First
Flags
For
Get
GetDevice
GetStream
Give
HIP
INVALID
If
Implementations
In
Increments
Intended
MAX
Meta
Meyer
NB
NON
NOT
NVCC
NoOpDeviceGuardImpl
OK
ONLY
Option
PYTORCH
Prominent
PyTorch
RAII
REQUIRED
RecordDataPtrOnStream
Registry
Return
Returns
Self
Set
SetDevice
Shiver
Since
So
Stream
TESTING
TIME
TIMING
TORCH
TYPES
The
These
This
To
Two
Ty
UncheckedSetDevice
Useful
VARIABLE
WARNING
Wait
When
You
exchangeDevice
getDeviceGuardImpl
xFF
ASSERT
C
DEBUG
INTERNAL
ONLY
PrimInt
Product
Return
Returns
Sum
TORCH
Throws
We
A
ASSERT
All
An
Args
CHECK
Calls
Copy
Creates
Destruct
Device
DeviceGuardImplInterface
DeviceType
Drop
Explicit
INTERNAL
If
Initializes
InlineDeviceGuard
InlineMultiStreamGuard
InlineOptionalDeviceGuard
InlineOptionalStreamGuard
InlineStreamGuard
It
Move
MultiStreamGuard
NOT
NOTE
Note
Option
OptionalStreamGuard
RAII
Resets
Restore
Returns
SFINAE
See
Self
Set
Stream
StreamGuard
StreamGuards
Streams
Subtly
TODO
TORCH
The
Then
This
U
Unfortunately
VALUE
VirtualGuardImpl
WARNING
exchangeStream
getDeviceGuardImpl
getDeviceTypeOfStreams
getStream
A
Align
Aligned
Default
Qint
Quint
Right
Self
Tensors
This
Underlying
A
AD
ADInplaceOrView
ALREADY
ALWAYS
AN
API
APIs
ASSERT
AT
ATen
Accept
Add
Adjust
After
All
Allocate
Allocator
Allow
Allows
Answer
Any
ArrayRef
ArrayRefs
As
Associate
Atomic
Autograd
AutogradBackend
AutogradMeta
AutogradMeta?
AutogradMetaFactory
AutogradMetaFactoryInterface
AutogradMetaFactoryRegisterer
AutogradMetaInterface
Avoid
BEEN
BY
BatchedTensorImpl
Because
Being
Box
But
C
CAN
CDHW
CEX
CHECK
CHW
COO
CPU
CSR
CUDA
Caffe
Call
Caller
Can
Cannot
Catches
Change
Changed
Channels
ChannelsLast
Check
Cleaning
Clear
Compute
Conjugate
Consider
Construct
Consult
ContiguityNotSupported
Contiguous
Copy
CopyBytes
Copying
Counter
Create
Cuda
Current
Currently
Custom
CustomBehavior
Customization
D
DEBUG
DECREF
DEFINITELY
DISABLE
DISABLED
DO
DOES
DON
DTYPE
Data
DataPtr
Default
Dense
Detached
Device
DeviceType
Did
Dim
Disabled
Disarm
DispatchKey
DispatchKeySet
DispatchKeys
Do
Dream
Drop
EVERY
EXCEPTION
EXTENSIBILITY
Each
Enum
Error
Every
Example
Expected
Extend
Extends
FALLTHROUGH
FLAGS
FUNCTION
Facebook
False
Finally
For
Forbidding
FreeMemory
Functions
FwGrad
GIL
GPU
GetAllocator
GetAutogradMetaFactory
Grad
H
HIP
HandleResize
HasContiguityPolicy
Historically
However
ID
IMPLEMENTED
INTERNAL
INVARIANT
Ideally
If
ImplType
In
Inference
InferenceMode
Inplace
Instead
IntArrayRef
Invariant
IsContiguousCustom
It
Keep
LOG
LWG
Layout
Like
LinkedListLink
Location
LongTensor
M
MAYBE
MLC
MOST
MS
MUST
Make
Match
May
Memory
MemoryFormat
Meta
Metadata
Metal
Might
MkldnnCPU
More
Most
Multiple
MutableGrad
My
NB
NCHW
NHWC
NOLINTNEXTLINE
NOR
NOT
NOTE
Named
NamedTensorMetaInterface
New
No
NoGradMode
Non
Normally
Not
Note
NumPy
O
OK
ONCE
ONLY
OOMed
OTHER
Old
On
One
OpaqueTensorImpl
Option
Ordinarily
Otherwise
Our
POD
PlacementDeleteContext
PlacementDtor
Please
Policy
Potentially
Preserve
Previously
PrimInt
Product
Py
PyInterpreter
PyInterpreterDecrefSig
PyInterpreterNameSig
PyInterpreterStatus
PyInterpreters
PyObject
PyObjects
PyTorch
Python
QuantizedCPU
QuantizedCUDA
QuantizedXPU
Question
RAISE
RAM
RHS
Read
Recompute
Release
ReleaseResources
Replace
RequiresGrad
Reserve
ReserveSpace
Reshape
Resize
Resizes
Result
Return
Returns
Right
Run
STORAGE
SavedVariable
Scalar
ScalarType
See
Self
Set
SetAutogradMetaFactory
SetDims
SetDimsTemplate
SetFwGrad
SetRequiresGrad
Sets
Setting
Shallow
ShareData
Shares
Sharing
Since
Single
Size
SizesAndStrides
SizesVector
SmallVector
SmallVectors
So
Some
Someone
Sort
Source
SparseCPU
SparseCUDA
SparseCsrCPU
SparseCsrCUDA
SparseHIP
SparseXPU
Specifically
Storage
StorageImpl
Storages
Strides
String
Struct
Structures
Subsequent
Support
Surely
TAGGED
TENSORIMPL
THIS
THPVariable
TLS
TODO
TORCH
TSAN
Tensor
TensorAccessor
TensorImpl
TensorImpl?
TensorImplAdapter
TensorImplInterface
TensorImplType
TensorIterator
Tensor`
Tensors
Test
TestTorch
That
The
These
This
Throw
Thus
To
Torchautograd
True
Ts
TypeMeta
TypeName
UBSAN
UNINITIALIZED
UNLIKELY
UNLOADED
US
USAGE
Undefined
UndefinedTensor
UndefinedTensorImpl
Unfortunately
Uninitialized
UniqueStorageShareExternalPointer
Unlike
Unpacked
Use
Used
VIEW
VIRTUAL
VIRTUAL`
Variable
VariableType
VariableVersion
VariableVersionDisabled
Variables
Version
VersionCounter
VersionCounterAdapter
View
ViewMeta
Vulkan
W
WARN
WARNING
We
What
When
Whether
Why
With
Wrap
Wrapped
XLA
XLATensorImpl
XPU
XXX
Yes
You
`C
`TENSORIMPL
`VariableVersion
`Variable`
deletePlacementDeleteContext
getAutocastRelatedKeySetFromBackend
getAutogradRelatedKeySetFromBackend
growthPct
highestPriorityBackendTypeId
isComplexType
kMkldnn
kSparse
kSparseCsr
kStrided
makeDataPtr
newCapacity
newData
newDims
newNumel
oldData
oldDims
oldSize
placementDelete
placementNew
typeMetaToScalarType
A
ADInplaceOrView
ANDROID
API
ARVR
An
Android
AutoDispatchBelowAutograd
Autograd
BackendSelect
But
C
Consider
Declared
DispatchKey
DispatchKeySet
DispatchKeySets
DispatchKeys
Don
Drop
Example
ExcludeDispatchKeyGuard
Exclusion
For
However
If
IncludeDispatchKeyGuard
Initialization
Inlining
Internal
LocalDispatchKeySet
MSC
NB
Non
Note
Originally
POD
PODLocalDispatchKeySet
Please
Pod
Profiling
Python
RAII
RAW
See
Self
TLS
The
This
ThreadLocalStateGuard
To
VER
We
Windows
XORing
isSupersetOf
API
AT
C
CHECK
CUDA
CUDAError
CUDAException
Cuda
ERROR
EXPR
Error
Exception
FILE
For
LINE
MESSAGES
MSG
Note
Runtime
STRIP
TORCH
This
Used
WARN
We
cudaError
cudaGetErrorString
cudaGetLastError
cudaSuccess
A
API
ASSERT
AT
All
Args
But
CUDAGuard
CUDAGuardImpl
Consider
Construct
Creates
Cuda
Destruct
Device
DeviceGuard
DeviceGuardImpl
DeviceGuardImplInterface
DeviceGuards
DeviceIndex
DeviceType
Drop
Example
Explicit
FakeGuardImpl
For
HIP
However
If
In
Initializes
InlineDeviceGuard
InlineOptionalDeviceGuard
It
Just
Move
NOT
NOTE
Note
Omitted
Option
Optional
OptionalDeviceGuard
OptionalDeviceGuards
RAII
Resets
Restore
Returns
See
Self
Set
Sets
So
TODO
Tensor
TensorList
The
There
These
This
U
VirtualGuardImpl
We
What
With
`OptionalDeviceGuard`
cudaGetDevice
cudaSetDevice
exchangeDevice
getDevice
getDeviceGuardImpl
kCUDA
kHIP
setDevice
uncheckedSetDevice
A
APIs
Allocates
Allocator
CHECK
Caffe
Creates
DataPtr
Device
DeviceType
GetAllocator
Legacy
Option
Returns
See
Self
Storage
StorageImpl
StorageImplAdapter
TODO
TORCH
TensorImpl
UniqueStorageShareExternalPointer
UseByteSize
A
ANDROID
API
Add
Addr
Align
AlignTo
Alignment
Aligns
Apache
B
BIT
Bit
BitReverseTable
Bits
But
C
CHAR
Ceil
Checks
Clamp
Count
Create
D
Delight
Denominator
Double
Dummy
Essentially
Euclid
Ex
Examples
Exceptions
F
FF
FFF
FFFFU
FIXME
Float
Get
Gets
HUGE
Hacker
High
INT
Identifier
If
Invalid
LICENSE
LL
LLVM
LeadingZerosCounter
License
Log
Low
MAX
MSVC
Macro
Make
MathExtras
Max
Multiply
NOTE
NaNs
NextPowerOf
Note
Numerator
Only
Option
Overflowed
Part
Per
PopulationCounter
Product
Project
Ptr
R
Replace
Requires
ResultOverflowed
Return
Returns
Reverse
S
SPDX
SaturatingAdd
SaturatingMultiply
See
Sign
Skew
So
Special
Subtract
Support
TODO
Template
The
This
TrailingZerosCounter
U
UB
UINT
Undefined
Unexpected
Use
Useful
VALF
Val
Value
WITH
We
Width
Y
Z
ZB
ZeroBehavior
alignAddr
alignTo
countLeadingOnes
countLeadingZeros
countPopulation
countTrailingOnes
countTrailingZeros
isInt
isMask
isPowerOf
isShiftedInt
isShiftedMask
isShiftedUInt
isUInt
llvmMathExtras
maskLeadingOnes
maskTrailingOnes
maxIntN
maxUIntN
minIntN
xF
xFF
~Value
~maskTrailingOnes
According
Args
Base
C
Class
Enable
Evaluates
Example
FuncType
Functor
LambdaType
MSVC
Result
See
Template
TypeTraits
We
BITS
Bitset
EXPECT
FALSE
IndexCallbackMock
NUM
TRUE
A
ASSERT
AT
Convenience
DEVICES
Device
DeviceCount
DeviceGuard
DeviceGuardImplInterface
DeviceIndex
DeviceType
Event
EventFlag
ExchangeDevice
ExchangeStream
FAKE
FakeGuardImpl
GUARD
GetDevice
GetStream
IMPL
MAX
Not
RecordDataPtrOnStream
Runtime
STATIC
See
Self
SetDevice
Stream
StreamId
TYPE
The
Ty
UNSAFE
UncheckedSetDevice
getDevice
kFakeGuardImplMaxDevices
AKA
API
AWESOME
All
And
As
BUILD
Basically
Buck
C
CMake
CPP
CU
CUDA
CUDACC
CUSTOM
Cuda
Definition
Do
ENUM
EXPORT
Enums
Export
For
GENERATED
GNUC
GPU
HIDDEN
HIP
Header
Here
IMPORT
If
In
Instead
LIB
LIBS
Linux
MACROS
MAIN
Macros
NEVER
NO
NOT
Once
PyTorch
SHARED
SPLIT
Stop
TORCH
The
This
USING
Visibility
WIN
We
When
Whose
Windows
XX
You
ALWAYS
C
Example
ForcedUnroll
Func
INLINE
Unroll
Utility
Various
A
AGGREGATE
ALLOC
API
ARGS
ASSERT
AT
Accumulates
All
AllocParams
Allocations
Allocator
Allows
An
And
Arc
Array
As
Attempt
BLOCK
BUFFER
Because
BitSet
Block
BlockComparator
BlockInfo
BlockPool
BlockPools
Box
But
C
CACHING
CHECK
CLASS
COUNT
CPU
CUDA
CUDACachingAllocator
CUDAGraph
CUDAGuard
CUDAOutOfMemoryError
Caching
Called
Can
CaptureId
CaptureStatus
Check
Comparison
Could
Create
Cross
Cuda
CudaCachingAllocator
CudaError
CudaMalloc
CudaStream
CudaStreamCaptureModeGuard
DEBUG
DataPtr
Default
Defensively
DeleterFnPtr
Deliberately
Device
DeviceAllocator
DeviceCachingAllocator
DeviceStats
DeviceType
Dumb
Dump
EB
Empty
Error
Event
Events
Execute
First
For
Free
FreeCudaMemoryCallbacksRegistry
FreeMemoryCallback
FreeMemoryCallbackInterface
Frees
GPU
GiB
Graph
HIPify
HashMap
HashSet
Holds
INTERNAL
IPC
If
In
Instead
Interaction
Invalid
IpcMutex
It
K
Keeps
Keys
KiB
LARGE
LIKELY
Large
MB
MEMORY
MIN
Make
Makes
Maps
Members
Memory
MempoolId
MempoolIdHash
MiB
Might
Most
Mostly
Mutex
NB
NCCL
NO
NUM
Negative
No
None
Not
Note
Number
ONLY
OOMs
Once
POOL
PYTORCH
Pair
Please
Pools
Potentially
Private
PrivatePool
Process
Processes
PyTorch
Q
REGISTER
ROUND
RawMutex
RecursiveMutex
RefCell
Resets
Retrieves
Returns
SIZE
SMALL
SUM
Search
See
SegmentInfo
Self
Since
Small
So
Stat
StatArray
StatType
StatTypes
Stops
StreamSet
String
Struct
Synchronize
THCCachingAllocator
TLS
TODO
TORCH
TYPES
The
Their
There
This
Thus
To
Tried
Trigger
Turn
ULL
UNLIKELY
Use
VA
VERSION
VERY
VecDeque
WITH
We
When
Why
With
Within
Yet
assertValidDevice
basePtr
cacheInfo
cachedAndFree
cudaError
cudaErrorMemoryAllocation
cudaErrorNotReady
cudaEvent
cudaEventCreateWithFlags
cudaEventDestroy
cudaEventDisableTiming
cudaEventQueries
cudaEventQuery
cudaEventRecord
cudaEventSynchronize
cudaEvents
cudaFree
cudaFreed
cudaFreeing
cudaGetDevice
cudaGetLastError
cudaGraphExec
cudaIpcCloseMemHandle
cudaIpcMemHandle
cudaIpcMemLazyEnablePeerAccess
cudaIpcOpenMemHandle
cudaMalloc
cudaMallocMaybeCapturing
cudaMallocs
cudaMemGetInfo
cudaSetDevice
cudaStreamCaptureModeRelaxed
cudaStreamCaptureStatus
cudaStreamCaptureStatusInvalidated
cudaStreamCaptureStatusNone
cudaStreamGetCaptureInfo
cudaSuccess
currentStreamCaptureStatusMayInitCtx
devPtr
emptyCache
forceUncachedAllocator
getBaseAllocation
getCudaFreeMutex
getCurrentCudaStream
getIpcDevPtr
getStats
ipcMemHandle
isRetry
kLargeBuffer
kMinBlockSize
kMinLargeAlloc
kRoundLarge
kSmallBuffer
kSmallSize
largestBlock
notifyCaptureBegin
notifyCaptureDestroy
notifyCaptureEnd
outSize
recordStream
reportMemoryUsageToProfiler
resetAccumulatedStats
resetPeakStats
setMemoryFraction
statType
AMD
ATen
Apple
C
CHECK
COMPILE
CPU
CUDA
Change
Compute
Cuda
DEVICE
Default
DeviceType
DeviceTypeName
DeviceTypes
Display
Eq
FPGA
Formatter
H
HABANA
HIP
HPU
Hash
Hasher
Hey
IDEEP
If
K
KCUDA
KFPGA
KMSNPU
MAX
META
METAL
MKLDNN
ML
MLC
MSNPU
Meta
Metal
NB
OPENCL
OPENGL
OpenCL
OpenGL
Option
PartialEq
Per
Protobuf
PyTorch
Reserved
Result
String
TIME
TORCH
TPU
TYPES
The
This
Thus
Try
Unknown
VULKAN
Vulkan
XLA
XPU
You
isValidDeviceType
CHECK
CP
Error
MultiByteToWideChar
NULL
String
TORCH
UTF
Unicode
WIN
WideCharToMultiByte
Wstring
An
Box
Copying
DataPtr
Device
DeviceCount
DeviceGuardImpl
DeviceGuardImplInterface
DeviceIndex
DeviceType
Event
EventFlag
ExchangeDevice
ExchangeStream
GetDevice
GetStream
OK
Option
RecordDataPtrOnStream
Self
SetDevice
Stream
This
Ty
UncheckedSetDevice
VirtualGuardImpl
destroyEvent
deviceCount
exchangeDevice
exchangeStream
getDefaultStream
getDevice
getDeviceGuardImpl
getStream
getStreamFromGlobalPool
isHighPriority
queryEvent
queryStream
recordDataPtrOnStream
setDevice
synchronizeStream
uncheckedSetDevice
A
ADInplaceOrView
APIs
All
AutoGradMode
Autograd
DispatchKey
DispatchKeySet
Drop
Expected
For
GradMode
GradMode?
InferenceMode
InferenceMode?
Inplace
Invariant
LocalDispatchKeySet
NoGradGuard
NormalMode
Note
Option
PODLocalDispatchKeySet
RAII
See
Self
Setting
TLS
TensorImpl
This
ThreadLocalState
TorchTensor
Torchones
Why
Without
`tensorTypeInCurrentExecutionContext`
ComplexDouble
ComplexFloat
Complexf
DefaultDtype
Double
Make
ScalarType
TypeMeta
toScalarType
A
AD
ADInplaceOrView
AFTER
AKA
API
ASSERT
AUTOCAST
AUTOGRAD
AUTOGRADOTHER
Add
Alias
An
At
AutocastCPU
AutocastCUDA
Autograd
AutogradCPU
AutogradCUDA
AutogradHPU
AutogradMLC
AutogradNestedTensor
AutogradOther
AutogradPrivateUse
AutogradXLA
AutogradXPU
BACKEND
BACKENDS
BackendSelect
BitAnd
BitIterator
BitOr
BitXor
But
CPU
Clear
Clone
CompositeExplicitAutograd
CompositeImplicitAutograd
Compute
Copy
Cuda
CustomRNGKeyId
DEBUG
DEFAULT
DISPATCH
Default
DefaultBackend
Define
DispatchKey
DispatchKeySet
DispatchKeySetFull
DispatchKeySetFullAfter
DispatchKeySetRaw
DispatchKeys
Dispatcher
Display
Does
EXCLUDED
EndOfBackendKeys
Eventually
FPGA
FULL
False
Find
Formatter
From
FullAfter
FuncType
Given
HIP
HPU
Historically
INCLUDED
INPLACE
INTERNAL
If
Implement
In
Initialization
IntoIter
IntoIterator
Intuitively
Is
IsNotDispatchKeySet
IsSame
It
Item
Iterator
JIT
KEYSET
Keys
LSB
MATH
MLC
MSNPU
Meta
Metal
MkldnnCPU
NB
NOT
Negation
NestedTensor
None
Not
Note
OK
ONLY
OR
OperatorEntry
Option
Output
PartialEq
Perform
Plumbing
PrivateUse
Public
QuantizedCPU
QuantizedCUDA
RAW
Remove
Resolve
Result
Return
Returns
SET
See
Self
Some
SparseCPU
SparseCUDA
SparseCsrCPU
SparseCsrCUDA
SparseHIP
String
Sub
TLS
TODO
TORCH
TensorImpl
Test
The
This
Through
True
ULL
Undefined
VIEW
Variable
Vulkan
WITH
XLA
XPU
`BitIterator`
`DispatchKeySet`
`IntoIterator`
`Iterator`
countLeadingZeros
getAutogradKeyFromBackend
getRuntimeDispatchKeySet
highestPriorityTypeId
~DispatchKeySet
A
ANDROID
ANONYMOUS
API
APIUsageDebug
APIUsageLoggerType
ARGS
Also
Android
Args
BINARY
C
CALLER
CHECK
Caffe
Capping
CommandLineFlagsHasBeenParsed
CompileTimeEmptyString
Convenient
DDPLoggingData
DDPUsageLoggerType
DEBUG
Data
DealWithFatal
Debug
Default
Drop
E
ERROR
EVERY
EnforceFailMessage
EnforceFiniteError
EnforceNotMet
EnforceOK
Equals
Error
Example
FATAL
FBCODE
FILE
FINITE
FIRST
FLAGS
Fix
For
GE
GLOG
GT
GetAPIUsageLogger
GetDDPUsageLogger
GetFetchStackTrace
Google
Greater
HashMap
IMPL
INFO
INT
If
In
Indicate
InitCaffeLogging
InitGoogleLogging
Input
InstallFailureSignalHandler
IsAPIUsageDebugMode
IsGoogleLoggingInitialized
IsVector
It
LE
LINE
LOG
LT
LogAPIUsage
LogAPIUsageFakeReturn
Logging
Logs
MIN
MS
MSC
MessageLoader
MessageLogger
Modify
NE
Normally
Nothing
ONCE
OP
Option
Optional
Other
Output
PREFIX
PY
ParseCommandLineFlags
Please
Pred
PyTorch
Return
Rich
SEVERITY
STDERR
Same
Self
Shape
Simulating
Some
String
StripBasename
THAT
THRESHOLD
THROW
TODO
TORCH
TSAN
The
This
ThrowEnforceFiniteNotMet
ThrowEnforceNotMet
Torch
UNIX
UNLIKELY
USAGE
UpdateLoggingLevelsFromFlags
VA
VARIABLE
VER
VERBOSE
VLOG
Very
W
WARN
WARNING
WITH
We
When
Windows
With
Write
You
`CAFFE
ddpData
enforceFailMsgImpl
enforceThatImpl
logFlag
ASSERT
CUDA
Device
DeviceGuard
DeviceType
FakeGuardImpl
HIP
InlineDeviceGuard
OptionalDeviceGuard
The
getDeviceIndex
setDeviceIndex
ClassAllowAssignment
ClassNoAssignment
CopyNotAllowed
Default
Disabling
EXPECT
FALSE
Id
ItemSize
Make
Match
NE
NOLINTNEXTLINE
Self
TODO
TRUE
TypeMeta
TypeMetaTestBar
TypeMetaTestFoo
placementDelete
placementNew
A
AutoGradMode
Default
Drop
GradMode
NoGradGuard
RAII
Self
A
CUDAGuard
CUDAGuardImpl
CUDAMultiStreamGuard
Create
Cuda
CudaStream
CudaStreamGuard
Device
DeviceGuard
DeviceIndex
Errors
If
Initializes
InlineDeviceGuard
InlineMultiStreamGuard
InlineOptionalDeviceGuard
InlineOptionalStreamGuard
InlineStreamGuard
It
MultiStreamGuard
NOT
NOTE
No
Note
Omitted
Option
OptionalCUDAGuard
OptionalCudaStreamGuard
OptionalDeviceGuard
OptionalStreamGuard
RAII
Resets
Restore
Returns
See
Self
Set
Sets
Stream
StreamGuard
The
Then
This
UNCHECKED
WARNING
Whither
cudaGetDevice
cudaSetDevice
cudaStream
cudaStreams
unwrapStreams
ASSERT
CHECK
Can
DEBUG
DispatchKey
INTERNAL
MSVC
ONLY
Self
TORCH
Tensor
TensorImpl
TypeMeta
Undefined
UndefinedTensorImpl
Without
globalContext?
ASSERT
BACKEND
C
CHECK
CUDA
CUDACachingAllocator
CUDAGuardImpl
Creates
Cuda
CudaEvent
CudaStream
DEFAULT
DISABLE
DataPtr
Device
DeviceGuardImplInterface
DeviceIndex
DeviceType
EVENT
Event
EventFlag
Flag
INTERNAL
Makes
Maps
May
Moves
NB
NOT
Option
PYTORCH
PyTorch
Resets
STATIC
Self
Stream
TIMING
TORCH
TYPE
These
WARN
createEvent
cudaError
cudaErrorNotReady
cudaEvent
cudaEventCreateWithFlags
cudaEventDefault
cudaEventDestroy
cudaEventDisableTiming
cudaEventQuery
cudaEventRecord
cudaGetDevice
cudaSetDevice
cudaStreamWaitEvent
cudaSuccess
getCurrentCudaStream
getDefaultCudaStream
getDevice
getStreamFromPool
isHighPriority
recordStream
setCurrentCudaStream
setDevice
uncheckedGetDevice
A
An
Copy
Create
Default
Device
From
HIGHLY
If
In
Initializes
InlineMultiStreamGuard
InlineOptionalStreamGuard
InlineStreamGuard
Let
Move
MultiStreamGuard
NOT
NOTE
Option
OptionalDeviceGuard
OptionalStreamGuard
RAII
Resets
Restore
Returns
See
Self
Set
Stream
StreamGuard
Then
This
Use
VirtualGuardImpl
WARNING
A
AND
API
ASSERT
AT
ATen
Additionally
Args
Autograd
AutogradCPU
AutogradCUDA
AutogradHPU
AutogradMLC
AutogradXLA
AutogradXPU
BE
Backend
Because
Bitmask
But
C
CHECK
CPU
CSR
CUDA
Caffe
Compiler
Consider
Constructors
Constructs
Contiguous
Convenience
Cuda
Default
Deprecate
Device
DeviceIndex
DeviceType
DispatchKey
Display
ERROR
Eek
Enum
FPGA
File
For
Format`
Formatter
Functions
HIP
HPU
Here
However
IDEEP
IMPLEMENTED
INTERNAL
INVARIANT
Ideally
If
In
It
Layout
MKLDNN
MLC
MSNPU
Make
MemoryFormat
Meta
Metal
Mkldnn
MkldnnCPU
Mutably
NB
NOT
NOTE
Note
OK
OPENCL
OPENGL
OR
Option
PURPOSELY
PrimInt
PyTorch
Python
QuantizedCPU
QuantizedCUDA
QuantizedXPU
Rather
Resolves
Result
Return
Returns
SAD
ScalarType
See
Self
Sets
Similarly
Since
So
Sparse
SparseCPU
SparseCUDA
SparseCsr
SparseCsrCPU
SparseCsrCUDA
SparseHIP
SparseXPU
Strided
String
TODO
TORCH
Tensor
TensorOptions
Tensors
The
Therefore
These
They
This
To
Torchzeros
TypeMeta
Unsupported
Variables
Vulkan
WARNING
WILL
We
XLA
XPU
`Device
`Device`
`Layout
`Layout`
`ScalarType
`ScalarType`
`TensorOption`
`TensorOptions
`TensorOptions`
`Torch`
`X`
computeDispatchKey
dispatchKeyToBackend
dispatchKeyToDeviceType
dispatchKeyToLayout
isQIntType
kCPU
kCUDA
kInt
kLong
kStrided
optTypeMetaToScalarType
scalarTypeToTypeMeta
typeMetaToScalarType
CUDATest
