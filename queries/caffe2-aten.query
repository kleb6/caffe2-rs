hello! could you please help me write a crate-description for the rust crate "caffe2-aten" containing the following symbols? not all of the symbols are defined in this crate, but they are used somehow (please do not explicitly list the symbols, or write anything other than a simple description.  no description header is necessary. however, please be descriptive without making too many assumptions):

ATen
ClampingParams
PyTorchQnnpU
aligned
assert
assume
aten
builtin
c
const
cpp
cpu
do
dup
ft
high
increment
input
kc
ks
lane
low
m
max
maxpool
min
n
native
neon
none
output
params
pytorch
qnnpack
quantized
src
sub
u
uint
uintptr
ukernel
usize
vcombine
vext
vextq
vget
vi
vim
vld
vmax
vmaxq
vminq
vmovq
vout
voutput
vreinterpret
vreinterpretq
vst
while

?
@note
AT
ATen
All
CHECK
ChannelsLast
DISPATCH
FN
IMPL
LIBRARY
List
MemoryFormat
NAME
Only
Option
QINT
Quantization
Quantized
QuantizedCPU
ReLUFused
SCALAR
SELECTIVE
Scalar
TORCH
TYPE
TYPES
Tensor
TensorIterator
TensorList
This
Unrecognized
all
aten
back
be
blocking
cat
channel
concatenation
const
contiguous
copy
cpp
cpu
define
dequantization
dequantize
dim
dispatch
double
dtypes
equal
false
fast
function
has
in
iter
kCPU
kPerChannelAffine
kPerTensorAffine
kPerTensorSymmetric
kernel
lazy
m
max
must
native
nhwc
non
op
out
parameters
path
per
point
points
push
pytorch
qcat
qconcat
qparams
qscheme
qtype
quantization
quantize
quantized
qx
qxs
qy
relu
reserve
same
scalar
scale
scales
scheme
schemes
share
sharing
should
size
src
static
stub
supported
tensor
toString
type
unary
underlying
uses
val
valid
value
xs
y
zero

ALLCLOSE
ANY
ASSERT
ATen
Acc
AccT
Contains
D
EQUAL
StackFunc
StartsWith
THROW
TOLERANCES
TRUE
Tensor
TensorList
TensorOptions
TestChunk
TestMatmul
TestSize
TestSplit
TestStack
TestStandardGammaGrad
TestWhere
Throw
Tolerances
acc
acct
algorithm
always
are
arguments
as
aten
atol
b
bT
be
begin
bmm
both
but
byte
case
cat
check
chunk
chunkMethod
chunkNs
compare
comparison
computation
cond
const
contiguous
cpp
cpu
d
device
dim
dimension
dimensions
direct
do
doesn
dot
double
element
empirically
empty
end
equal
equals
expand
expandable
expected
folding
func
function
gamma
give
gpu
grad
has
hasCUDA
helper
higher
in
insert
instead
kByte
kCPU
kCUDA
kDouble
kFloat
least
list
manual
match
matmul
method
mixing
mm
mul
must
mv
namespace
native
need
neg
no
non
o
one
ones
precision
pytorch
rand
randn
rebuilding
require
requireEqualTensorList
res
result
rtol
same
scalar
seed
select
selected
should
size
so
specified
split
splitMethod
splitNs
squeeze
src
stack
standard
stride
sum
technically
tensor
test
this
toType
type
types
unsqueeze
use
usize
view
we
with
work
wrong
y
z
zeros

ATen
ConvParam
Error
Note
PYTORCH
PrePackConvWeights
QNNP
QNNPACK
Remove
Runtime
Self
The
UNREACHABLE
We
Wrong
XZP
allocate
appropriately
are
assert
aten
be
bias
break
brq
bytes
c
can
case
cc
change
channels
char
const
conv
cpp
cpu
cr
deconv
deconvolution
default
dilation
dims
dw
dwconv
enum
error
failed
false
functions
gemm
group
groups
height
here
input
k
kc
kernel
kr
later
likely
log
malloc
memset
micro
modified
n
native
needs
new
nr
nullptr
once
only
output
pack
packed
packing
padding
params
points
prepack
pytorch
qnnp
qnnpack
quantized
runtime
size
sizeof
sr
src
stride
switch
swizzle
that
then
this
transpose
type
u
uintptr
ukernel
usize
void
w
weights
width
won
wrq
xzp
zero
zu

AND
ATen
DECONVOLUTION
DeconvolutionOperatorTester
OP
RUNTIME
STATIC
TEST
aten
batch
batchSize
cc
channel
cpp
cpu
d
deconvolution
dilation
groupInputChannels
groupOutputChannels
grouped
groups
input
inputPixelStride
inputSize
iterations
kernelSize
lazy
native
output
outputPixelStride
padding
paddingHeight
paddingWidth
per
pytorch
qmax
qmin
qnnpack
quantized
s
src
static
stride
test
with
zero

A
ASSERT
ATen
Activation
CHECK
CPU
Ceil
ChannelsLast
Contiguous
D
Dilation
FP
Finally
Furthermore
Here
INTERNAL
Input
IntArrayRef
Kernel
Layout
Make
Max
MaxPooling
MemoryFormat
Min
Mode
NCHW
NHWC
Namely
Normalize
Operator
Output
Padding
Parameter
Parameters
Still
Stride
Supports
TORCH
Tensor
XNNPACK
allocate
an
and
any
application
are
as
aten
available
batch
be
been
both
bottom
call
can
case
ceil
channels
conditions
configuration
const
containing
contig
contiguous
cpp
cpu
create
d
dealing
defaulted
device
dilation
dim
disabled
empty
eq
failed
false
flags
format
ft
gated
given
grad
gradients
greater
guaranteed
have
height
in
input
internal
kFloat
kernel
kernels
left
legitimately
list
max
maxpool
memory
min
mobile
mode
must
names
namespace
native
needed
negative
nhwc
no
non
none
normalized
not
numbers
op
operator
options
output
outputHeight
outputWidth
padded
padding
parameters
path
pixel
point
pool
pooling
positive
prohibits
pt
pthreadpool
ptr
pytorch
required
requires
result
right
scalar
scoped
setting
setup
shape
size
so
src
status
stride
subsampling
success
suggest
supported
sure
tail
taken
tensor
than
their
this
threadpool
top
type
u
unorthodox
usage
use
using
valid
vim
we
which
width
with
xnn
xnnpack

ASSERT
ATen
AVERAGE
AveragePoolingOperatorTester
OP
POOLING
and
aten
average
avgpool
batch
batchSize
cc
changing
channels
cpp
cpu
decreasing
few
height
increasing
initialize
input
inputHeight
inputPixelStride
inputScale
inputWidth
inputZeroPoint
kr
large
many
mr
mx
native
nextBatchSize
nextInputHeight
nextInputWidth
output
outputPixelStride
outputScale
outputZeroPoint
padding
paddingBottom
paddingLeft
paddingRight
paddingTop
params
point
pool
poolSize
pooling
poolingHeight
poolingWidth
pytorch
qmax
qmin
qnnp
qnnpack
qr
quantized
scale
setup
small
src
status
stride
strideHeight
strideWidth
success
swap
test
testQ
testSetupQ
u
unit
usize
width
with
xm
zero

ASSERT
AT
ATen
An
ContiguityNotSupported
Copy
Copying
DEBUG
Device
DispatchKeySet
ERROR
For
HasContiguityPolicy
INTERNAL
IntrusivePtr
NOTE
ONLY
Opaque
OpaqueHandle
OpaqueTensorImpl
Return
Self
Shallow
TORCH
TensorImpl
This
TypeMeta
VariableVersion
We
`
`allow
`shallow
`version
access
add
allow
an
and
another
are
arithmetic
arrayref
assertions
assumes
aten
base
because
but
cast
cfg
change
change`
changing
check
compatible
const
constructor
contiguity
copies
copy
could
counter
counter`
cpp
d
debug
define
dense
dest
detach
detach`
device
dim
do
does
doesn
ensure
even
false
fields
from
function
future
guarded
h
handle
has
have
in
interface
into
intrusive
key
let
make
metadata
move
name
need
never
new
no
non
not
now
offset
one
opaque
ops
or
order
other
overlapping
pointer
policy
properly
public
pytorch
refresh
release
resize
resources
s
see
set
shallow
should
size
specific
src
static
storage
stride
strides
support
supported
tensor
tensorimpl
tensors
that
there
this
throw
thus
type
u
unsafe
unwrap
usage
version
why
would

ATen
CPU
CPUGuardImpl
DeviceType
NoOpDeviceGuardImpl
aten
c
cpp
detail
guard
pytorch
register
src

?
@
@note
A
AND
ASSERT
AT
ATen
Above
Adjust
Allocate
Args
B
BatchLinearAlgebra
Below
C
CHECK
COMPLEX
CONTIGUOUS
CPU
CUDA
Checks
CholeskyFn
CholeskyInverseFn
Complex
Compute
Contiguous
D
DEBUG
DECLARE
DISPATCH
Define
Do
ERROR
EigFn
Error
Exepected
Expected
FLOATING
FORMAT
False
First
For
Fortran
Frobenius
From
GEEV
GELS
GELSD
GELSS
GELSY
GEQRF
GESV
GETRF
GETRI
GPU
Gels
Gelsd
Gelss
Gelsy
GeqrfFn
GradMode
H
Here
INTERNAL
If
Illegal
In
Input
Int
IntArrayRef
LAPACK
LEGACY
LQ
LU
Lapack
LapackLstsqDriverType
LapackLstsqDriverTypeGelss
LinalgEigFn
LinalgEighFn
Long
LstsqFn
LuFn
LuSolveFn
MAGMA
MEMORY
MemoryFormat
NaNs
Next
Note
Now
NumPy
Number
ONCE
ONLY
ORGQR
On
Option
OrgqrFn
OrmqrFn
Output
PyTorch
Q
QR
Qt
R
Replace
Resize
Rt
Run
S
Scalar
ScalarType
See
Since
Solves
String
StringView
Supports
TODO
TORCH
TYPES
Tensor
Tensors
That
The
There
These
This
To
TriangularSolveFn
True
Two
U
UPLO
V
VR
VT
Value
Valueype
Vh
WARN
We
\n
`Q`
`R`
`compute
`driver
`driver`
`gels`
`info`
`input`
`inverse`
`reduced
`result`
`tau`
able
about
above
accommodate
accordingly
actual
addition
additional
after
algebra
algorithm
alias
all
allocate
allocated
allow
allowed
allows
already
also
although
always
an
and
and\n
any
anything
appear
apply
appropriate
arbitrary
are
args
argument
arguments
arrays
as
asserts
assumed
assumptions
aten
available
avoiding
b
back
batch
batchCheckErrors
batchCount
batched
batches
batchsize
be
because
been
before
begin
behavior
behaviour
below
blocking
boolean
both
broadcast
broadcasted
but
c
calculates
calculations
call
calling
calls
can
case
cases
cast
cbegin
cdouble
cend
cfg
cfloat
cgeev
cgels
cgelsd
cgelss
cgelsy
cgeqrf
cgesdd
cgesv
cgetrf
cgetri
cgetrs
changed
char
check
checkLinalgCompatibleDtype
checkSameDevice
checkUplo
checked
checking
checks
cheev
cheevd
cholesky
class
clear
clone
cloneBatchedColumnMajor
codes
column
columns
com
comment
compatibility
compilation
complete
complex
computation
computations
compute
computeLRWorkDim
computed
computes
cond
condition
conditions
conj
conjugate
consecutively
consider
consistent
const
contain
containing
content
contiguous
controls
convert
copy
copyBatchedColumnMajor
correct
cpotrf
cpotri
cpotrs
cpp
cpu
create
ctrtrs
cuSOLVER
cuda
cungqr
cunmqr
current
currently
d
deal
deallocate
declare
decltype
decomposition
default
define
definitions
depending
deprecated
description
details
determine
determining
device
dgeev
dgels
dgelsd
dgelss
dgelsy
dgeqrf
dgesdd
dgesv
dgetrf
dgetri
dgetrs
diag
diagonal
diff
difference
differences
differentiable
dim
dimension
dimensional
dimensions
dims
directions
directly
discussion
dispatch
dispatched
dispatching
distinguish
do
docs
documentation
does
doesn
doing
don
done
dorgqr
dormqr
double
doubles
dpotrf
dpotri
dpotrs
driver
drivers
dst
dsyev
dsyevd
dtrtrs
dtypes
easier
effectively
eig
eigendecomposition
eigenvalue
eigenvalues
eigenvectors
eigh
eigvals
eigvalsh
eigvecs
either
element
elementary
elements
empty
enabled
end
endif
enum
epsilon
equal
equals
equation
equations
error
errors
everywhere
ex
exepected
exit
expand
expected
expects
explicit
exposed
extern
extra
extract
factor
factorization
false
far
faster
favor
field
fill
filled
find
first
flavor
floating
floats
following
form
format
fortran
found
from
full
function
functions
further
future
garbage
geev
gels
gelsd
gelss
gelsy
general
geqrf
gesdd
gesv
getrf
getri
getrs
getting
github
going
got
grad
greater
h
hand
has
have
having
helper
here
hierarchy
higher
hold
holding
householder
how
https
hybrid
ifndef
ignore
imag
imaginary
immediately
implement
implementation
implementations
implemented
implicitly
in
in\n
includes
incorrect
incorrectly
info
information
infos
infs
inplace
input
instead
integer
interface
intermediate
internal
inv
inverse
inversion
invert
ipiv
irange
isComplexType
isfinite
issuecomment
issues
item
its
iwork
j
jobvl
jobvr
jobz
jpvt
k
kCPU
kInt
kLong
keepdim
lapack
lapackCholeskySolve
lapackGels
lapackGelsd
lapackGelss
lapackGelsy
lapackGetri
lapackLU
lapackLstsq
lapackLu
lapackSolve
lapackSvd
lapackSymeig
large
last
later
layout
lazy
lda
ldb
ldc
ldu
ldvl
ldvr
ldvt
least
left
legacy
length
less
let
level
library
like
linalg
linear
linearSolveCheckInputs
live
liwork
loop
lower
lrwork
lstsq
lu
lwork
m
magnitudes
main
major
make
mat
match
matches
math
matmul
matrices
matrix
matrixStride
matrixes
max
may
maybe
memory
message
messages
messes
might
mimicking
min
minimizing
mn
mode
mode`
modified
modify
more
much
mul
must
n
name
named
narrow
native
nd
ndim
need
needed
new
no
non
norm
not
nothing
now
nrhs
nullptr
number
obtained
once
one
only
operating
operation
operations
opposite
optimization
optimum
optionally
options
or
order
orgqr
original
ormqr
orthogonal
other
otherwise
out
outf
output
outputs
outside
overwrites
overwritten
pair
pairs
parameter
parse
part
parts
pass
passing
path
per
perform
performs
pinv
pivots
place
please
point
pop
portion
positive
possible
post
postprocessing
potrf
potri
potrs
pow
preferred
primal
problem
process
processed
processing
produce
produced
product
promote
promotion
provide
provided
ptr
pull
push
pytorch
q`
qr
queries
r
rank
rather
raw
rcond
real
reconstruction
reduced
reflectors
registered
reinterpret
relative
release
removed
replaced
repr
req
required
requirements
requires
res
reside
residual
residuals
resize
result
results
returned
returns
reversed
rhs
right
routine
routines
rows
rwork
s
safely
same
save
saved
saves
saving
scalar
sd
see
separate
sequence
set
sgeev
sgels
sgelsd
sgelss
sgelsy
sgeqrf
sgesdd
sgesv
sgetrf
sgetri
sgetrs
shape
should
side
significantly
similar
singleCheckErrors
singular
size
sized
slice
smaller
so
solution
solution\n
solve
solved
some
sorgqr
sormqr
space
specific
spotrf
spotri
spotrs
square
squareCheckInputs
squared
squareness
squares
squeeze
squeezed
src
ssyev
ssyevd
st
start
static
storage
store
stored
stores
storing
str
str`
stride
strides
strtrs
stub
such
suggest
sum
supported
sure
svd
svdvals
swap
syev
syevd
symeig
system
takes
tau
temporary
tensor
tensors
terms
th
than
that
them
then
therefore
these
they
this
tie
tmp
toBool
toComplexType
toInt
toValueType
tolerance
tolower
too
top
torch
toupper
trans
transform
transition
transpose
transposed
triangular
tril
triu
trtrs
tuple
type
typeMetaToScalarType
typename
types
u
uncomment
unitary
unitriangular
unlike
unordered
unpack
unpacking
unsigned
unsqueeze
unused
up
uplo
upon
upper
use
used
useful
user
uses
using
usual
uv
v
valid
vals
value
valued
values
variant
vec
vecs
vectors
vh
view
vl
void
vr
vt
w
wants
was
way
we
well
what
whenever
whether
which
while
wi
will
with
with\n
wkopt
work
working
works
workspace
would
wr
wraps
wrong
xla
yml
zero
zeros
zgeev
zgels
zgelsd
zgelss
zgelsy
zgeqrf
zgesdd
zgesv
zgetrf
zgetri
zgetrs
zheev
zheevd
zpotrf
zpotri
zpotrs
ztrtrs
zungqr
zunmqr
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

?
ASSERT
ATen
CPU
CUDA
GetAllocator
INTERNAL
IntArrayRef
Storage
TORCH
Tensor
TensorShapeCUDA
This
TypeMeta
allocator
along
as
aten
avoid
be
because
byte
checkSetStorage
consistent
cpp
cpu
cuda
device
dispatch
don
done
getCUDADeviceAllocator
getting
have
implementation?
in
lines
native
needs
not
nullopt
nullptr
offset
opt
optional
pytorch
resize
result
same
set
size
split
src
storage
stride
this
unify
unsafeGetTensorImpl
use
way
we
with

API
ASSERT
ATEN
ATen
Args
F
FN
Func
NB
Output
TRUE
Tensor
add
allclose
are
args
aten
b
be
because
const
cpp
decltype
empty
expected
faithful
forward
function
guaranteed
how
method
mul
only
operators
out
overloads
pass
pytorch
rand
randn
result
sin
src
tensor
test
that
through
variant
works
wrapper

API
ATen
All
Bool
CHECK
Check
Complex
Consider
Corresponding
FIXME
ForeachUtils
HCC
HIP
However
In
KB
Note
PLATFORM
Please
Resulting
Scalar
ScalarLists
ScalarType
TODO
TORCH
Tensor
TensorList
TensorLists
There
This
To
`
`TensorListScalarListMetadata`
`check
`is
`tensorList
above
access
all
already
and
any
api
argument
as
aten
b
be
before
begin
boolean
c
call
calling
can
case
check
checked
comment
complex
complicate
conditions
const
corresponding
cpp
d
dense
device
division
do
does
dtypes
due
duplicated
either
elements
empty
end
endif
expected
false
fast
fastpath
following
foreach
function
functionality
future
got
h
has
have
ifdef
in
include
includeBool
input
integer
integral
irange
isComplex
isIntegralType
j
kStrided
kernel
launch
layout
least
let
limit
list
lists
make
means
method
might
mkozuki
must
native
need
non
not
now
number
okay
okay`
one
only
op
or
overlapping
path
possibility
preconditions
promote
promotion
provides
pytorch
really
restrictions
restrictions`
result
route
same
satisfied
scalar
scalarLIst
scalarList
scalars
set
several
should
size
specializing
src
strided
strides
supported
supports
sure
tensor
tensorList
tensorLists
tensors
that
there
thing
this
type
unwrap
use
using
via
we
whether
which
will
with

?
ATen
CHECK
Expected
LossMulti
TORCH
Tensor
TensorArg
arg
aten
batch
but
check
cpp
dim
empty
got
h
inconsistent
input
loss
margin
matrix
multi
multilabel
native
ndims
nframe
non
optional
or
pytorch
shape
size
src
valid
with

?
ASSERT
ATen
AliasAnalysisKind
AliasInfo
Arg
Argument
BC
BufWriter
C
CONSERVATIVE
Check
Checks
Compatibility
Default
Display
E
Eventually
Every
Expected
Formatter
Function
FunctionSchema
HashMap
INTERNAL
IValue
If
Inferred
Kind
ListType
NB
NOK
Narrowing
Non
OK
OperatorName
Option
OptionalType
Output
Parameter
PartialEq
Result
Returns
Self
String
StringType
StringView
Symbol
TODO
TORCH
Tensor
Tensor?
TensorType
The
These
Type
TypeKind
TypePtr
Values
\n
actual
additional
adjusting
after
alias
aliasInfo
allow
always
an
analysis
and
annotated
any
appends
arbitrary
are
arg
argument
argument?
arguments
arity
as
aten
available
b
backward
be
because
become
both
break
broadcast
broadcasting
but
c
calls
can
cases
cast
castRaw
cbegin
cend
check
checkSchema
checked
checks
class
clone
compatible
compiler
conditions
consider
considering
const
constructed
continue
core
correct
corresponding
count
cout
cpp
currently
d
default
directly
doesn
due
dump
either
empty
eq
equal
error
errors
expected
explicit
false
find
first
fmt
following
follows
format
found
from
full
function
getElementType
getNamespace
h
has
have
here
hint
historically
however
ignore
in
index
infer?
inferred
info
information
inherit
inherits
input
instead
io
isInferredType
isNone
isWrite
keyword
kind
known
kwarg
kwargs
least
length
let
lhs
list
lists
many
map
matter
may
merging
method
mismatch
missing
move
msg
must
mutation
name
namespace
new
no
none
normalize
not
ns
nullopt
nullptr
number
object
objects
ofInts
old
once
one
only
operator
operators
opt
optional
or
ostringstream
other
otherwise
out
output
overload
parser
pos
position
positional
present
primarily
primitive
print
printQuotedString
program
provide
pt
pytorch
remapped
remove
reporting
repr
represent
resolving
ret
returns
rhs
s
same
scalars
schema
seen
serialized
set
setNamespaceIfNotSet
should
size
sized
so
specifiable
specified
src
statically
str
structure
substituted
subtype
successfully
t?
takes
that
then
there
this
those
toQualString
toStringRef
treat
ty
type
typecheck?
types
u
unopt
unwrap
use
used
usize
v
value
values
vararg
varargs
varret
was
we
what
whether
which
whose
why
will
with
wo
work

ADD
APPLE
ATen
Add
After
And
Args
BEGIN
BEQ
BHS
BLO
BNE
BX
CMP
ELF
END
Each
F
FUNCTION
For
GNU
Generate
It
LDR
LSL
Load
MOV
MOVLO
MOVLS
MOVNE
Note
Now
POP
PUSH
S
SUB
SUBS
Shift
Stack
Store
TEQ
TOS
This
Thus
U
VADD
VCVT
VEOR
VEXT
VLD
VMLAL
VMOV
VMUL
VPOP
VPUSH
VST
VSUBL
aarch
activation
addr
after
align
and
arch
are
arm
armv
as
aten
b
base
be
because
bit
block
blocks
bytes
c
ch
channel
col
const
contain
contains
conv
cpp
cpu
d
dq
dynamic
element
endif
first
fpu
ft
gemm
has
id
ids
ifdef
ifndef
in
index
indx
input
ip
k
kernel
last
late
lazy
load
loading
loop
lr
mr
multipliers
n
native
needs
neon
next
non
none
nonzero
note
nr
nth
number
offset
one
only
opposed
out
output
packed
packedA
params
passed
pattern
per
point
pointer
points
processed
progbits
ptr
pushing
pytorch
qnnp
qnnpack
quantization
quantized
r
reg
requant
restrict
row
s
scale
section
should
sp
sparse
sparsity
src
stack
static
stride
syntax
temp
that
this
transposed
u
ukernel
unified
union
user
usize
value
values
via
vim
vmultiplier
void
vxa
vxb
w
we
weight
weights
well
which
will
zero

@colesbury
@soumith
AT
ATen
CHECK
CUDA
CUDART
CUDNN
CUSOLVER
Comments
CreateCuSolverDnHandle
CuSolverDnHandle
CuSolverDnPoolType
CusolverDnHandlePool
DESTROY
DestroyCuSolverDnHandle
DeviceThreadHandlePool
HANDLE
It
NO
PoolWindow
PoolWindows
See
Sometimes
TORCH
This
Thread
VERSION
Windows
already
and
are
as
aten
atexit
avoid
back
be
because
c
caused
cfg
com
context
copied
cpp
create
cuDNN
cuda
cudaGetDevice
current
cusolver
cusolverDnCreate
cusolverDnDestroy
cusolverDnSetStream
decided
destroy
destroyed
destruction
device
dn
dumb
endif
fbcode
from
getCurrentCUDAStream
gets
github
handle
handles
hangs
happens
https
ifdef
implementation
in
initialization
initialized
issues
its
lazily
local
make
myPoolWindow
newPoolWindow
not
or
ordering
pool
ptr
ptrs
pull
pytorch
releasing
reserve
reserved
setting
shared
solver
something
src
static
stream
terminates
that
this
thread
time
type
unique
will
workaround
would

ATen
Broken
CONNECTED
Dynamic
FULLY
FullyConnectedOperatorTester
Mode
OP
Runtime
Static
TEST
TODO
aten
batch
batchSize
cc
channel
connected
cpp
cpu
dynamic
fully
input
inputChannels
inputStride
integration
iterations
native
output
outputChannels
outputStride
per
pytorch
qmax
qmin
qnnpack
quantized
runtime
small
src
static
stride
test
testQ
unit
with
zero

A
ADD
APPLE
ATen
After
And
Args
B
BEGIN
BEQ
BHS
BLO
BX
CMP
ELF
END
Each
FUNCTION
GNU
K
LDR
LSR
M
MOV
MOVLO
MOVLS
MOVNE
Note
Now
Original
POP
PUSH
Packed
S
SUBS
Sequence
TOS
This
Thus
VEXT
VLD
VST
VTRN
We
aarch
activations
adjacent
after
align
all
alls
and
appropriate
arbitrary
arch
are
arm
armv
as
aten
b
be
block
blocks
c
can
care
combine
const
contain
contiguous
contiguously
copying
cpp
cpu
create
d
endif
format
fpu
ft
gemm
given
group
h
has
have
helps
high
ifdef
ifndef
in
ip
just
k
kx
last
lazy
loading
locality
loop
low
lr
m
matrix
memory
more
mr
multiple
native
neon
none
not
note
now
one
out
packA
packed
passed
placed
pointer
progbits
pushing
pytorch
qnnpack
quantized
r
reg
registers
rest
rows
section
sets
sp
sparse
src
stack
static
store
stored
stride
syntax
taken
that
them
then
those
tranpose
tranposed
transpose
transposed
u
uint
ukernel
unified
usize
va
valid
values
via
vim
void
w
we
wil
will
writing

ATen
MM
PyTorchQnnpConvQuantizationParams
SHUFFLE
add
adds
aten
c
channel
const
conv
cpp
cpu
cvtepi
cvtps
cvtsi
do
epi
epu
extract
ft
index
input
k
kc
kernel
ks
load
loadl
loadu
m
madd
max
min
mm
mr
mul
native
none
nr
output
packs
packus
params
point
points
predecrement
ps
pytorch
qnnpack
quantization
quantized
requantization
restrict
scales
set
setzero
shift
shuffle
si
src
srl
srli
sse
stride
sub
u
uintptr
ukernel
unpackhi
unpacklo
usize
va
vacc
vb
vim
vmultiplier
void
vout
voutput
vxa
vxb
vzero
w
while
zero

AT
ATen
CHECK
ERROR
Fill
Functions
GRAIN
Implementations
SIZE
Scalar
TORCH
Tensor
TensorIterator
TensorIteratorConfig
Tensors
Trust
`
`copy
add
all
and
are
as
aten
b
back
be
boundary
build
but
bytes
chacks
check
constants
copy
cpp
cpu
declare
define
dense
device
diag
diagonal
dim
dimension
dimensions
dispatch
equal
false
fill
got
h
handle
height
idempotent
in
input
integers
internal
item
itemsize
iter
kCPU
kFloat
larger
length
main
mem
memset
meta
min
multiply
must
nDims
native
nelements
non
nullptr
offset
okay
ones
only
out
output
outputs
overlap
overlapping
ptr
push
pytorch
quantization
quantized
resize
same
scalar
set
size
so
src
step
storage
stride
strided
strides
stub
supports
tensor
than
that
type
value
void
width
with
wrap
zero
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ATen
Allow
CHECK
D
Expected
FUNC
IMPL
IntArrayRef
META
Non
Option
TORCH
Tensor
UpSampleNearest
as
aten
backward
batch
be
begin
but
check
common
compute
const
cpp
cpu
d
define
dim
dimension
dimensions
dispatch
double
empty
end
expected
factors
format
full
got
grad
h
have
input
integers
kCPU
kernel
lazy
memory
multiply
native
nearest
not
optional
options
osize
other
out
output
pytorch
same
scale
scales
set
shape
size
src
static
suggest
tensor
upsample
value
w
with
zero

ATen
PytorchRequantizationFunction
aten
const
cpp
cpu
declare
export
fp
ft
function
gemmlowp
h
ident
input
lrintf
macro
magic
n
name
native
neon
none
output
point
precise
psimd
pytorch
qmax
qmin
qnnp
qnnpack
quantized
requantization
requantize
rules
scalar
scale
signed
src
sse
ssse
stubs
type
u
unsigned
usize
vim
void
zero

?
API
ASSERT
ATen
Access
Activation
BUFFER
Bias
Block
Buffer
C
CHECK
COMBINED
CONV
CONVS
Command
CommandBuffer
Compute
Conv
Convolution
D
DESCRIPTOR
Deconvolutions
Destination
DeviceType
Dilation
Experimentation
Filter
Future
Groups
H
HW
HasState
IEEE
IMAGE
IMPL
INT
INTERNAL
Image
Implemented
Input
IntArrayRef
IntrusivePtr
Invalid
It
K
KERNEL
KH
KO
KW
Kernel
LIBRARY
LIKELY
Layout
Max
Min
Not
OC
OH
OK
OLD
OW
Object
Only
Option
Output
Padding
Parameter
Pass
Payload
Pool
Read
Reason
ResourcePool
SAMPLER
STORAGE
Scalar
Self
SizeNumel
Source
Stage
State
Stride
TODO
TORCH
TYPE
Tensor
The
TorchJitCustomClassHolder
TransformBlock
UNIFORM
VK
VTensor
VULKAN
Vulkan
W
WINOGRAD
Weight
Winograd
Write
accepts
access
adapter
addded
align
all
an
and
appropriate
are
arg
arr
async
aten
available
barriers
base
batch
be
bias
biases
bit
block
both
break
buffer
bufferVIdx
but
bypasses
c
calculates
can
case
cfg
channels
clamp
combination
command
complement
const
context
contiguous
conv
convert
convolution
cpp
cpu
create
d
dDepthwise
dMethod
dOld
dOldApi
dOpContext
dOpContextPacked
dOpContextState
dOpContextUnpacked
dPointwise
dSlidingWindow
dWinograd
default
defined
depth
depthwise
determine
device
dilate
dilation
dim
dispatch
div
downcast
dst
dw
effective
either
enum
every
expand
experimentation
extents
false
fill
filter
final
floating
func
function
future
gpu
grad
group
groups
h
handle
has
have
height
host
ic
identical
idx
ih
ikernel
image
implied
in
individually
infinity
input
inputSize
inserts
integers
intrusive
invalid
inverse
isFloatingPoint
item
ivec
iw
k
kFloat
kUseConv
kUseWinogradConvs
keep
kernel
kh
kw
kx
ky
lazy
leak
lessthan
lifetime
limits
local
m
make
managed
max
memcpy
memory
memset
method
min
mod
move
must
n
namespace
native
nbytes
nchw
ndimension
necessary
needed
new
nogroup
not
numbers
object
oc
oh
old
only
ops
options
or
originals
out
output
outputMax
outputMin
outputSize
overrideable
ow
pack
packed
padding
param
parameter
parameters
params
payload
persistent
point
pointwise
pool
prepack
provided
ptr
pw
pytorch
queue
representations
requires
resource
retval
ridx
s
safe
scalar
set
sh
shader
size
sizeof
so
src
stack
stacks
static
stream
stride
submit
subprogram
supported
switch
sx
sy
synchronization
sz
szvi
tensor
tensors
their
tile
track
transform
transposed
triggers
type
u
uniform
units
unpack
unpacked
unsupported
up
usable
usage
use
using
usize
utils
uvec
v
vTensor
valid
value
vec
vi
void
vulkan
w
wait
weight
weights
which
width
winograd
work
y

ANY
AS
ATen
All
Apache
BASIS
C
CONDITIONS
Compute
Copyright
FFFFF
Google
IS
Inc
It
KIND
LICENSE
License
Licensed
OF
OR
QNNPACK
Reserved
Rights
See
Size
The
UINT
Unless
VQRDMUL
Version
WARRANTIES
WITHOUT
You
aarch
adapted
adjusted
agreed
an
and
apache
applicable
assert
aten
below
benchmarks
bits
but
c
clamped
comparative
compliance
const
copy
cpp
cpu
distributed
doubling
either
endif
except
exponent
express
file
fixup
fp
from
ft
gemmlowp
governing
high
http
ifdef
implementation
implied
in
input
itself
language
law
library
licenses
limitations
may
multiplier
n
native
neon
none
not
obtain
only
or
org
output
packed
parameters
permissions
point
product
pytorch
qmax
qmin
qnnp
qnnpack
quantized
requantization
requantize
required
s
scale
scaled
shift
software
specific
src
tests
this
u
uint
under
unit
use
used
using
vandq
vcombine
vdupq
vim
vld
vmaxq
vminq
vmultiplier
vqaddq
vqmax
vqmin
vqmovn
vqmovun
vqrdmulhq
vrshlq
vshift
vshrq
vst
vzero
w
with
writing
www
xy
xyzw
y
you
z
zero
zw

ATen
ClampingParams
PyTorchQnnpU
address
assert
aten
c
const
cpp
cpu
do
epu
ft
increment
input
k
kc
ks
load
loadu
m
max
maxpool
min
mm
n
native
none
o
output
params
ptrdiff
pytorch
qnnpack
quantized
si
src
sse
storeu
u
uintptr
ukernel
usize
vi
vim
vmax
vo
vout
voutput
while

?
A
ATen
Add
C
CHECK
Check
Code
Compute
Convert
Current
ELLIPSIS
Ensure
Expected
Fast
Finally
Find
For
Fused
Here
If
Ignore
Implicit
LABELS
LETTERS
Labels
Linear
Lookup
MOBILE
MaybeOwned
Move
Multiply
NUM
Note
OF
Only
Option
Parse
Repeated
Save
See
Start
StringView
Sum
Suppress
TORCH
TOTAL
Tensor
TensorList
The
There
Unsqueeze
We
Z
\
`
about
above
add
adding
addmm
after
align
all
along
already
also
an
and
any
appear
appeared
appearing
appears
are
arrow
as
asserts
assumes
assumptions
aten
avoid
away
b
back
backward
batch
be
because
been
before
begin
bias
bilinear
bitset
bmm
borrow
borrowed
both
break
broadcast
broadcasted
broadcasting
buf
but
can
case
cast
cbegin
cdims
cend
char
check
computation
computations
compute
computed
computes
const
constexpr
contracted
contraction
copy
correct
count
covered
cpp
create
csize
curr
d
deal
decltype
default
defined
desired
device
diagonal
diagonals
dim
dimension
dimensions
dims
do
does
don
dot
einstein
einsum
elements
ell
ellipsis
emplace
empty
end
endif
equation
erase
every
execute
expand
expected
explicit
extract
false
faster
fewer
fill
finally
find
first
flatten
flattened
following
found
frequency
from
given
got
hacky
has
have
identify
implements
in
index
input
insert
into
invalid
irange
isalpha
isupper
its
j
k
keep
keepdim
kept
label
labels
last
later
least
left
length
lhs
like
linear
list
lists
lo
lpermutation
lro
main
make
makes
mapping
marginally
match
matmul
matrix
max
maximum
maybe
means
missing
mkldnn
mm
more
most
movedim
msg
mul
multiplication
multiply
multiplying
must
narrow
native
ndims
need
needed
next
nlabels
no
non
nontrivially
not
now
npos
number
once
one
only
onto
op
operand
operands
operations
opermutation
ops
opt
optimizing
optional
options
or
order
orgiginal
original
ostringstream
other
out
output
over
owned
pair
pairwise
parsing
part
parts
path
per
perm
permut
permutation
permutations
permute
permuted
pipeline
place
pos
pre
present
promotion
provide
provided
purpose
push
pytorch
readable
reducing
reduction
reductions
remapped
removal
repeated
reserve
reshape
reshaping
resize
result
reverts
rhs
right
ro
roughly
rpermutation
rsizes
s
same
scalar
second
seen
shape
shapes
should
similarly
simpler
size
sized
sl
slicemul
so
some
spaces
specified
split
squeeze
sr
src
static
store
str
subscript
subscripts
substr
sum
sumdim
sumdims
summed
summing
sumproduct
support
switch
take
tensor
tensordot
tensors
than
that
them
then
this
three
tmp
total
track
trilinear
type
u
unify
unroll
unrolled
unsigned
unsqueeze
unsqueezed
unsqueezing
unused
use
used
using
usize
valid
value
vec
vectors
view
void
want
warining
warning
was
we
weight
were
which
will
with
within
work
wrapper
xnnpack
z
zA
zero
zeros

ATen
PyTorchQnnpConvQuantizationParams
aarch
aligned
assume
aten
builtin
c
channel
clamped
const
conv
cpp
cpu
do
dup
endif
ft
high
ifdef
index
input
k
kc
kernel
ks
lane
low
max
min
mr
n
native
neon
none
nr
output
params
point
points
predecrement
pytorch
qnnpack
quantization
quantized
requantization
restrict
s
scale
scales
shift
sizeof
src
stride
sub
u
uint
uintptr
ukernel
usize
va
vacc
vaddq
vb
vcombine
vcvtnq
vcvtq
vdupq
vextq
vfmagic
vfmax
vfmin
vget
vim
vimagic
vld
vmaxq
vminq
vmlal
vmov
vmulq
void
vout
voutput
vqaddq
vqmovn
vqmovun
vreinterpret
vreinterpretq
vshl
vst
vsubl
vsubq
vxa
vxb
w
while
zero

ATen
Arithmetic
BaseType
Comparison
Const
ConstStridedRandomAccessor
Constructors
DefaultPtrTraits
Index
Integer
Note
Pointer
Prefix
PtrTraits
PtrType
RESTRICT
RestrictPtrTraits
StridedRandomAccessor
The
Todo
U
`other`
`this`
access
accessor
an
and
are
aten
below
call
cast
category
class
const
copy
cpp
decrement
defined
difference
different
explicit
friend
h
here
idx
increment
index
introduce
iterator
lazy
like
modifier
native
nullptr
offset
operations
operator
operators
other
over
platforms
pointer
postfix
ptr
public
pytorch
random
reference
reinterpret
represent
restrict
same
sequences
src
static
stride
strided
tag
that
this
traits
type
typename
using
value
version
we
well

AT
ATen
BatchNorm
CHECK
CPU
CheckedFrom
Constant
ENABLED
ERROR
Float
Half
If
MIOPEN
MIOpen
MaybeOwned
Note
Option
ROCM
Remove
ScalarType
See
TODO
Tensor
TensorArg
TensorDescriptor
Unused
access
and
as
aten
average
back
backward
backwards
batch
be
bias
borrow
build
but
c
can
cast
causes
cfg
checkAllContiguous
checkAllDefined
checkAllSameGPU
checkAllSameType
checkDimRange
checkNumel
checkSameSize
checkScalarType
compiled
condition
const
correct
cpp
dataType
defined
desc
descriptor
dim
don
double
emplace
empty
entirely
epsilon
etc
exclusive
expand
expandScale
exponential
factor
features
file
from
getMiopenDataType
getMiopenHandle
grad
hacky
handle
has
idesc
initialize
input
maybe
mean
miopen
miopenBNPerActivation
miopenBNSpatial
miopenBatchNormMode
miopenBatchNormalizationBackward
miopenBatchNormalizationForwardInference
miopenBatchNormalizationForwardTraining
mode
native
norm
not
one
opt
optional
options
or
output
owned
part
passed
philosophy
preprocessor
problems
ptr
pytorch
removal
require
running
save
scalar
scale
size
so
src
static
support
tensor
tensors
that
them
this
training
tuple
type
undefined
value
var
view
wdesc
we
weight
while
with
wrapper
zero

ATen
B
BATCH
Bn
C
CHECK
DIMS
Finally
First
Format
H
NON
NUM
Next
PixelShuffle
TORCH
Tensor
This
W
added
allows
alongside
and
aten
batch
be
begin
but
c
channel
channels
collapsing
const
constexpr
cpp
dim
dimension
dimensions
dims
divisible
done
downscale
end
expects
factor
final
from
got
h
have
height
in
input
insert
into
iota
its
least
maintain
native
nd
new
next
not
oc
oh
ow
permutation
permute
permuted
permuting
pixel
positive
pytorch
reshape
reshaped
resulting
s
separate
shape
shuffle
shuffling
single
size
split
square
squared
src
st
unshuffle
unshuffling
upscale
used
w
width
with
within

ATen
CHECK
Check
Expected
Index
Long
Option
ScalarType
ScatterGatherChecks
Size
String
TORCH
Tensor
Test
Tests
Used
`gather`
`scatter
`scatter`
add`
all
and
apart
as
aten
be
break
check
checks
continue
cpp
d
dim
dimension
dimensions
dims
does
dtyp
ensure
equal
expected
false
from
gather
h
has
have
index
input
like
match
method
methods
must
name
native
nonempty
not
number
opt
out
pytorch
same
scalar
scatter
shape
size
smaller
src
tensor
than
type
value
vs
whether
wrong

?
ASSERT
AT
ATen
All
BN
Backend
CHECK
CPU
ChannelsLast
Contiguous
DIM
DISPATCH
DimVector
E
ERROR
Expected
FLOATING
FUNC
For
Helper
IMPL
INTERNAL
InvStd
JIT
MAX
META
MIOPEN
MaybeOwned
MemoryFormat
NHWC
Normalization
Note
Option
Q
Reduce
RenormScaleFactorFn
Scalar
See
TODO
TORCH
TYPES
Tensor
TensorIterator
TensorIteratorBase
TensorIteratorConfig
The
Unsupported
Var
VarTransform
XXX
Y
able
about
acc
accessor
accscalar
actual
add
alias
all
an
and
are
arg
as
aten
averages
b
backend
backends
backward
batch
batchnormMinEpsilonCuDNN
be
because
before
begin
bias
binary
borrow
borrowing
break
broadcasts
build
but
c
calculate
can
cast
cat
centered
chain
channel
check
checkBackend
clone
collect
common
compiledWithCuDNN
compiledWithMIOpen
compute
conditional
const
contain
contiguous
conversion
convert
copy
corresponding
cpp
cpu
cuda
cudnn
d
dL
dX
dY
declare
define
defined
device
dim
dimension
dimensions
dims
directly
dispatch
don
done
dot
dotp
elements
empty
enabled
enabling
end
eps
epsilon
erase
eval
evaluation
except
expected
factor
false
features
format
from
function
getCUDAHooks
got
grad
gradOuput
gradOutput
gradient
h
hacky
half
have
implementation
in
index
indices
inference
information
input
inside
instance
instead
into
invoke
invstd
iota
isComplex
iter
its
k
kBFloat
kByte
kCPU
kDouble
kFloat
kHalf
keep
keepdim
kept
kernel
lazy
least
like
linalg
make
manipulating
mask
match
maxnorm
maybe
mean
mem
memory
miopen
mode
modify
momentum
mul
must
n
name
native
nd
ndim
need
needs
non
norm
normalization
not
only
op
opt
optional
options
or
out
outf
output
outputs
overlap
owned
parallel
path
per
pointers
positive
precision
product
projection
pytorch
real
reduce
removal
renorm
repeat
reserve
reserved
reservedSpace
reshaped
rmean
running
rvar
same
save
scalar
scale
scaled
select
selection
serial
set
shape
should
sigma
since
size
so
space
spatial
sqrt
src
static
stats
strided
strides
stub
suggest
sum
supported
sure
synchronized
tensor
tensors
that
their
then
they
this
tie
time
toAccumulateType
toDouble
toValueType
train
training
transform
tuple
type
u
unary
unbiased
update
use
used
using
value
valued
var
variance
vec
versionCuDNN
view
void
w
want
we
weight
while
will
with
wrap
wrapper
zero

?
@note
ALL
AND
AT
ATen
Applying
BFloat
BinaryOpsKernel
Bool
Boolean
C
CHECK
COMPLEX
CPU
CPython
DISPATCH
Divide
Division
Double
FLOATING
Float
Floor
For
HALF
Half
IEEE
INTEGRAL
In
It
NAN
NOTE
NaN
Note
ONCE
Objects
Python
SIMD
Scalar
ScalarType
See
So
Subtracts
TODO
TORCH
TYPES
TensorIterator
TensorIteratorBase
There
This
UNLIKELY
Undefined
Vectorized
WARN
With
XOR
ZeroDivisionError
abs
ace
add
added
addition
additional
aims
alpha
always
and
are
as
atan
aten
b
ba
backward
base
basic
be
behavior
beta
bit
bitwise
blendv
blob
both
c
calc
calculated
can
case
cast
cf
clamp
com
common
complicated
conj
const
constant
copysign
cpp
cpu
cpython
d
dd
delta
diff
different
dispatch
div
divide
dividend
division
divisor
does
don
double
due
dy
eps
eq
equivalent
errors
exp
fail
false
five
fix
floatobject
floor
floordiv
fmadd
fmax
fmin
fmod
from
gcd
ge
github
gt
half
have
heaviside
hi
https
huber
hypot
igamma
igammac
ignore
ignored
implementation
in
includeBool
inf
infinity
instead
integer
integral
intentionally
isComplexType
isFloatingType
isIntegralType
isinf
isnan
iter
just
kBFloat
kBool
kByte
kHalf
kNanVec
kOneVec
kZeroVec
kernel
lcm
le
limits
lo
log
logaddexp
logical
logit
lshift
lt
m
maintain
make
mask
max
maximum
may
min
minimum
mod
more
mse
mul
multiplication
native
ne
neg
nextafter
no
nonzero
not
one
operation
operator
or
otherwise
outputs
performing
point
pow
property
py
python
pytorch
quiet
quot
r
reference
register
rem
remainder
res
result
results
rewrite
rounding
rshift
s
same
scalar
see
sigmoid
sign
signs
since
slower
smooth
so
some
special
src
standard
static
stub
sub
subtraction
tanh
tensors
than
this
trunc
truncation
try
type
types
ubsan
undefined
unsigned
ups
using
val
vec
vectorize
which
with
work
wraps
xlog
xlogy
xor
y
z
zero

ANY
ASSERT
ATen
An
BINARY
Bool
COMPARISON
CPU
CUDA
ComparisonLoopBinary
CpuKernelMultipleOutputs
EXPECT
FOR
ITER
MULTIPLE
Mixing
NO
OUTPUT
OUTPUTS
POINTWISE
ScalarType
SerialLoopBinary
SerialLoopBinaryNoOutput
SerialLoopPointwise
SerialLoopPoinwiseNoOutput
SerialLoopUnary
SerialLoopUnaryNoOutput
TEST
THROW
TRUE
TYPE
Tensor
TensorIterator
TensorIteratorConfig
TensorIteratorTest
TensorOptions
The
To
UNARY
Verifies
acc
add
all
alternative
an
and
are
aten
b
be
binary
build
c
calculate
check
clamp
coerced
common
comparison
compute
config
convert
cpp
cpu
ctype
cuda
device
devices
diff
dim
do
empty
equal
exception
expected
export
fail
false
first
forall
hasCUDA
id
ident
in
input
isFloatingType
isn
iter
iterator
k
kBool
kByte
kCPU
kCUDA
kDouble
kFloat
kInt
kLong
keep
kernel
lambda
lazy
lift
loop
macro
min
mixed
mul
multiple
name
native
no
non
not
ones
only
op
operation
options
out
output
outputs
overflow
owned
parameter
pointwise
prevent
promote
promoting
pytorch
raise
randint
randn
random
result
rules
same
scalar
serial
should
single
squeeze
src
static
sub
subtraction
tensor
tensors
test
this
thread
toBool
tuple
type
types
unary
unit
unsigned
void
way
we
will
with
y
zero
zeros

A
ATen
C
ENTRY
Entry
FORALL
Instead
InternedStrings
NOTE
NS
SYMBOL
SYMBOLS
Self
String
Symbol
all
and
append
as
assignments
aten
aten\
be
because
but
byte
cast
char
clear
codegen
compile
const
constexpr
constructible
constructors
core
could
cpp
create
cuda\
dedupe
define
directly
done
entries
entry
even
expanding
follows
function
haven
how
huge
implemented
in
info
instead
into
keys
lazy
len
loop
me
minutes
more
move
n
name
names
namespace
namespaces
namespaces\
needed
new
no
not
ns
offsets
operator
optimize
or
packing
particularly
pointers
prim\
pytorch
qual
raw
register
reserve
s
save
sep
several
space
src
static
storing
straightforward
strlen
structs
switched
sym
symbols
tables
take
takes
thanks
that
then
this
time
u
undef
unqual
usize
we
which
would

ATen
Apply
ApplyDynamic
ApplyDynamicRelu
ApplyRelu
Bias
LinearPackedParamsBase
LinearPackedParamsBaseInterface
Option
Tensor
TorchJitCustomClassHolder
Unpack
apply
aten
bias
cpp
cpu
dynamic
error
h
implemented
input
native
not
out
output
packed
parameter
params
point
pytorch
quantized
range
reduce
relu
runtime
scale
set
src
this
throw
trait
type
unpack
variant
zero

ASSERT
ATen
Clang
Enum
EnumName
NOTE
String
Unsupported
Variant
We
```
an
aten
basic
complain
const
constructor
cpp
default
enum
enumtype
error
func
initialization
invoke
kEnum
lazy
name
need
object
otherwise
provide
provided
pytorch
src
static
test
testns
type
user
v
variant
visit
without
would

ATen
ClampingParams
PyTorchQnnpFp
Size
aarch
aten
c
clamping
const
cpp
cpu
defined
dup
endif
ft
k
lane
low
max
min
mr
native
neon
none
nr
params
pytorch
qnnpack
quantized
sgemm
src
stride
uintptr
ukernel
va
vacc
vb
vextq
vfmaq
vget
vim
vld
vmax
vmaxq
vmin
vminq
vmlaq
vst
w

?
ASSERT
ATen
Call
Compute
GE
LE
NEAR
Prepare
PyTorchQ
VAddMicrokernelTester
VAddUKernelFunction
Verify
aData
aScale
aZeroPoint
add
assert
aten
b
bData
bScale
bZeroPoint
begin
bind
compute
const
cpp
cpu
default
device
distribution
end
false
fill
generate
h
inplaceA
inplaceB
inplacea
inplaceb
isnormal
iteration
iterations
kernel
max
micro
microkernel
min
mt
n
native
optimized
parameters
params
point
pytorch
qmax
qmin
qnnp
qnnpack
quantization
quantizationParams
quantize
quantized
random
randomDevice
ref
reference
results
rng
scalar
scalarQuantizationParams
scale
src
test
tester
this
u
uniform
union
usize
vadd
xA
y
yFP
yRef
yScale
yZeroPoint
zero

???
A
API
ARGS
ASSERT
ATen
All
Another
Append
As
At
B
BATCHING
BINARY
BatchDims
Batched
BatchedTensor
BatchedTensors
Batching
BatchingRegistrations
Binop
Bk
BroadcastingVmapTransform
C
CHECK
COMPARISON
CPU
CPUTensor
CUDATensor
Calling
Calls
ChannelsLast
Checks
Claim
Comparison
Contiguous
Converts
CppFunction
D
Device
Diagonal
Do
Don
DoubleTensor
Each
Equation
ExtraArgs
FIXME
Fiddling
First
FloatTensor
For
Func
Furthermore
Future
Got
Hand
Here
How
However
IMPL
INTERNAL
Ideally
If
Ik
In
Input
IntArrayRef
IntList
K
LIBRARY
Layout
Let
Logical
MUST
Memory
MemoryFormat
MultiBatchVmapTransform
NB
NOTE
NYI
No
Note
OP
Option
POINTWISE
Part
Preserve
Promotion
PyTorch
RULE
S
Sanity
Scalar
ScalarType
Scalars
See
Shape
Should
Sk
So
Steps
TO
TORCH
TRIVIAL
Tensor
Tensor?
TensorIterator
TensorList
TensorOptions
TensorScalarType
TensorShape
TensorTensorScalarType
TensorTensorType
The
Then
There
Therefore
These
This
TorchScript
Type
UNARY
Under
Unop
VA
View
VmapDimVector
VmapPhysicalView
VmapTransform
VmapTransform?
VmapTransforms
WLOG
We
What
When
\
\leq
\sum
`
`as
`dim`
`new
`offset`
`other`
`result`
`self`
`size`
`sizes`
`stride`
`strides`
`tensor`
`torch
`x`
`xs`
`y`
aa
about
above
abs
absolute
ac
access
accessible
accomplish
accounting
acos
actively
actually
add
adding
additional
against
all
allow
allowed
allows
also
always
am
ambiguity
an
and
another
any
anything
apply
applyInplace
are
arg
args
argument
arguments
around
as
asin
assume
assumptions
atan
aten
attempt
autograd
b
back
backward
base
basic
batch
batched
batchedTensorForLoopFallback
batching
bc
bdeabb
bdim
bdims
be
because
before
begin
behavior
behaviors
being
better
binary
blob
blocking
bmm
bound
bounds
bug
but
c
call
called
calling
calls
can
cannot
careful
case
cases
cast
cat
ceil
check
checkBasicAsStridedValidForSlice
checkBatchDimsAtFrontInLayout
checking
checks
choose
chosen
chunk
chunks
claim
clamp
clone
com
comparison
complex
composite
comprehension
conj
consider
const
construct
contiguous
contiguous?
copy
correctness
correspond
cos
cosh
could
cpp
cpu
crazy
create
created
creates
cross
cuda
d
da
de
decide
defaultStrides
define
definition
dependent
destination
detail
details
device
diag
diagonal
dictates
did
didn
differ
different
digamma
dim
dimension
dimensionality
dimensions
dims
dims?
directly
discourage
dispatch
dispatched
dispatcher
div
do
documented
does
doesn
doing
don
dot
double
down
due
eager
ed
efficiently
either
element
empty
emulate
emulating
end
eq
equal
equivalent
error
even
everything
exactly
example
examples
exist
exp
expand
expanded
expanding
expect
expected
exploded
expm
express
extra
fail
fail?
fallback
fallthrough
false
file
fill
fine
first
floor
fmap
following
format
formats
frac
free
from
front
functions
future
ge
general
generate
getGradInputPhysicalDim
getPhysicalDim
getPhysicalDims
getPhysicalShape
getPhysicalToLogicalMap
github
given
gives
going
got
grad
greater
gt
guard
h
had
handle
handling
happens
hard
has
have
having
helper
here
hides
high
how
https
imag
implementation
implementations
implements
implicit
imply
important
in
index
indexable
indexed
indexes
indices
inequality
inplace
input
insert
inside
instead
intlist
into
irrespective
isBatchedTensor
isPhysicalScalarTensor
isn
its
itself
j
just
k
keepdim
key
know
lambda
larger
largest
last
latter
layout
lazy
le
least
length
let
level
lgamma
like
list
little
ll
loc
location
locations
log
logic
logical
logicalToPhysical
look
loop
lower
lowest
lt
lvl
m
made
make
makeBatched
makeFromBoxedFunction
makes
manually
many
match
matches
matmul
matrix
max
maximum
may
maybe
maybeGetBatchedImpl
means
mechanism
memory
metaprogram
method
might
min
mismatch
mm
mode
more
motivating
move
movedim
mul
multiple
must
mv
name
narrow
native
ne
necessary
need
neg
new
nice
no
non
not
nullopt
numBatchDims
number
numpy
offset
old
one
only
onto
op
operands
operates
operation
operations
operator
operators
ops
optional
options
or
order
original
other
otherwise
our
out
output
outside
over
overloads
part
partial
particular
pass
passed
passing
per
perform
performance
permute
philosophically
physical
pick
pin
pinned
place
plans
please
point
pointwise
positive
possible
potentially
pow
presence
preserve
pretty
probably
problem
produce
promote
prone
proof
provide
provided
provides
push
pytorch
quite
randn
range
rank
re
real
reasonable
reciprocal
recorded
refactor
refactoring
references
refers
register
relation
relu
rely
remove
replicate
report
reserve
reshape
resolution
resolve
respect
result
results
returns
rewrite
round
rsqrt
rsub
rule
rule?
rules
rules?
running
runs
rzou
s
sad
said
same
sample
sanity
satisfies
say
scalar
scalars
sections
see
select
semantics
sequence
shape
shapes
should
sigmoid
sign
signature
similar
sin
sinh
size
slice
slippery
slope
slow
smallest
so
some
something
source
special
specify
split
sqrt
squeeze
squeezed
src
stable
stack
start
static
step
steps
storage
strategy
stride
strided
strided`
strides
strides?
sub
succeeds
such
sum
support
supported
swaps
takes
tan
tanh
tensor
tensors
terminology
terms
than
that
them
then
there
these
they
thing
things
think
this
though
three
threshold
throw
too
torch
trace
translating
transpose
treated
tricky
trivially
trunc
try
turns
type
unary
unbind
undef
underlying
unfold
unless
unnecessary
unsafeGetBatchedImpl
unsqueeze
unwrap
us
use
user
uses
using
valid
value
variant
ve
very
via
view
views
vmap
vmap?
vmapped
vs
want
wanted
wavy
way
we
weird
were
what
whatever
whether
whole
why
will
with
within
won
words
work
works
worried
would
wrap
wrapper
wraps
write
writing
written
wrt
xs
y
yet
you
your
z
zero
zeros
zi

ASSERT
ATen
MAX
MaxPoolingOperatorTester
OP
POOLING
and
aten
batch
batchSize
cc
changing
channels
cpp
cpu
decreasing
dilation
dilationHeight
dilationWidth
few
ft
height
increasing
initialize
input
inputHeight
inputPixelStride
inputWidth
kr
large
many
max
maxpool
mr
mx
native
nextBatchSize
nextInputHeight
nextInputWidth
none
output
outputPixelStride
padding
paddingBottom
paddingLeft
paddingRight
paddingTop
params
pool
poolSize
pooling
poolingHeight
poolingWidth
pytorch
qmax
qmin
qnnp
qnnpack
qr
quantized
setup
small
src
status
stride
strideHeight
strideWidth
success
swap
test
testSetupU
testU
u
unit
usize
vim
width
with
xm
zero

ATen
PTThreadPool
Self
ThreadPool
aten
base
cpp
h
id
init
let
new
node
numa
or
pool
pytorch
setThreadName
size
src
threads
unwrap

ATen
Allow
CHECK
CompositeExplicitAutograd
D
Expected
FUNC
IMPL
IntArrayRef
META
Non
Option
QuantizedCPU
TORCH
Tensor
UpSampleNearest
as
aten
backward
batch
be
begin
but
can
check
common
compute
const
cpp
cpu
d
define
dim
dimension
dimensions
dispatch
double
empty
end
expected
factors
format
full
got
grad
h
handle
have
input
integers
kCPU
kernel
kernels
lazy
memory
multiply
native
nearest
not
optional
options
osize
other
out
output
overloads
pytorch
same
scale
scales
set
shape
size
src
static
structured
suggest
tensor
these
update
upsample
value
w
with
zero

ASSERT
ATen
Absolutely
CHECK
DEBUG
FULL
HARD
INTERNAL
MemOverlap
MemOverlapStatus
MemoryOverlap
NB
NO
ONLY
PARTIAL
Please
TOO
TORCH
Tensor
TensorImpl
Test
There
This
Whether
YES
across
alias
aliasing
and
are
assert
aten
b
be
before
begin
but
cast
char
clone
compute
const
cpp
dense
element
elements
end
enum
equality
expensive
h
has
input
internal
itemsize
kStrided
lap
layout
location
many
memory
might
miss
more
multiple
no
non
not
one
operation
or
overlap
overlapping
partial
people
performing
pointer
precision
python
pytorch
rather
reduces
refer
refers
renumber
same
similar
single
situations
size
some
src
static
status
storage
storages
strides
tensor
test
than
them
there
these
too
unsafe
unsafeGetTensorImpl
unsupported
update
usize
was
we
which
will
written
yes
you

ASSERT
ATen
C
Check
Dimname
DimnameList
Drop
Error
FALSE
For
H
In
NamedTensor
NamesMode
NoNamesGuard
None
Python
Set
String
Symbol
THROW
TRUE
TensorName
TensorNames
Test
W
alias
aliased
also
and
aten
attach
both
but
check
checkUnique
const
cpp
dimname
dimnameFromString
dimnames
doesn
dropping
empty
enabled
equal
error
expected
exposed
false
from
fromSymbol
future
guard
has
here
in
index
inplace
internal
irange
its
legacy
let
ll
merge
meta
metadata
name
named
namedinference
names
nchh
nchw
no
not
now
nullopt
nullptr
op
opt
other
position
print
propagation
pytorch
result
retrieved
right
s
set
size
smoke
so
src
str
symbol
tensor
tensornames
test
that
them
this
throw
toDimnameVec
type
unify
unifyFromRight
unifyFromRightInplace
unique
unsafeGetTensorImpl
value
we
with
wrapper
zeros

ATen
AdaptivePooling
IntArrayRef
Tensor
adaptive
aten
b
backward
c
ceil
const
cpp
d
declare
dispatch
end
floor
grad
h
index
input
kernel
lazy
native
output
pool
pooling
pytorch
size
src
start
static
using
void

ATen
BITSET
BitSet
CHECK
DIM
DimBitsetSize
SIZE
TORCH
This
Windows
WrapDimUtilsMulti
an
appears
are
around
aten
bitset
const
cpp
dim
dims
extra
file
h
in
interaction
list
maybe
multiple
ndims
only
operator
overloading
pytorch
seen
size
src
strange
supported
tensors
times
up
usize
with
work
wrap

?
ASSERT
ATen
Arg
Argument
Box
CHECK
CapsuleType
Class
ClassBase
ClassType
ClassTypePtr
Custom
Ensure
FunctionSchema
HashMap
INTERNAL
IValue
InitializerList
JitCompilationUnit
JitFunction
Namespace
QualifiedName
Self
Skip
String
TORCH
TypeId
addAttribute
already
arg
argIdx
args
arguments
aten
back
bcc
called
capsule
checkValidIdent
class
className
classTypePtr
classes
cloneWithArguments
const
core
count
cpp
create
custom
customClassMethods
customClasses
default
detail
doc
emplace
false
fmap
getCustomClass
getCustomClassTypeMap
getSchema
heck
index
info
insert
intrusive
intrusivePtrClassTypeid
isObject
map
method
methods
module
move
name
namespace
namespaceName
new
nullptr
old
once
only
ptr
pytorch
qualClassName
qualifiedName
register
registerCustomClass
registered
registration
reserve
schema
schemas
size
src
static
tagged
taggedCapsuleClassTypeid
that
toObject
torch
type
typeid
unique
unordered
usize
v
value
weak
with

ATen
Bool
CHECK
Promotion
ResultTypeState
Scalar
ScalarType
TORCH
Tensor
TensorImpl
TensorList
True
TypeProperties
Undefined
`from`
`self`
and
as
aten
b
be
c
can
canCast
cast
categories
combine
compatible
complex
conj
const
copied
copy
cpp
csr
cuda
current
d
default
defined
dim
dimResult
distributed
false
floating
from
h
has
have
higher
in
inference
isComplexType
isFloatingType
key
lower
move
native
new
number
options
other
point
promote
promoteTypes
pytorch
quantized
result
ret
s
scalar
set
shallow
signed
skip
so
sparse
src
state
tensor
tensors
that
type
typeMetaToScalarType
types
undefined
unsafeGetTensorImpl
unsupported
update
wrapped
wrappedResult
zero
zeroResult

?
ATen
Design
In
Legend
NB
PYTORCH
QNNPACK
QUANTIZATION
Quantization
RUNTIME
Run
This
We
Weights
`
`if
and
are
as
aten
b
bdq
be
because
block
blocking
blocks
boff
both
branching
brq
c
case
cfg
channels
const
conv
cpp
cpu
cr
deconv
define
dilation
divisble
dq
due
dw
dynamic
end
fills
from
ft
function
gemm
h
hgemm
however
in
input
introduced
izp
just
k
kc
kernels
ki
kr
ks
ksum
kv
kzp
lazy
linear
means
min
mode
multiple
must
n
native
nc
need
needed
none
not
np
nr
offset
ones
output
pack
packed
packing
padded
parameter
part
perf
point
points
prepacked
produce
ptr
pytorch
qnnpack
quantization
quantized
relies
remaining
result
rq
runtime
same
sconv
second
sgemm
should
significant
size
sizeof
some
space
sr
src
sse
start
static
subtracting
suffer
swizzle
tail
that
their
then
they
this
time
too
u
uint
uintptr
ukernels
union
use
usize
value
vim
void
w
want
wdq
we
weight
weights
what
which
wights
will
with
wrq
y
zero

?
A
AND
ASSERT
AT
ATen
At
B
BFloat
Bail
Bool
Boolean
Byte
CHECK
CPU
CUDA
Checks
ClampFn
ComplexDouble
ComplexFloat
Concatenate
Create
D
DISPATCH
DO
Device
Dimname
Don
Duplicate
Expected
FLOATING
FUNC
Floating
Heuristic
However
IMPL
INTERNAL
If
Implements
Index
Integral
IsInfinityOpFn
IsinDefaultFn
Layout
Long
Longer
META
MaybeOwned
MemoryFormat
ModeFn
NOT
NaNs
NoNamesGuard
None
Note
NumPy
ONCE
Once
Option
Preserve
PyTorch
Python
ReduceMinmaxFn
Reorder
Scalar
ScalarType
See
Sorting
Stable
Strided
THIS
TLDR
TODO
TORCH
TYPES
Tensor
TensorCompare
TensorIterator
TensorIteratorConfig
TensorOptions
The
This
True
Unsupported
Use
WARN
When
WhereFn
`
`set
`torch
`wrapped
aa
abs
actual
add
adjacent
after
algorithm
alias
all
allclose
allowed
allows
also
always
ambiguous
aminmax
an
and
are
argmax
argmin
argsort
arraysetops
as
assert
assume
async
aten
atol
b
backend
based
bc
bda
be
because
before
behavior
being
below
binary
bit
bitwise
blob
boolean
borrowing
both
build
but
c
call
calls
cannot
case
cast
casting
cat
cause
check
clamp
clip
close
closeness
com
common
compared
comparing
complex
computation
computed
computes
condition
consistent
const
convert
copy
cpp
cpu
cuda
d
dd
de
declare
default
define
defines
deprecated
descending
detail
device
did
difference
dim
dimname
dimreduce
dispatch
divergent
does
doesn
double
doubles
dtype`
dtypes
duplicate
either
element
elements
empty
encountered
enforce
eq
equal
equality
error
exactly
except
expand
expected
explanation
false
fb
fill
finite
first
flat
floating
form
forward
found
from
fully
function
future
github
got
greater
guard
h
hack
however
https
iand
ident
iff
ignore
imag
implementation
implemented
in
includeBool
incorrect
index
indexing
indices
infinite
infinity
input
instead
integer
interface
inverse
invert
ior
isBoolean
isComplex
isFloatingPoint
isFloatingType
isIntegral
isIntegralType
isclose
isfinite
isin
isinf
isnan
isneginf
isposinf
isreal
issues
item
items
iter
just
kBFloat
kBool
kCPU
kHalf
kLong
keep
keepdim
kernel
large
last
layout
lazy
least
less
let
lib
like
likely
limits
list
localScalar
locations
maintaining
make
mask
match
matter
max
may
maybe
mem
min
minmax
mode
more
moves
must
n
namedinference
names
nan
native
ne
necessary
need
negative
never
no
non
nonzero
normal
not
number
number`
numpy
once
one
ones
only
op
operation
operator
optional
options
or
order
original
other
out
outf
outplace
output
outputs
overlap
overloads
part
partial
parts
per
perhaps
point
position
pow
pre
priority
produce
promote
promotion
propagate
provided
put
py
pytorch
qscheme
quantized
rather
ravel
real
received
redispatch
reduce
reduction
removed
reportNYIDimnameOverload
repr
represented
representing
reshaped
resize
resolved
respectively
result
ret
reverse
revisit
rtol
s
safe
same
scalar
scalars
scale
set
should?
since
single
skip
slice
so
sort
sorted
sorting
src
stable
static
strided
stub
support
supported
supports
symmetrically
take
taken
tensor
tensors
test
than
that
them
then
this
tie
toUnderlying
torch
trickiness
trivial
tuple
ty
type
types
u
uint
unary
unique
unsafeGetTensorImpl
unsupported
unwrap
use
used
usually
val
value
values
version
very
view
void
we
which
will
with
within
won
wouldn
wrap
wrapped
y
yet
zero
zeros

ATen
arith
arm
aten
checks
cpp
cpu
cpuinfo
do
export
fp
ft
h
has
initialize
isa
macro
native
neon
none
pytorch
qnnpack
quantized
requires
rules
src
sse
test
vim
while

ATen
DispatchKey
IMPL
LIBRARY
RegisterDispatchKey
TORCH
aten
cpp
dispatch
lazy
m
pytorch
registrations
src
static
templates

Arc
CUDA
DataPtr
Device
DeviceType
Every
Here
IPC
Invariant
Note
Refer
Self
THC
THCAllocator
THCIpcDeleter
THCudaCheck
The
allocation
allocator
and
aten
base
basePtr
be
bit
block
c
caching
calling
cast
closed
construct
context
count
cpp
cudaGetDevice
cudaIpcCloseMemHandle
cudaMalloc
cur
decreased
delete
deleteTHCIpcDeleter
deleter
details
device
doesn
dynamic
goes
h
here
in
ipc
large
lie
make
managed
may
memory
more
move
must
new
one
out
point
ptr
pytorch
re
reference
referring
region
represented
s
saved
scope
should
single
src
start
static
storage
support
thc
this
time
type
unfortunate
until
used
void
which
while
will
within
zero

?
AT
ATen
Adaptive
Allow
Average
Basically
Batch
CHECK
Channels
ChannelsLast
D
DIM
DISPATCH
Falling
Fast
Given
Include
MemoryFormat
NDHWC
PYTORCH
Parameter
Pool
QEngine
QINT
QNNPACK
Quantized
Scalar
Set
TODO
TORCH
TYPES
Tensor
This
WARN
ada
adaavgpool
adaptive
add
affine
and
are
as
aten
average
avgpool
b
back
batch
being
but
c
cast
ceil
cfg
clip
computation
compute
computes
const
constraint
contig
contiguous
count
cpp
cpu
current
d
default
define
definition
device
dim
dimension
dimensions
dispatch
elements
empty
enable
end
endif
expected
false
floor
format
frame
function
gives
globalContext
has
have
height
helper
id
idx
iendD
iendH
iendW
ifdef
ih
implementation
implemented
in
include
index
input
ip
isizeD
isizeH
isizeW
isized
isizeh
isizew
istartD
istartH
istartW
istrideB
istrideC
istrideD
istrideH
istrideW
istridec
istrided
istrideh
istridew
iw
kD
kDHWr
kDHr
kDr
kH
kQUInt
kSpatialDim
kW
kernel
len
local
loop
matrix
max
memory
min
mode
native
ndhwc
nearbyint
nhwc
non
not
nullopt
number
od
oh
op
options
or
osizeD
osizeH
osizeW
osized
osizeh
osizew
out
output
over
ow
pad
padding
parallel
path
point
pointers
pool
pooling
ptr
push
pytorch
qEngine
qadaptive
qnnp
qnnpack
quantized
reserve
same
scalar
scale
set
shape
single
size
sizeB
sizeC
sizec
spatial
src
start
static
stride
strides
stub
suggest
sum
tensor
that
type
typename
underlying
val
width
with
zero

ATen
Conv
MetalConvolution
Option
Tensor
aten
bias
context
conv
cpp
d
dOpContext
dilation
groups
h
input
metal
native
ops
padding
prepack
pytorch
src
stride
weight

ATen
Apply
ArgNames
Args
BENCHMARK
Benchmark
BenchmarkState
C
CharacteristicArguments
Hardswish
MAIN
NO
PYTORCH
QNNPACK
SetBytesProcessed
SetItemsProcessed
SkipWithError
arguments
aten
b
batchSize
begin
bench
bind
bytesPerIteration
c
cast
cc
channels
characteristic
const
cpp
cpu
create
delete
device
distribution
end
endif
failed
fill
flags
generate
hardswish
hardswishOperator
ifndef
initialize
input
itemsPerIteration
iterations
lazy
max
min
mt
n
native
nc
nullptr
operator
output
point
pool
pytorch
qnnp
qnnpack
quantized
random
randomDevice
range
ref
rng
scale
setup
sizeof
src
state
static
status
stride
success
thread
u
uniform
usize
xA
zero

?
A
ARG
AT
ATen
C
CUDA
CUDAAPI
CUcontext
CUdevice
CUfunction
CUjit
CUjitInputType
CUlinkState
CUmodule
CUresult
CUstream
Can
Consider
DLL
DynamicLibrary
FORALL
In
Irregularly
LIB
LazyNVRTC
Library
Linux
MAJOR
MEMBER
MINOR
NAME
NVRTC
NvrtcProgram
NvrtcResult
Quote
REFERENCE
RETTYPE
SHORTHASH
STRINGIZE
STUB
Section
Similarly
String
The
Toolkit
VERSION
WIN
Windows
XY
Y
add
alt
an
and
as
aten
block
blockDimX
blockDimY
blockDimZ
bytes
c
cast
cfg
com
const
constexpr
cpp
create
cu
cuCtxGetCurrent
cuDevicePrimaryCtxGetState
cuGetErrorString
cuLaunchKernel
cuLinkAddData
cuLinkComplete
cuLinkCreate
cuModuleGetFunction
cuModuleLoadData
cuModuleLoadDataEx
cuModuleUnload
cuOccupancyMaxActiveBlocksPerMultiprocessor
cuda
decltype
define
defined
denote
detail
dimx
dimy
dimz
dll
docs
empty
endif
error
ex
export
extra
field
following
form
func
functions
getAltLibName
getCUDALibrary
getLibName
getLibVersion
getNVRTCLibrary
grid
gridDimX
gridDimY
gridDimZ
gte
h
hStream
have
headers
html
https
image
in
include
includeNames
index
kernel
kernelParams
later
launch
lazy
lazyNVRTC
lib
libcuda
libname
libnvrtc
library
link
load
macro
major
mem
minor
module
name
names
nullptr
numHeaders
numOptions
nvcuda
nvidia
nvrtc
nvrtcAddNameExpression
nvrtcCompileProgram
nvrtcCreateProgram
nvrtcDestroyProgram
nvrtcGetCUBIN
nvrtcGetCUBINSize
nvrtcGetErrorString
nvrtcGetLoweredName
nvrtcGetPTX
nvrtcGetPTXSize
nvrtcGetProgramLog
nvrtcGetProgramLogSize
nvrtcProgram
nvrtcResult
nvrtcVersion
option
optionValues
options
or
params
previous
prior
prog
program
pytorch
reinterpret
releases
rules
runtime
same
set
shaped
shared
sharedMemBytes
size
so
soname
src
state
static
str
stream
stub
stubs
sym
this
throw
toolkit
toolkits
ty
type
u
undef
usize
values
version
versioning
versions
void
was
will
with

ASSERT
ATen
Allocator
Amazingly
CHECK
CPU
DEBUG
DataPtr
Default
DeleterFnPtr
Device
DeviceType
DispatchKey
GetMetaAllocator
IMPLEMENTED
INTERNAL
Layout
MemoryFormat
Meta
MetaAllocator
MetaTensor
NB
NOT
ONLY
Option
ScalarType
Size
SparseMeta
Strided
TORCH
Tensor
The
alloc
allocate
allocation
allocator
always
and
aten
base
be
because
but
cpp
cpu
default
deleter
derive
device
empty
except
exerciseable
format
generic
gives
ignores
implementation
layout
lazy
memcpy
memory
meta
most
native
nbytes
no
non
not
nullptr
opt
or
pin
pointer
pytorch
r
raw
requested
resize
size
skipped
source
src
static
storage
stride
strided
supported
tensors
there
type
unsafeGetTensorImpl
us
void
whatever
which
will
work
yet
you

?
API
ATen
ArrayRef
AutoDispatchBelowADInplaceOrView
BFloat
Bool
Box
C
CHECK
ChannelsLast
ContextDeleter
Ctx
DataPtr
DeleterFnPtr
Device
False
Functions
Half
InefficientStdFunctionContext
InitializerList
IntArrayRef
MemoryFormat
NoTracerDispatchMode
Option
Provides
Remove
S
See
Self
Size
So
Special
Specified
Storage
TENSOR
TODO
TORCH
Tensor
TensorImpl
TensorMaker
TensorOptions
The
These
Utils
VALUE
`from
align
ambiguous
and
are
arguments
aten
b
be
because
bias
blob
blob`
byte
bytes
c
can
case
cast
check
clang
complex
compute
computeDispatchKey
computeStorageNbytes
computeStorageSize
conj
const
construct
context
contiguous
conv
conversion
convnd
cpp
ctx
d
default
defined
delete
deleter
device
dilation
dim
does
example
exclusive
existing
export
external
floating
fluent
forall
format
from
functions
getDeviceFromPtr
gh
globalContext
groups
guard
h
has
ident
in
index
inference
initializer
input
instead
itemsize
k
lazy
let
like
list
macro
make
makeDataPtr
makeDataPtrFromContext
makeDataPtrFromDeleter
match
mean
memory
move
mutually
needed
new
nonnegative
noop
noopDelete
not
nullopt
nullptr
off
only
opt
options
opts
or
over
overload
overloads
padding
parameters
point
precedence
ptr
pytorch
release
required
rules
s
scalar
select
set
signed
size
src
static
storage
stride
strides
takes
temp
templates
tensor
tensors
this
tracer
type
types
unbiased
unique
unsafeGetTensorImpl
unwrap
use
used
usize
value
values
var
void
weight
with
would
zeros

API
BFLOAT
BOOL
DOUBLE
FILE
FLOAT
GENERIC
HALF
IS
REAL
Scalar
TH
THLongTensor
THTensor
THTensorMath
accumulate
aten
cpp
define
defined
dimension
dims
endif
equal
generic
h
hist
histc
ifndef
in
index
indices
k
keepdim
kthvalue
lazy
maxvalue
minvalue
nbins
non
only
part
preserveReduceDimSemantics
ptrdiff
put
pytorch
r
reduce
src
static
ta
take
tb
tensor
values
void

?
ASSERT
ATen
Call
Compute
MaxPoolMicrokernelTester
MaxPoolUKernelFunction
Prepare
PyTorchU
Verify
assert
aten
begin
bind
channel
clamping
clampingParams
compute
const
cpp
cpu
default
device
distribution
end
fill
ft
generate
h
indirectX
iteration
iterations
j
k
kc
kernel
kh
kr
ks
kw
max
maxValue
maxpool
micro
microkernel
min
mr
mt
n
native
none
optimized
packed
packedKs
packedn
parameters
params
pixel
pytorch
qmax
qmin
qnnp
qnnpack
qr
quantization
quantized
random
randomDevice
ref
reference
results
rng
s
shuffle
size
sizeof
src
stride
test
tester
this
u
uniform
union
usize
vim
void
xA
xStride
y
yRef
yStride
zero

?
AT
ATen
ERROR
IsMetalAvailable
Metal
MetalCopy
MetalImplRegistrar
MetalInterface
Self
Tensor
aten
atomic
available
backend
build
const
copy
cpp
false
h
lazy
linked
load
metal
new
not
pytorch
registry
src
static
store
trait
was

@
ALL
AND
AT
ATen
BlasKernel
COMPLEX
DISPATCH
NoTranspose
Scalar
ScalarType
TYPES
Transpose
TransposeType
alpha
aten
axpy
b
beta
c
cast
const
copy
core
cpp
cpu
cpublas
dispatch
gemm
identity
incx
incy
j
k
kBFloat
kHalf
lda
ldb
ldc
m
n
native
notrans
pytorch
register
scale
src
static
stub
sum
transa
transab
transb
ty
type
val
void
y

?
@available
ATen
Clamp
HardSigmoid
MPSCNNNeuron
MPSCNNNeuronOp
MetalNeuronType
NeuronType
None
Option
Relu
Scalar
Sigmoid
Tanh
aten
cpp
enum
h
hardSigmoid
has
iOS
inf
infinity
limits
max
metal
min
native
neuron
nil
output
pytorch
range
relu
sigmoid
src
tanh
toFloat
ty
type
value
with

ATen
Lerp
LerpFnScalar
LerpFnTensor
Scalar
Tensor
aten
b
c
cpp
cpu
declare
define
dispatch
empty
end
h
kCPU
kernel
lerp
native
options
out
pytorch
result
ret
scalar
src
tensor
type
void
weight
weights

?
AT
ATen
CHECK
CPU
Cannot
D
DimVector
ERROR
Expected
False
False`
If
In
IntArrayRef
Length
NB
NOTE
ONNX
PackedSequence
Python
Scalar
See
TORCH
Tensor
TensorList
The
Then
This
To
True
We
Where
You
`
`PackedSequence`
`batch
`data`
`enforce
`grad`
`len
`lengths`
above
after
all
an
and
annotated
append
arbitrary
are
argument
as
aten
avoid
b
back
backward
batch
be
been
begin
being
below
block
but
called
can
cat
check
checkLongTensor
column
const
constructor
contiguous
copy
corresponds
could
counter
cpp
current
currseq
dec
decreasing
device
dim
dimension
dims
do
duplicate
eager
element
elements
empty
end
enforce
entry
equal
equivalent
error
every
example
exportability
first
found
fourth
from
full
going
got
grad
greater
guaranteed
happen
has
hasn
here
horizontal
imagine
implemented
in
increases
index
input
insert
into
irange
iterate
iteration
j
kCPU
kLong
least
left
len
length
lengths
level
like
lines
list
lives
long
longest
looks
loop
lower
make
max
memory
method
modified
moments
more
move
must
narrow
native
need
never
new
not
notation
now
offset
one
options
or
order
our
out
output
over
pack
packed
pad
padded
padding
pass
passed
prev
prevent
ptr
push
pytorch
real
realize
received
references
requirement
reserve
returns
reverse
rightmost
s
samples
scalar
scanning
second
select
seq
sequence
sequences
shape
shortest
should
sidestep
size
sizes`
slice
sliced
slicing
sorted
sorted`
sortedness
src
step
steps
str
swap
tensor
tensors
than
that
them
then
there
think
this
time
tmp
total
trailing
transpose
tuple
type
understand
use
valid
value
var
vec
vertical
view
wait
want
we
were
what
which
whole
will
yet
you
zeros

ATen
Apply
ArgNames
Args
BENCHMARK
Benchmark
BenchmarkState
C
GenerateSizes
MAIN
Tensor
add
aten
b
batchSize
benchmarks
c
cast
channels
const
cpp
generate
lazy
n
pytorch
rand
range
src
state
static
tensor
usize

A
ALLOC
API
ARCH
ASSERT
ATen
Aarray
B
BIT
Barray
C
CHECK
CUBLAS
CUDA
CUDACachingAllocator
CUDART
CUDASolver
CUSOLVER
Complex
CuBlasFillMode
CuBlasOperation
CuBlasSideMode
CuSolverDnHandle
CuSolverDnParams
CuSolverEigMode
CuSolverStatus
CudaDataType
Dtype
ERROR
EXECUTION
F
FAILED
GTE
GesVdjInfo
INITIALIZED
INTERNAL
INVALID
MATRIX
MISMATCH
NOT
OP
R
S
STATUS
SUCCES
SUCCESS
SUPPORTED
Scalar
SyEvjInfo
TORCH
TYPE
U
Unknown
V
VALUE
VERSION
Value
Vtype
W
aarray
all
allocate
allocator
argtypes
aten
barray
batch
batchSize
batched
batchsize
bit
buffer
bufferOnDevice
bufferOnHost
bufferSize
buffersize
bytes
c
case
cast
cfg
complex
compute
computeType
const
cpp
cuComplex
cuDoubleComplex
cuSOLVER
cublasFillMode
cuda
cudasolver
cusolver
cusolverDnCgeqrf
cusolverDnCgesvdj
cusolverDnCgesvdjBatched
cusolverDnCgetrf
cusolverDnCgetrs
cusolverDnCheevd
cusolverDnCheevj
cusolverDnCheevjBatched
cusolverDnCpotrf
cusolverDnCpotrfBatched
cusolverDnCpotrs
cusolverDnCpotrsBatched
cusolverDnCungqr
cusolverDnCunmqr
cusolverDnDgeqrf
cusolverDnDgesvdj
cusolverDnDgesvdjBatched
cusolverDnDgetrf
cusolverDnDgetrs
cusolverDnDorgqr
cusolverDnDormqr
cusolverDnDpotrf
cusolverDnDpotrfBatched
cusolverDnDpotrs
cusolverDnDpotrsBatched
cusolverDnDsyevd
cusolverDnDsyevj
cusolverDnDsyevjBatched
cusolverDnHandle
cusolverDnSgeqrf
cusolverDnSgesvdj
cusolverDnSgesvdjBatched
cusolverDnSgetrf
cusolverDnSgetrs
cusolverDnSorgqr
cusolverDnSormqr
cusolverDnSpotrf
cusolverDnSpotrfBatched
cusolverDnSpotrs
cusolverDnSpotrsBatched
cusolverDnSsyevd
cusolverDnSsyevj
cusolverDnSsyevjBatched
cusolverDnXgeqrf
cusolverDnXpotrf
cusolverDnXpotrs
cusolverDnXsyevd
cusolverDnZgeqrf
cusolverDnZgesvdj
cusolverDnZgesvdjBatched
cusolverDnZgetrf
cusolverDnZgetrs
cusolverDnZheevd
cusolverDnZheevj
cusolverDnZheevjBatched
cusolverDnZpotrf
cusolverDnZpotrfBatched
cusolverDnZpotrs
cusolverDnZpotrsBatched
cusolverDnZungqr
cusolverDnZunmqr
dA
da
dataPtr
dataTypeA
dataTypeB
datatype
default
dev
devInfo
device
doesn
double
econ
error
export
false
geqrf
gesvdj
getrf
getrs
h
handle
host
ident
implemented
in
includes
info
ipiv
jobz
k
lda
ldb
ldc
ldda
ldu
ldv
lwork
m
macro
message
n
name
not
nrhs
number
orgqr
ormqr
params
potrf
potrfBatched
potrs
potrsBatched
pytorch
reinterpret
ret
rules
side
size
sizeof
solver
src
static
status
support
switch
syevd
syevj
syevjBatched
tau
trans
type
typea
typeb
typeid
u
uplo
usize
version
void
work
workspace
workspaceInBytesOnDevice
workspaceInBytesOnHost
xgeqrf
xpotrf
xpotrs
xsyevd

?
A
ALL
AND
AT
ATen
An
Arguments
BFloat
CHECK
D
DISPATCH
Half
If
Instead
Int
Long
Number
Returns
RowwisePrune
Scalar
ScalarType
TORCH
TYPES
Tensor
The
This
True
We
an
and
are
as
aten
be
been
boolean
can
cols
compressed
considered
containing
contains
contig
contiguous
corresponding
cpp
d
dimensional
dimensions
doesn
either
elements
empty
entry
equivalent
forward
from
going
has
have
help
helper
importance
important
in
index
indicator
indices
input
introduces
kept
last
long
look
map
mapped
mapping
mask
masked
matrix
means
memcpy
native
ndimension
needs
non
not
number
one
only
operator
options
or
original
otherwise
out
particular
per
place
post
preserved
prune
pruned
pruning
ptr
pytorch
re
refer
represents
returns
row
rows
rowwise
scalar
should
size
sizeof
sparsity
src
tensor
tensors
that
this
tuple
type
up
used
value
weight
weights
well
whether
with
would
zero

ATen
CHECK
Cannot
Create
Creation
Device
Layout
MemoryFormat
Must
Note
Option
Provide
QScheme
QuantizerPtr
ScalarType
See
TORCH
Tensor
TensorFactories
TensorOptions
We
affine
an
and
argument
aten
axis
backends
based
because
better
both
change
channel
cpp
creation
delete
device
don
empty
error
explicit
explicitly
false
format
frontend
functions
hacky
has
have
in
infra
input
kPerChannelAffine
kPerTensorAffine
layout
like
ll
make
memory
merge
message
native
new
not
once
optional
options
other
output
parameters
pass
per
pin
pinned
please
point
points
provide
python
pytorch
qscheme
qtensor
quantization
quantized
quantizer
quint
ready
redundant
removal
requires
scale
scales
set
setter
size
src
stub
support
supported
tensor
that
toString
torch
type
typeMetaToScalarType
use
value
we
with
wrapper
wrong
zero
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ATen
Function
Tensor
TensorDimApply
and
apply
are
aten
break
continue
counter
cpp
dim
empty
finished
ft
func
h
has
indices
input
native
ndims
non
none
ptr
pytorch
size
src
stride
tensor
tensors
values
vim
while
zero

A
ASSERT
ATen
Adds
BatchDim
BatchDims
BatchedTensor
BatchedTensorImpl
Batching
Because
For
INTERNAL
If
Inside
Moves
Must
Note
Poor
Preconditions
PyTorch
Removes
Returns
So
TORCH
Tensor
The
This
VmapDimVector
We
When
`batched
`batched`
`dst`
`level`
`movedim`
`newly
`out
`self`
`src`
`x`
about
add
addBatchDim
always
and
any
appears
are
as
assert
aten
back
batch
batched
bdim
bdims
be
because
before
begin
block
call
can
causes
const
continue
controls
correct
correspond
corresponding
count
cpp
create
didn
dim
dim`
dimension
dimensions
dims
dims`
do
does
doesn
down
dst
end
example
exist
existing
exit
expand
expanded
exposed
false
find
followed
following
found
from
function
general
had
happen
has
have
in
index
inner
insert
inside
interact
into
keep
lambda
last
later
level
line
logical
make
makeBatched
man
maybe
maybeGetBatchedImpl
more
move
moveaxis
movedim
must
native
need
new
newly
not
np
nullptr
one
order
other
out
outer
output
pair
permutation
permute
physical
place
preserving
probably
push
put
pytorch
randn
reason
regular
remove
removed
replace
reserve
result
returned
returns
right
s
said
shape
should
single
size
so
specified
src
tensor
that
then
this
tie
torch
track
turned
undo
used
user
value
version
via
vmap
want
we
well
which
while
why
with
without
wrap
y

?
ATen
AngleImpl
CeilImpl
Complex
ConjImpl
FloorImpl
ImagImpl
MaxImpl
MinImpl
NaN
No
Note
NumPy
Op
Output
RealImpl
RoundImpl
Scalar
Self
SgnImpl
TYPE
The
This
TruncImpl
ZAbs
`
`nan`
`pi`
abs
act
and
angle
arg
as
aten
b
ceil
complex
conj
consistent
corresponds
cpp
cpu
double
dtypes
equivalent
floor
function
h
imag
isnan
its
limits
math
max
min
native
nearbyint
negative
no
non
number
numbers
operations
ops
or
other
overload
pi
positive
propagated
pytorch
quiet
real
returned
round
sgn
src
that
trait
trunc
with
z
zabs
zmath

ASSERT
ATen
Add
AddOperatorTester
Compute
Create
GE
LE
NE
NEAR
Verify
aScale
aStride
aZeroPoint
add
and
assert
aten
b
bScale
bStride
bZeroPoint
batch
batchSize
begin
bind
c
cbegin
cend
channels
cpp
cpu
create
default
delete
destroy
device
distribution
element
end
fill
generate
h
initialize
isnormal
iteration
iterations
max
min
mt
native
nc
nullptr
op
operator
point
pool
pytorch
qmax
qmin
qnnp
qnnpack
quantized
random
randomDevice
ref
reference
results
rng
scale
setup
size
src
status
stride
success
test
tester
testq
this
thread
u
uniform
usize
xA
y
yRef
yScale
yStride
yZeroPoint
zero

?
ASSERT
ATen
Call
ClampMicrokernelTester
ClampUKernelFunction
Compute
GE
LE
Prepare
PyTorchU
Verify
assert
aten
begin
bind
clamp
clamping
clampingParams
compute
const
cpp
cpu
default
device
distribution
end
false
fill
generate
h
inplace
iteration
iterations
kernel
max
micro
microkernel
min
mt
n
native
optimized
parameters
params
position
pytorch
qmax
qmin
qnnp
qnnpack
quantized
random
randomDevice
ref
reference
results
rng
src
test
tester
this
u
uniform
union
usize
xA
xData
y
yRef

ATen
Allow
CHECK
D
FUNC
IMPL
IntArrayRef
META
Non
Option
TORCH
Tensor
UpSampleNearest
aten
backward
batch
but
check
common
compute
const
cpp
cpu
d
define
dim
dimensions
dispatch
double
empty
expected
factors
full
got
grad
input
kCPU
kernel
lazy
native
nearest
not
optional
options
osize
other
out
output
pytorch
scale
scales
set
size
src
static
tensor
upsample
value
w
with
zero

A
AT
ATen
AVX
Bicubic
Bilinear
BitIndexMath
Border
C
CHECK
CONTIGUOUS
Calculate
Check
Clips
D
DISPATCH
FLOATING
FORMAT
Functions
Gather
GridSampler
GridSamplerInterpolation
GridSamplerPadding
H
If
Interpolate
LEGACY
MEMORY
Mapping
Must
NC
NCDHW
NCHW
NDHW
NHW
NOTE
Native
Nearest
No
Note
Reflection
Reflects
Scalar
See
TORCH
TYPES
Tensor
The
There
This
Unnormalizes
UpSample
Vectorized
W
Zeros
`d
`fabs`
`fmod`
`grad
`if`
`in`
above
acceptable
access
add
affect
after
align
also
an
and
are
area
argument
as
assign
assuming
aten
back
backward
batch
be
being
below
bicubic
bilinear
bit
bne
bnw
border
borders
bottom
boundary
bounded
bounds
bse
bsw
but
c
calculate
calculation
can
canUse
cast
cause
centers
checking
clip
co
coeff
coefficients
coeffs
compute
condition
considered
const
contiguous
convolution
coord
coordinate
coordinates
corner
corners
corresponding
cpp
cpu
cubic
cudnn
d
define
defined
delta
depends
device
differential
dim
dimension
dimensions
direction
directon
dispatch
does
east
edges
empty
end
enum
example
except
expected
extra
fabs
factor
fall
fallback
filled
fixed
flips
floor
fmod
forward
from
gGrid
gInp
gOut
gather
gix
giy
giz
got
grad
gradient
gradients
grid
gsizes
gstrides
h
half
has
have
height
here
high
idx
image
implementation
implemented
important
in
in`
inclusive
index
inp
input
input`
instructions
integer
interp
interpolation
into
ints
isizes
istrides
its
iy
iz
j
kCPU
kDouble
kFloat
kStrided
kernel
larger
last
layout
like
limit
loop
low
make
max
min
mode
more
mult
multipliers
n
native
ne
nearbyint
nearest
needed
neighbor
neighor
non
north
not
number
nw
offset
offsets
only
opt
options
or
ordinates
out
output
over
overflow
padding
parallel
pass
passed
pixel
pixels
pointer
points
positive
possible
problem
ptr
pytorch
refl
reflect
reflection
represented
res
returns
round
sC
sCoor
sD
sH
sN
sW
safe
same
sampler
scalar
scale
sd
se
sent
set
sh
shape
sign
signed
similarly
since
size
so
source
south
span
spatial
src
start
static
stride
strided
strides
support
supports
surfaces
sw
tensor
than
that
their
then
they
tne
tnw
top
torch
tse
tsw
tuple
twice
tx
ty
type
undefined
unnormalize
until
upsample
use
used
useful
using
val
value
values
vec
via
view
w
we
weighted
west
which
width
will
with
within
works
would
wrong
x`
y
z
zero
zeros

?
ASAN
ASSERT
ATen
Avoid
BIT
BYTE
CHECK
CPU
Currently
D
ELEM
EmbeddingPackedParamsBase
Expect
FBGEMM
FN
Fallback
For
Function
Functions
Generate
GenerateEmbeddingSpMDM
GenerateEmbeddingSpMDMNBit
GenerateEmbeddingSpMDMNBitRowWiseSparse
GenerateEmbeddingSpMDMRowWiseSparse
Get
Half
IMPL
INTERNAL
IndexType
IntrusivePtr
Invalid
LIBRARY
M
MaybeOwned
NAME
NB
NOLINT
NUM
None
OffsetType
Op
Option
PER
PackedEmbeddingBagWeight
QEmbedding
QEmbeddingBag
RATE
SELECTIVE
ScalarType
Set
Sparse
TORCH
Tensor
TorchBind
TorchClass
Using
`
`null
above
accessor
account
additional
and
arange
argument
as
aten
available
avoid
back
bag
batch
be
bias
bit
block
borrowed
but
byte
can
case
cast
combination
compressed
const
constexpr
contiguous
continue
cpp
cppcoreguidelines
cpu
create
current
d
default
device
different
dim
distance
embedding
embeddingbag
empty
end
endif
expects
explicitly
failed
fallback
false
fbgemm
fixed
fma
found
fp
freq
from
function
given
got
grad
h
has
helper
here
idx
ifdef
implementation
in
include
index
indicator
indices
input
instead
j
kByte
kFloat
kHalf
kInt
kLong
kernel
last
layout
lazy
length
lengths
less
look
lookup
lower
m
magic
mapping
memcpy
memset
mini
mode
native
need
no
non
normalize
not
nullopt
nullptr
numbers
offset
offsets
only
op
operator
options
or
out
output
overhead
owned
packed
parallel
params
passed
per
performance
pointer
positional
prefetch
pruned
ptr
push
pytorch
qembedding
qembeddingbag
quantization
quantized
rate
register
reinterpret
resize
rowwise
sample
scalar
scale
sequences
set
shape
should
since
size
sizeof
sparse
src
start
static
success
support
supports
table
tensor
than
that
then
treated
type
u
uncompressed
unused
up
use
val
value
violation
w
we
weight
weights
which
with
without
work
works
zero

?
AND
AT
ATen
BFloat
C
CHECK
CONTIGUOUS
CeilLog
D
DISPATCH
Expected
FLOATING
FORMAT
INDEX
IntArrayRef
LEGACY
LIKELY
LossNLL
MEMORY
MaybeOwned
Mean
NaN
None
Note
Option
Otherwise
Reduction
Returns
Scalar
ScalarType
See
TORCH
TYPES
Target
Tensor
VALUE
acc
accessor
accumulate
address
all
allow
as
aten
b
backward
batch
batches
be
begin
borrow
bounds
break
but
c
cascade
case
cast
check
classes
const
constexpr
contiguous
continue
cpp
cpu
cross
cur
d
defined
dim
dimensions
dims
either
element
elements
empty
end
entropy
expected
false
first
forward
frame
from
got
grad
hacky
ignore
ignored
index
input
insert
j
level
levels
like
log
loss
make
mask
match
max
maybe
mismatch
more
multi
must
n
native
nd
ndim
nll
no
not
nullptr
opt
optTypeMetaToScalarType
optional
options
or
out
output
owned
parallel
partial
power
produce
ptr
pytorch
reducing
reduction
removal
resize
result
ret
returns
scalar
see
shape
should
single
size
slice
softmax
source
src
start
static
step
sum
sums
support
supported
tensor
tensors
total
tuple
type
undefined
unmodified
utils
val
value
vec
view
w
weight
wrapper
write
zero
zeros

ALL
AND
AT
ATen
Bool
CHECK
DISPATCH
ForwardIt
Number
Option
Returns
Scalar
ScalarType
TORCH
TYPES
Tensor
The
There
Unique
`dim`
always
and
applied
are
aren
as
aten
b
back
be
begin
calculate
cannot
cast
check
consecutive
const
contiguous
copy
count
counts
cpp
cpu
current
dim
dimensions
dims
distance
due
elements
empty
end
equal
erase
exist
false
fill
first
flat
formed
ft
has
how
idx
ignore
implementation
index
indices
input
inverse
iota
iterators
kLong
last
lhs
make
many
map
more
move
narrow
native
new
none
not
nullptr
one
options
orig
output
ptr
pytorch
reserve
reshape
resize
result
rhs
save
scalar
selected
set
size
sized
so
sort
sorted
sorts
src
stack
static
tensor
tensors
than
they
tie
transpose
tuple
type
unbind
unhashable
unique
unordered
using
usize
value
vec
view
vim
well
while
zero
zeros

ATen
Design
PYTORCH
QNNPACK
QUANTIZATION
RUNTIME
Run
aten
cpp
cpu
endif
epi
ft
h
m
mm
native
no
none
op
point
pytorch
qnnpack
quantization
quantized
requantization
runtime
src
sse
sub
time
va
vim
vzp
zero

ATen
AVX
Acceptable
BufWriter
CAST
CONVERT
DATA
DEFINE
DEINTERLEAVE
DO
Display
Do
FLOAT
Formatter
From
GATHER
HEADER
IN
INT
INTERLEAVE
It
MASK
NOT
Note
Only
REQUIRE
Result
STATIC
See
THIS
TODO
This
Vectorized
Yes
add
addr
all
an
and
anonymous
any
apart
architectures
aten
avx
b
base
be
bits
buf
but
caching
can
cast
castp
castpd
castps
castsi
cfg
ch
class
classes
cols
com
combined
compilation
compile
const
context
convert
cpp
cpu
crossing
ctrl
cvttps
d
define
deinterleave
different
don
double
during
enable
entirely
epi
export
feature
fmt
functions
gather
group
grouped
h
have
header
https
ident
important
in
initializers
inlined
interleave
internal
into
io
its
kernels
label
lanes
lazy
linkage
linking
m
macro
make
mask
means
methods
mm
must
namespace
not
os
pair
pd
permute
permutevar
ps
pytorch
range
right
rules
s
same
saw
scale
set
setr
si
since
size
so
something
src
stackoverflow
static
store
stream
sub
subheaders
sufficient
support
swap
swapped
that
their
them
these
this
this?
unit
unnamed
unsigned
use
uses
val
vec
vindex
we
windows
with
works
you
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

?
A
ATen
Allow
Based
Because
Bicubic
CHECK
D
FIXME
False
Follow
For
Full
H
Here
If
Input
Interpolate
It
Must
Non
Note
Opencv
Option
Our
Scalar
Scale
SmallVector
TODO
TORCH
Tensor
The
This
True
UpSample
UpsamplingBicubic
UpsamplingBilinear
UpsamplingLinear
UpsamplingNearest
UpsamplingTrilinear
W
We
When
With
access
accordingly
affect
affected
affects
after
algorithm
align
allow
an
and
are
area
areas
as
aten
back
backward
batch
be
behavior
behaviors
bicubic
bilinear
both
bound
bounded
but
calculate
calculations
can
case
cast
center
channels
check
coefficients
coeffs
common
compute
computed
const
convolution
copy
corner
corners
cpp
cubic
current
d
declare
default
defaults
defined
depending
depth
different
dim
dimensions
dispatch
distance
distribution
doesn
double
dst
dx
empty
en
ensure
equals
exact
exactly
example
expected
factor
factors
false
flag
floating
floorf
follows
formula
from
got
grad
greater
h
has
have
height
here
https
idx
impls
in
increment
index
indices
infer
inferred
input
interp
interpolate
interpolation
its
kernel
lambda
later
linear
logic
magic
math
matter
matters
max
may
might
min
mode
models
modes
native
nbatch
nchannels
nearest
need
negative
neighbor
new
no
not
nullopt
offset
one
ones
opencv
opposite
or
org
other
output
pixel
pixels
point
preserved
provided
push
pytorch
range
ratio
real
recompute
remove
resize
ret
scale
scaled
scales
see
serialized
set
shape
should
simply
size
so
source
spatial
specify
src
starting
static
supplied
tensor
than
then
they
this
trilinear
type
unbound
upsample
upsampling
use
used
user
value
values
view
w
we
weight
weights
were
which
whole
width
wiki
wikipedia
will
with
y
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

?
ASSERT
AT
ATen
BACKWARD
C
CCE
CHECK
COMPLEX
CONFIG
CONJUGATE
Collapse
Conjugate
Construct
Constructs
Contiguous
Convert
D
DEFAULT
DFTI
DISPATCH
DISTANCE
DOUBLE
DftiCommitDescriptor
DftiComputeBackward
DftiComputeForward
DftiDescriptor
DftiSetValue
DimVector
Double
ENABLED
ERROR
EVEN
Execute
Explicit
Explicitly
FFT
FORWARD
False
Float
For
Fourier
GRAIN
Hermitian
IFFT
INPLACE
INPUT
INTERNAL
In
Inplace
Instead
IntArrayRef
LONG
MAX
MKL
MemoryFormat
MklDimVector
Multi
NOT
NOTE
NUMBER
OF
OUTPUT
PLACEMENT
Permute
R
REAL
Range
Register
SCALE
SINGLE
SIZE
STORAGE
STRIDES
Scalar
ScalarType
See
SmallVector
Sort
SpectralOps
SpectralOpsUtils
SpectralUtils
Such
Symmetry
TORCH
TRANSFORMS
TYPES
Tensor
The
This
Transform
UBSAN
Update
VALUE
We
advance
advances
agree
all
allowed
always
an
and
arch
are
as
aten
avx
b
back
batch
batched
be
become
begin
best
boolean
bytes
c
can
cannot
case
cases
cast
cbegin
cend
cfg
collapsing
come
compiled
compiling
complex
conj
conjugate
conjugated
const
contract
copy
cpp
cpu
create
d
default
descriptor
desired
details
dim
dimension
dimensional
dimensions
dims
direction
disabled
disagreement
dispatch
dist
doesn
domain
don
double
due
element
elements
empty
enabled
end
exceeds
exclude
exec
explicitly
false
fft
fill
fills
finalize
first
flag
floating
followed
following
forward
from
full
future
general
gives
h
half
halfsize
idist
idx
ignore
ignored
implementation
in
including
index
indexes
init
input
instead
integers
internal
into
inverting
involved
iota
isn
istrides
itemsize
iter
iterate
just
kDimVectorStaticSize
lambda
last
layout
let
linear
locality
loop
mask
max
maximizes
may
maybe
min
mirror
mirrored
mkl
mod
mode
more
multi
multiple
multiply
n
native
ndim
negative
new
no
non
none
norm
normalization
not
numpy
odist
offset
okay
one
onesided
only
operate
operation
options
or
order
original
ostrides
other
out
output
over
parallel
part
partition
performance
permutation
permute
place
plan
point
pointer
pointers
positives
prec
precision
ptr
pytorch
r
range
real
reference
register
remaining
reordered
representing
requested
rescale
reshape
reshaping
resize
result
root
row
scalar
scalarTypeToTypeMeta
scale
set
shape
sided
signal
simple
single
size
sizeof
slice
so
sort
sorted
sqrt
src
standard
start
static
storage
strictly
stride
strided
strides
structs
stub
super
support
switch
symmetric
symmetry
technically
tensors
then
this
though
times
toComplexType
toValueType
transform
transformed
transforms
type
types
ubsan
undefined
units
unwrap
updating
use
used
using
usize
val
value
values
void
way
we
while
will
with
zero

ATen
PyTorchQnnpAvgPoolQuantizationParams
Size
aarch
aligned
assert
assume
aten
bias
builtin
c
const
cpp
cpu
defined
dup
endif
ft
gavgpool
hi
high
input
lane
lo
low
m
max
min
n
native
neon
none
output
params
point
pytorch
qnnpack
quantization
quantized
s
scale
src
stride
u
uint
ukernel
up
vacc
vaddq
vaddw
vbias
vcombine
vcvtnq
vcvtq
vdupq
vext
vfmagic
vfmax
vfmin
vget
vim
vimagic
vinput
vld
vmax
vmaxq
vmin
vminq
vmov
vmovl
vmulq
vout
voutput
vqaddq
vqmovn
vqmovun
vreinterpret
vreinterpretq
vscale
vst
vsubq
vxinput
while
xm
zero

A
AND
ASSERT
AT
ATen
Array
BFloat
Bool
COMPILING
COMPLEX
CeilLog
Contiguous
Custom
DISPATCH
FLOATING
FOR
Half
INTEGRAL
INTERNAL
Input
Interpret
It
LOOP
LoadImpl
MIN
Move
NRows
Note
O
OUTER
Recursive
Recursively
SIZE
Scalar
ScalarType
Simultaneously
Special
SumKernel
TORCH
TYPES
TensorIterator
The
This
UNARY
Vectorized
acc
accumulate
accuracy
added
algorithm
allows
also
and
ans
are
as
aten
axes
b
base
basic
be
better
binary
break
calculate
calculates
can
case
case?
cast
char
chunk
chunking
chunks
col
const
constexpr
contiguous
cpp
cpu
defined
depth
dim
dimension
dispatch
divup
does
done
elements
endif
errors
extra
factor
fill
final
find
first
floating
groups
idx
ilp
implementation
in
includeBool
increase
index
inner
into
irange
isIntegralType
iter
j
just
k
kernel
large
larger
lazy
level
levels
like
limited
linear
load
loadu
look
loop
loss
magnitude
mask
max
means
min
minimising
more
multi
n
native
non
not
nrows
numbers
once
only
or
order
out
outer
output
over
parallel
partial
partials
pass
point
power
practice
pragma
precision
ptr
ptrs
pytorch
recursion
recursive
reduce
reduced
reduction
register
reinterpret
remaining
result
ret
rounding
row
rows
running
scalar
second
shaped
similar
simple
simplified
single
size
sizeof
so
src
st
static
step
storage
store
stride
strides
stub
sum
summed
sums
swap
there
these
this
thus
together
typename
u
unroll
until
using
usize
utils
value
values
vec
vectorized
with
without
would

ATen
Bias
CHECK
CONTIGUOUS
ConvolutionTBC
D
FORMAT
Input
LEGACY
MEMORY
Make
Note
O
TORCH
Tensor
W
Weight
addmm
are
assumes
aten
backward
batch
batchSize
be
bias
cast
channel
channels
column
conv
copy
correct
cpp
d
dBias
dI
dInput
dO
dOutput
dW
dWeight
dim
dims
empty
equal
expand
false
features
gemm
have
iShift
ilen
in
input
inputPlanes
k
kernel
kw
like
m
major
make
matrices
max
min
must
narrow
native
not
oShift
olen
options
out
output
outputPlanes
pad
pytorch
r
real
row
shapes
size
src
static
sum
sure
tbc
tensor
time
tmp
tuple
view
weight
weights
width
zeros

ASSERT
ATen
Alias
BlobSetTensor
CUDA
CUDAContext
Caffe
CreateBlob
Get
GetBlob
GetDeviceType
NOT
NetDef
PROTO
PT
Resize
RunNetOnce
Set
Sum
TRUE
Tensor
TensorImpl
Test
Workspace
add
addr
are
aten
available
b
but
c
caffe
change
const
context
copies
cpp
cpu
cuda
cudaMemcpy
cudaMemcpyDefault
d
device
direction
dumbest
empty
everywhere
from
in
input
interop
item
kCUDA
kFloat
larger
legacy
math
mutual
net
ones
op
option
output
possible
preserve
preserved
ptr
pytorch
resize
resizes
result
semantics
set
shared
simple
sizeof
smaller
src
still
storage
sum
tensor
test
type
value
view
visible
workspace
write

ARM
ATen
GAvgPoolMicrokernelTester
NEON
REQUIRES
SSE
TEST
aarch
all
any
arch
arm
aten
cc
cfg
cpp
cpu
div
eq
few
ft
gavgpool
gt
large
lt
m
max
min
mod
mp
multipass
n
native
neon
none
nr
pass
point
pytorch
qnnpack
quantized
scale
small
src
sse
stride
super
test
ukernel
up
use
usize
vim
with
xScale
xStride
xZeroPoint
xm
y
yMax
yMin
yScale
yZeroPoint
zero

?
ATen
Accumulate
Clamp
LIKELY
MM
Multiply
PYTORCH
Pack
PyTorchQnnpAddQuantizationParams
QNNP
SHUFFLE
Shift
accumulate
add
adds
and
asr
aten
b
c
cmpgt
const
cpp
cpu
cvtsi
decrement
do
epi
epu
extract
factors
ft
hi
lo
load
loadl
m
mask
max
min
mm
mulhi
mullo
multiplier
n
native
none
output
packs
packus
params
point
product
products
pytorch
qnnpack
quantization
quantized
remainder
right
round
s
saturate
setzero
shift
shuffle
si
sra
src
srl
srli
sse
storel
sub
threshold
u
ukernel
unpackhi
unpacklo
usize
va
vacc
vadd
vb
vim
vload
vrem
vremainder
vshift
vxa
vxb
vy
vzero
while
y
zero

?
ATen
Because
Device
GetDeviceArg
IMPL
LIBRARY
Option
Output
Self
Separate
TODO
TORCH
Tensor
TensorList
Tensors
ToCpu
ToDeviceOpt
We
XLA
actually
an
anywhere
arg
aten
back
break
call
calls
can
const
convenience
converting
cpp
cpu
default
defined
device
dispatch
doesn
entire
extracting
first
functions
h
has
helper
helpers
just
kill
lazy
like
list
look
m
materialized
maybe
move
nullopt
opt
optional
out
output
pos
push
pytorch
register
registrations
size
src
static
templates
tensor
tensors
this
trait
translate
type
undefined
unwrap
usize
valid
value
will
xla

ATen
BackendSelect
CPU
IMPL
LIBRARY
RegisterBackendSelect
TORCH
We
aten
backend
calls
comment
compute
correct
cpp
dispatch
eventually
factory
function
functions
generated
here
higher
key
keys
lazy
lower
m
makes
manually
ops
priority
pytorch
re
register
registrations
select
specific
src
static
templates
than
then
usual
which
with

?
API
ASSERT
AT
ATen
ArrayRef
Avoid
Backward
Bias
Both
CHECK
CONTIGUOUS
CUDNN
ChannelsLast
Check
CheckedFrom
Checking
Contiguous
Conv
ConvPlaceholders
ConvShared
Convolution
ConvolutionParams
CuBLAS
CuDNN
CudnnDataType
DATA
DOUBLE
DeviceIndex
Display
ENABLED
ERROR
Entry
FLOAT
FORMAT
False
For
Formatter
Function
FuseFrozenConvAddRelu
GPU
HALF
Here
However
If
In
Input
It
LEGACY
MEMORY
Make
MemoryFormat
NB
NC
NOTE
Not
OTOH
Option
POD
Raw
Result
Scalar
See
String
TODO
TORCH
Tensor
TensorArg
TensorGeometry
TensorGeometryArg
The
There
These
Things
This
Too
Transposed
True
Use
Used
We
Weight
Where
You
\
\n
\n\n
able
actually
add
affect
algorithmically
all
allocation
allow
allowTF
alpha
also
although
always
ambiguity
an
and
anymore
are
arg
args
argument
arguments
as
assumed
aten
b
back
backends
backward
backwards
be
because
begin
being
benchmark
benchmarkCuDNN
bias
both
break
but
c
cache
call
can
case
cfg
change
channels
char
check
checkAllSameGPU
checkAllSameType
checkDimRange
checkSameDim
checkSize
checking
checks
clients
come
computation
compute
computed
computing
const
constructor
contains
contig
contiguous
conv
convenient
conventional
convolution
convolutions
copy
correctly
count
cpp
cuDNN
cuda
cudnn
cudnnDataType
cudnnTypeToString
current
currnet
d
dataType
default
degree
depending
deprecated
design
deterministic
device
different
differently
dilation
dim
directly
distinguish
division
do
does
doesn
don
double
downside
easily
effort
empty
enable
enabled
end
enough
entire
entry
equivalent
error
errors
even
everything
example
exception
exclusive
exercise
expected
expecting
explicitly
exposed
false
few
file
fill
fmt
follow
following
format
formula
forward
forwards
framework
freedom
from
full
function
functions
general
geometry
getCudnnDataType
globalContext
got
grad
greater
groups
h
half
handle
handled
happen
happen?
harder
has
hashes
have
here
however
id
implementation
implemented
implemeted
import
in
include
indirectly
input
instead
interesting
interfaces
invokes
iterator
just
kCUDA
kernel
knew
last
layout
leaves
legacy
let
like
magic
many
mask
matches
matmul
max
may
means
memory
memset
messages
more
name
native
ndimension
necessary
need
needed
negative
net
never
nn
no
not
nullopt
omitted
one
only
operators
opt
optTypeMetaToScalarType
options
original
ostream
ostringstream
ourselves
out
output
outside
pad
padding
parameter
parameters
parametrize
params
partial
pass
performs
pin
pinned
placeholder
please
point
points
pressing
previously
principle
purposely
pybool
pytorch
raises
randn
raw
real
reasons
regular
relational
relationships
relu
reporting
repro
requires
resize
resolve
resolved
responsibility
result
reuse
reused
s
satisfy
scalar
script
seem
seems
set
shape
shared
should
shouldn
single
sites
situations
size
sized
sizeof
slightly
snippet
so
some
specify
src
ss
str
strategy
strictly
stride
strides
stringstream
such
suggest
supplied
sure
swaps
switch
synchronize
takes
tensor
tf
than
that
their
then
there
these
they
this
too
torch
torch\n
transpose
transposed
trigger
try
tuple
type
u
unambiguous
uniformity
unsupported
up
us
use
used
user
using
usize
v
value
values
variants
varies
version
very
via
vs
was
we
weight
what
which
while
with
worth
would
write
writing
xd
you
your
z
zero
zeros

AT
ATen
Allocator
CPU
CPUFixedAllocator
ERROR
This
actually
allocation
allocator
an
aten
attempting
blob
called
cast
cpp
cpu
creates
delete
exceptions
external
fake
file
fixed
free
function
h
just
lazy
libc
malloc
passed
ptrdiff
pytorch
realloc
release
resize
src
state
static
tensor
that
throws
used
view
void

API
ATen
CUDNN
EXPERIMENTAL
GTE
HAS
Instead
Macros
Note
PyTorch
The
V
VERSION
actually
all
aten
be
below
cfg
const
convenience
correctly
cpp
cuDNN
cudnn
debugging
false
h
native
not
pytorch
set
should
src
that
today
v
version
whatever
with
work

ASSERT
ATen
Allocator
DataPtr
DeleterFnPtr
Device
DeviceType
DispatchKey
Make
TRUE
Tensor
TensorImpl
TypeMeta
UndefinedTensorImpl
XLA
XLAAllocator
XLAFree
XLAMalloc
allocate
allocator
aten
base
c
cpp
deleter
device
free
intrusive
libc
make
malloc
move
no
ptr
ptrdiff
pytorch
raw
size
src
storage
tensor
test
usize
void
xla

ASSERT
ATen
Args
Assert
AutoDispatchBelowAutograd
CPU
CUDA
Dict
DispatchKey
Dispatcher
EXPECT
Error
FALSE
FN
For
IMPL
IValue
KernelFunction
LIBRARY
List
MAKE
Option
RegisterOperators
Return
String
TORCH
TRUE
Tensor
Tensor?
The
Tried
Type
all
and
any
anymore
are
arg
args
argument
arguments
assert
aten
b
backwards
based
be
because
both
boxed
boxing
but
c
call
callBoxed
callOp
callOpUnboxed
called
calls
can
cannot
captured
catch
catchAllKernel
class
compatibility
concat
concatKernel
const
core
cpp
cpu
cuda
decltype
decrement
decrementKernel
def
destructed
dict
differences
different
dispatch
does
don
dummy
dummyTensor
error
errorKernel
expect
expectCallsConcatUnboxed
expectCallsDecrement
expectCallsIncrement
expectDoesntFindKernel
expectDoesntFindOperator
expectThrows
extractDispatchKey
fail
fails
fallback
false
final
findSchema
findSchemaDifferences
first
func
function
given
gone
guts
has
have
in
increment
incrementKernel
inference
infers
input
insert
int?
isNone
jit
kernel
kernelForSchemaInference
kernelWithDictInputWithOutput
kernelWithDictInputWithoutOutput
kernelWithDictOutput
kernelWithIntInputWithOutput
kernelWithIntInputWithoutOutput
kernelWithIntListInputWithOutput
kernelWithIntListInputWithoutOutput
kernelWithIntListOutput
kernelWithIntOutput
kernelWithMultipleOutputs
kernelWithOptInputWithMultipleOutputs
kernelWithOptInputWithOutput
kernelWithOptInputWithoutOutput
kernelWithTensorInputByReferenceWithOutput
kernelWithTensorInputByReferenceWithoutOutput
kernelWithTensorInputByValueWithOutput
kernelWithTensorInputByValueWithoutOutput
kernelWithTensorListInputWithOutput
kernelWithTensorListInputWithoutOutput
kernelWithTensorListOutput
kernelWithTensorOutput
kernelWithZeroOutputs
kernelWithoutInputs
kernelWithoutOutput
kernelWithoutTensorInputs
kernels
key
lazy
library
list
m
make
matches
mismatch
mismatched
mismatching
mode
multiple
must
my
never
new
no
non
not
note
now
nullopt
number
one
only
op
operator
operators
opt
optional
options
out
output
outputs
parseSchema
place
present
pytorch
reference
registered
registering
registrar
registrars
registration
result
returning
returns
right
runs
schema
schemas
scope
second
set
should
singleton
size
specified
specifying
src
static
still
str
str?
tensor
test
text
that
then
there
this
toGenericDict
toInt
toIntVector
toString
toTensor
toTensorVector
toTypedDict
torch
tuple
type
types
unboxed
value
void
vs
was
way
whole
with
without
work
zero

ASSERT
ATen
Compute
Create
Hardsigmoid
HardsigmoidOperatorTester
NE
NEAR
Verify
and
assert
aten
batch
batchSize
begin
bind
c
channels
const
cpp
cpu
create
default
delete
destroy
device
distribution
end
fill
ft
generate
h
hardsigmoid
hardsigmoidOp
hardsigmoidX
initialize
input
inputScale
inputStride
inputZeroPoint
isnormal
iteration
iterations
max
min
mt
native
nc
none
nullptr
operator
output
outputRef
outputScale
outputStride
outputZeroPoint
point
pool
pytorch
qmax
qmin
qnnp
qnnpack
quantized
random
randomDevice
ref
reference
results
rng
scale
scaledHardsigmoidX
setup
size
src
status
stride
success
test
tester
testq
this
thread
u
uniform
usize
vim
xA
y
zero

ATen
Backend
BufWriter
C
Columns
DeprecatedTypeProperties
Display
Drop
FormatGuard
Formatter
Formatting
IosBase
MKLDNN
Proxy
Result
Self
Tensor
\n
all
aten
autograd
axis
b
base
break
built
c
ceil
channel
check
compilers
contiguous
copyfmt
core
counter
cpp
default
defaultfloat
define
defined
dense
dequantize
double
drop
endl
expMax
expMin
fabs
false
finished
firstColumn
fixed
floatfield
floor
fmt
format
formatting
fw
getIntrusivePtr
grad
guard
have
here
indent
indices
inside
intMode
io
ios
isfinite
kCPU
kDouble
kPerChannelAffine
kPerChannelAffineFloatQParams
kPerTensorAffine
lastColumn
level
linesize
log
make
matrix
meta
mkldnn
nColumnPerLine
ndimension
new
not
nullptr
number
offset
our
out
own
per
point
points
pow
print
printFormat
printIndent
printMatrix
printScale
printTensor
ptr
pytorch
qscheme
quantized
restores
row
saved
saves
scale
scales
scientific
scope
select
setprecision
setw
size
so
sparse
src
start
stream
sz
tangent
tensor
tie
toString
tuple
undefined
unsetf
usize
values
was
we
while
z
zero

AT
ATen
CHECK
CONTIGUOUS
DISPATCH
ERROR
FLOATING
FORMAT
Found
Inconsistent
Input
LEGACY
Long
MEMORY
MaxUnpooling
Output
Scalar
ScalarType
Shape
THNN
TODO
TORCH
TYPES
Tensor
Tensors
There
also
an
and
are
as
aten
backprop
backward
be
but
channels
check
contiguous
cpp
cpu
critical
d
defined
depth
dimc
dimensions
dimh
dimn
dimt
dimw
elements
empty
error
exactly
false
finalInputOffset
finalOutputOffset
forward
found
frame
from
got
grad
gradInput
gradOutput
gradient
greater
has
have
height
iH
iT
iW
ih
iheight
in
ind
index
indices
init
input
inputHeight
inputOffset
inputWidth
internal
invalid
iw
iwidth
j
k
lazy
like
match
max
maxp
must
n
nBatch
nInputOffset
nOutputOffset
nSlices
native
nbatch
ndimension
non
nslices
numBatch
numChannels
number
oH
oT
oW
oh
oheight
omp
options
or
ot
out
output
outputOffset
ow
owidth
padding
parallel
position
pragma
private
ptr
pytorch
rawIndices
rawInput
rawOutput
resize
retrieve
same
scalar
shape
should
size
slices
src
stride
strides
than
threads
three
type
unpooling
update
volumes
width
zero

?
A
API
ASSERT
AT
ATen
And
Because
But
By
C
CHECK
CI
CPU
CUDA
CUDACachingAllocator
CUDAGeneratorImpl
CUDAGraph
CUDAGraphExec
CUDART
CUDAStream
CUDAStreams
Called
Calling
Capture
CaptureId
Class
Convenience
Cuda
Default
Device
Drop
EVER
Either
Ensures
For
GetCaptureInfo
Graph
Here
How
However
INTERNAL
If
Instead
Interaction
Invalid
Juptyer
Just
MempoolId
NULL
Note
Now
OptionalDeviceGuard
PhiloxCudaState
Python
Pytorch
Pytorch?
Q
REPL
RNG
Returns
Right
STREAM
See
Self
Set
Sets
Sharing
Stackoverflow
Standalone
Stashes
Stream
TORCH
Temporary
Tensor
TensorOptions
The
Their
There
These
They
This
To
Trailing
Use
User
VERSION
WARN
We
When
Why
Wrapper
about
above
accommodates
across
after
agreed
all
allocates
allocation
allocator
allowed
already
an
and
another
any
anymore
anyway
api
argument
arguments
as
ask
asked
aten
atomic
b
ba
back
bad
be
because
began
begin
begins
behavior
being
below
best
bindings
bug
build
built
business
but
call
called
calling
calls
came
can
capture
captured
captures
case
catches
causes
cbf
certain
cfg
chance
checks
chose
clean
cleaning
collected
com
compile
compromise
conservative
constructed
consumed
consumer
cooperation
corrupted
could
count
cpp
create
created
cross
cuda
cudaDeviceSynchronize
cudaGraphDestroy
cudaGraphExecDestroy
cudaGraphInstantiate
cudaGraphLaunch
cudaStreamBeginCapture
cudaStreamCaptureModeGlobal
cudaStreamCaptureStatus
cudaStreamCaptureStatusActive
cudaStreamEndCapture
cudaStreamGetCaptureInfo
current
cycle
d
default
defined
del
deleted
depending
describes
destructor
detail
dev
device
devs
did
different
distinguish
do
docs
does
don
driver
drop
during
easy
elided
empty
end
endif
engine
ensure
epilogue
error
errors
example?
exception
exceptions
exec
expect
extension
extragraph
fail
failed
failure
false
fill
first
fix
forbid
forward
from
future
garbage
gen
generator
getCurrentCUDAStream
getDefaultCUDAGenerator
getDefaultCUDAStream
god
gracefully
graph
graphs
group
gte
guard
h
handle
hard
has
hate
have
here
how
html
https
id
idea
identifies
ignored
in
increment
instance
instances
instantiated
instead
interact
internal
into
invalid
its
itself
just
kCUDA
kLong
keep
kernel
kernels
kinds
know
launch
launches
launching
least
let
libcuda
like
likely
limitation
live
ll
lock
makes
management
matching
may
mean
means
memory
mempool
message
method
modify
moment
most
moving
multi
must
mutex
naively
native
need
needed
never
new
no
non
nonzero
not
notebook
notifyCaptureBegin
notifyCaptureDestroy
notifyCaptureEnd
now
nullopt
numerics
nvidia
occurred
offending
offset
ok
one
only
op
ops
option
options
or
order
other
our
own
owns
part
participate
particular
people
permit
philox
point
pool
possible
potentially
preceding
prefer
prevent
prevents
principle
print
private
prologue
properly
ptr
pytorch
recommended
reference
refuses
region
relaxed
replay
replayed
report
request
require
reserved
reset
restrictions
retrieved
rng
running
runtime
s
safe
same
saves
script
second
see
sense
set
sets
share
should
side
simplicity
simplify
small
so
some
soon
src
starts
state
statefulness
states
static
status
straightforwardly
stream
succeeded
successful
such
supplied
support
surefire
sync
syncs
tell
that
their
them
then
therefore
these
they
thin
think
this
this?
those
thread
through
throw
throwing
thrown
topologies
trackers
ts
u
underlying
underway
unique
unsafe
unwrap
up
us
usable
use
used
user
users
uses
uuid
val
valid
value
ve
version
warning
warnings
was
wasn
way
we
weird
were
which
while
who
wholegraph
will
with
without
won
workaround
worry
would
wrapper
write
yield
you
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

@colesbury
@soumith
AT
ATen
Ampere
CHECK
CUBLAS
CUDA
CUDABLAS
CUDNN
Comments
CreateCuBlasHandle
CuBLAS
CuBlasPoolType
CublasHandlePool
DEFAULT
DESTROY
DestroyCuBlasHandle
DeviceThreadHandlePool
FP
Guard
HANDLE
HCC
HIP
Handle
It
MATH
NO
NoTF
OP
On
PLATFORM
PoolWindow
PoolWindows
See
Sometimes
TENSOR
TF
TORCH
This
Thread
To
VERSION
Windows
allow
allowTF
allowed
already
and
architecture
are
as
aten
atexit
atomics
avoid
back
based
be
because
blas
c
calculations
can
caused
com
context
copied
cpp
create
cuBLAS
cuDNN
cublas
cublasCreate
cublasDestroy
cublasSetMathMode
cublasSetStream
cuda
cudaGetDevice
current
decided
defined
destroy
destroyed
destruction
deterministicAlgorithms
device
disable
dumb
enable
endif
fbcode
flag
from
getCurrentCUDAStream
gets
github
globalContext
handle
handles
hangs
happens
https
ifdef
implementation
in
initialization
initialized
issues
its
lazily
local
make
math
mode
myPoolWindow
newPoolWindow
not
or
ordering
pool
ptr
ptrs
pull
pytorch
releasing
reserve
reserved
rocblas
set
setting
shared
should
something
speedup
src
static
stream
terminates
tf
that
this
thread
time
type
unique
use
value
will
workaround
would

ATen
PyTorchQnnpAvgPoolQuantizationParams
add
address
adds
assert
aten
avgpool
bias
c
const
cpp
cpu
cvtepi
cvtps
cvtsi
decrement
do
epi
epu
extract
ft
hi
increment
input
k
kc
ks
lo
load
loadl
loadu
m
max
min
mm
mul
n
native
none
output
packs
packus
params
point
ps
pytorch
qnnpack
quantization
quantized
scale
setzero
si
src
srl
srli
sse
storel
u
uintptr
ukernel
unpackhi
unpacklo
up
usize
vacc
vbias
vi
vim
vout
vscale
vscaled
vshift
vsum
vxi
vzero
while
zero

ASSERT
ATen
DOUBLE
EXPECT
FLOAT
Float
INT
Integer
LONG
LOWEST
MAX
MIN
Pows
ScalarType
THROW
TorchScalarType
Torchempty
Torchpow
Torchtensor
Vals
act
actual
all
assert
aten
base
be
begin
below
cannot
cast
checks
clone
const
continue
converted
copy
cout
cpp
cubed
dbl
digits
double
doubles
end
eq
error
exp
floats
ft
has
integral
ints
inverse
isnan
item
kDouble
kFloat
kInt
kLong
lazy
like
limits
long
longs
lowest
max
min
native
neg
non
none
one
out
overflow
pow
powi
pows
precision
pytorch
rotate
runtime
scalar
scalars
shift
size
special
sqrt
squared
src
static
tensor
tensors
test
torch
type
typed
typedef
typename
using
usize
val
vals
valsDtype
value
vim
without

ALL
AND
AT
ATen
BFloat
Bool
CHECK
COMPLEX
DISPATCH
Half
Scalar
ScalarType
TORCH
TYPES
Tensor
aten
be
cannot
coalesced
converted
cpp
cpu
dense
dequantize
elements
item
local
native
nnz
ptr
pytorch
quantized
r
scalar
sparse
src
sum
type
value
values
with

?
ATen
AlignedAllocator
BENCHMARK
BenchmarkFixture
BenchmarkState
F
MAIN
NO
PYTORCH
QNNPACK
Requantization
Self
SetBytesProcessed
SetItemsProcessed
aarch
any
arch
arm
aten
b
base
begin
bench
bind
cache
cc
cfg
chrono
clear
clock
const
count
cpp
cpu
cpuinfo
d
distribution
divide
divideRoundUp
down
end
epoch
fast
fill
fp
gemmlowp
generate
initialize
input
iterations
lazy
lrintf
magic
min
mt
n
native
neon
new
not
now
output
point
precise
psimd
pytorch
qmax
qmin
qnnp
qnnpack
quantized
ref
requantization
requantize
reserve
resize
rng
round
scalar
scale
seed
set
signed
since
size
sizeof
src
sse
ssse
state
static
system
tear
time
u
uint
uniform
unsigned
up
usize
xA
zero

ASSERT
ATen
ActualSignature
AllowLegacyTypes
Args
Box
BoxedKernelFunction
BoxedKernelWrapper
C
DEBUG
DispatchKeySet
Dispatcher
FN
FuncPtr
FuncType
INTERNAL
InternalBoxedKernelFunction
It
Kernel
KernelFunction
KernelFunctor
Keys
LIKELY
Lambda
MOBILE
NDEBUG
Note
ONLY
On
OperatorHandle
OperatorKernel
Please
Plumbing
Return
See
Self
Stack
TORCH
The
This
Through
Tried
We
WrapFunctionIntoFunctor
WrapFunctionIntoRuntimeFunctor
above
always
ambiguous
an
args
argument
assert
assertion
aten
autograd
autogradother
base
be
binary
boxed
boxing
build
but
c
call
callBoxed
callUnboxedKernelFunction
callers
cannot
cast
class
compile
core
costly
cpp
created
debug
decay
deduced
defined
details
dispatch
dispatchKeySet
doesn
don
dropping
enable
endif
explicitly
fallthrough
forward
forwarding
from
func
function
functor
gated
guts
h
handle
have
ifndef
infer
inherit
instead
intentionally
into
invalid
kernel
kernelFunctor
key
ks
lambda
let
make
makeFromBoxedFunction
makeFromUnboxedFunction
makeFromUnboxedFunctor
makeFromUnboxedLambda
makeFromUnboxedRuntimeFunction
mobile
move
must
named
new
no
non
not
note
nullptr
object
op
opHandle
optimize
other
parameter
perfect
performance
pointer
ptr
pytorch
rather
re
reinterpret
require
runtime
s
same
set
size
so
specify
src
stack
stateless
static
supported
than
that
time
traits
type
typename
unboxed
uninitialized
unique
use
using
valid
value
void
want
we
which
with
would
wrap
wrapper

ATen
HARDSWISH
HardswishOperatorTester
OP
aten
batch
batchSize
cc
channels
cpp
cpu
ft
hardswish
input
inputScale
inputStride
inputZeroPoint
iterations
native
none
output
outputScale
outputStride
outputZeroPoint
point
pytorch
qmax
qmin
qnnpack
quantized
scale
small
src
stride
strided
test
testQ
u
unit
usize
vim
with
zero

ATen
Args
ArgsTuple
ClassType
Const
Fallback
For
Free
FunctionTraits
Modified
Pointers
Reference
ReturnType
S
an
and
anything
are
arg
argument
arguments
arity
aten
binary
class
com
composed
const
cpp
decltype
detail
double
element
enum
equivalent
example
figure
following
from
ft
func
function
functions
functors
h
https
in
lambda
lazy
make
member
members
none
nullary
number
operator
out
parameter
possible
public
pytorch
questions
result
s
sizeof
src
stackoverflow
static
th
that
themselves
those
traits
tuple
type
typedef
typename
types
unary
using
usize
vim
with

API
BOOL
CU
CUDA
FILE
GENERIC
IS
REAL
Scalar
THC
THCState
THCTensor
THCTensorMathPairwise
TORCH
aten
cpp
define
defined
endif
equal
ft
generic
h
ifndef
lazy
mul
none
pytorch
src
state
static
value
vim
void

AT
ATen
CHECK
Can
Class
ERROR
Onehot
TORCH
Tensor
applicable
assert
asserts
aten
avoid
back
be
but
classes
converted
could
cpp
cuda
device
empty
from
here
hot
index
infer
inference
item
kCUDA
kLong
least
max
min
must
native
negative
non
not
number
one
only
options
positive
possible
push
pytorch
rely
representation
ret
scatter
shape
should
smaller
src
sync
tensor
than
that
thrown
toLong
total
type
unsqueeze
values
vec
zeros

?
APIVitals
ASSERT
ATen
Attribute
DEFINE
TORCH
TRUE
TestAttr
TestAttr\t\t
TestValue
Testing
TestingSetVital
VITAL
WIN
\t\t
and
api
aten
basic
buffer
cout
cpp
endif
find
ifdef
multi
npos
off
ones
putenv
pytorch
rdbuf
rvalue
s
sbuf
setVital
setenv
src
str
streambuf
stringstream
test
vitals

ALL
ASSERT
ATen
At
C
CPUCachingAllocator
InitGoogleTest
MOBILE
Mobile
RUN
TESTS
TRUE
Tensor
This
WithCPUCachingAllocatorGuard
alloc
allocator
argc
argv
aten
cachine
caching
check
correctly
cpp
cpu
endif
exposed
free
guard
ifdef
inside
just
main
manual
mobile
moment
only
outside
pointer
ptr
pytorch
rand
reset
seed
should
src
test
testing
u

ASSERT
ATen
Add
Compute
Create
GE
GlobalAveragePoolingOperatorTester
LE
NE
NEAR
Verify
acc
and
assert
aten
average
batch
batchSize
begin
bind
c
channel
channels
const
cpp
cpu
create
default
delete
destroy
device
distribution
double
end
fill
ft
generate
global
globalAveragePoolingOp
h
in
index
initialize
input
inputScale
inputStride
inputZeroPoint
isnormal
iteration
iterations
j
k
max
min
mt
native
none
nullptr
nwc
operator
output
outputMax
outputMin
outputRef
outputScale
outputStride
outputZeroPoint
point
pool
pooling
pytorch
qnnp
qnnpack
quantized
random
randomDevice
ref
reference
results
rng
scale
setup
size
src
status
stride
success
test
tester
testq
this
thread
u
uniform
usize
vim
width
xA
zero

@brief
@code
@note
A
AMD
ATen
Apple
C
CPU
CUDA
DLContext
DLDataType
DLDataTypeCode
DLDevice
DLDeviceType
DLManagedTensor
DLPACK
DLTensor
Destructor
Device
Examples
For
Frameworks
GPU
GPUs
GetDataSize
IEEE
It
Metal
NULL
Notice
Number
Opaque
OpenCL
Pinned
Plain
Python
ROCm
Reserved
Tensor
The
This
Type
U
VERSION
Verilog
Vulkan
We
When
agree
alias
aligned
allocated
also
always
an
and
another
are
argument
as
aten
base
be
beginning
being
bfloat
bits
borrowing
buffer
but
byte
bytes
c
calculated
call
called
caller
can
choices
cl
common
compact
complex
const
contents
context
cpp
ctx
cudaMallocHost
current
defined
deleter
deletes
depending
destruct
destructor
destructors
device
devices
differ
dimensions
dl
dlpack
does
doesn
elements
enum
exchange
extension
facilitate
floating
follows
footprint
framework
generation
given
graphics
h
handle
hold
holds
host
id
implementation
in
index
indicating
instead
integer
intended
kDLBfloat
kDLCPU
kDLCPUPinned
kDLComplex
kDLExtDev
kDLFloat
kDLGPU
kDLInt
kDLMetal
kDLOpaqueHandle
kDLOpenCL
kDLROCM
kDLUInt
kDLVPI
kDLVulkan
keep
lanes
layout
longer
majored
manage
managed
manager
meant
mem
memory
minimal
ndim
need
needed
next
no
not
notify
number
object
offset
one
opaque
operator
options
or
original
per
point
pointer
points
provide
purposes
pytorch
quickly
reasonable
release
removed
required
reserved
resource
row
semantics
shape
should
signature
signed
simulator
size
src
static
store
strides
structure
tensor
test
testing
that
there
this
transfer
tvm
type
types
u
unsigned
used
usize
value
values
vectorized
version
void
way
well
which
will

?
AND
AT
ATen
BFloat
CHECK
CONTIGUOUS
CPU
DISPATCH
Dimname
FLOATING
FORMAT
Float
GRAIN
Half
LEGACY
LogSoftMax
MEMORY
NoNamesGuard
Option
SIZE
Scalar
ScalarType
SoftMax
TORCH
TYPES
Tensor
TensorArg
acc
and
arg
aten
b
backward
base
be
begin
c
checkSameSize
const
contiguous
conversion
converted
cpp
cpu
cuda
d
define
device
dim
dimensions
dimname
dispatch
empty
end
exp
false
ft
gI
grad
gradInput
gradOutput
grain
guard
half
has
host
idx
inner
input
internal
kCPU
kernel
lastdim
layout
less
like
log
max
maybe
memory
min
must
namedinference
names
native
ndimension
negative
non
none
not
nullopt
outer
output
parallel
pin
position
propagate
ptr
pytorch
result
scalar
size
softmax
src
stride
sum
supported
than
tmpsum
toType
type
value
view
vim
with
wrap
z

ATen
Dict
Distributed
False
Implementations
LIBRARY
None
Ops
RegisterSchema
SCHEMA
SELECTIVE
String
TORCH
Tensor
\\n\\t\\f\\v
aten
capitalize
center
chars
context
count
cpp
csrc
def
distributed
end
endswith
expandtabs
fillchar
find
gradients
id
in
index
int?
isidentifier
islower
isprintable
istitle
isupper
jit
join
keepends
lazy
ljust
located
lstrip
m
max
new
old
ops
partition
prim
pytorch
register
registrations
replace
rfind
rindex
rjust
rpartition
rsplit
rstrip
runtime
schema
separator
slice
split
splitlines
src
start
startswith
static
step
str
str?
strip
substr
tabsize
templates
title
torch
values
width
zfill

?
ATen
Args
Last
an
args
aten
base
bit
case
cast
ceil
cpp
cpu
exact
except
false
findLastSet
first
floor
forward
h
index
init
llvm
log
native
offset
powers
pytorch
set
so
src
static
step
subtract
u
utils

?
A
ALL
AND
AT
ATen
Activation
Add
B
Benchmark
Benchmarked
C
CHECK
COMPLEX
DISPATCH
ELEMENTS
ENABLED
Eigen
FLOATING
FOR
GELU
GRAIN
GeluBackwardKernel
GeluBackwardKernelImpl
GeluBackwardMKLKernelImpl
GeluKernel
GeluKernelImpl
GeluMKLKernelImpl
Internal
M
MIN
MKL
MKLCdfNorm
MKLExp
MKLMul
MOBILE
MULTI
Numbers
Pi
S
SIZE
SQRT
SQRTPI
Same
Scalar
THREADING
TODO
TORCH
TYPES
Tensor
TensorIterator
TensorIteratorBase
US
Vectorized
Y
alpha
and
another
as
aten
b
backward
based
be
begin
benchmark
benchmarking
benchmarks
beta
bit
blendv
buffer
c
can
cast
cd
cdf
cfg
cmp
compiled
computed
const
constexpr
contiguous
converts
core
cpp
cpu
d
dX
dY
deriv
dispatch
double
dy
elu
empty
end
endif
erf
exp
false
fast
filter
formula
from
gelu
glu
grad
gradNonZeroMask
grain
half
hardshrink
hardsigmoid
hardswish
hardtanh
hasMKL
ifdef
indexing
input
internal
into
iter
kAlpha
kAlphaVec
kBFloat
kBeta
kBetaVec
kMinusPointFiveVec
kNegThreeVec
kOneHalfVec
kOneSixthVec
kOneVec
kPointFiveVec
kSixVec
kThreeVec
kZeroVec
kernel
lambd
lazy
leaky
loadu
log
long
m
machine
mask
max
maximum
min
minimum
mish
mkl
mobile
module
native
needs
neg
negcoef
negiptcoef
negval
not
omp
one
only
operator
options
other
output
parallel
pdf
phone
poscoef
pt
ptr
py
python
pytorch
quick
r
register
relu
result
scalar
scale
server
shrink
side
sigmoid
sign
silu
six
sixth
size
softplus
softshrink
sqrt
src
static
store
stub
sub
tag
tanh
test
that
thread
threads
three
threshold
torchscript
type
typename
use
using
v
val
value
vdCdfNorm
vdExp
vdMul
vec
very
void
vsCdfNorm
vsExp
vsMul
which
with
y
yangxm
z
zero

A
ALL
AND
ARGS
AT
ATen
BIT
BUCK
BUILD
BYTE
Bool
Byte
C
CASE
CHAR
COMPLEX
CPU
CUDA
CUDACC
Char
ComplexDouble
ComplexFloat
DEPRECATED
DISPATCH
DON
DTYPE
DeprecatedTypeProperties
Dispatch
Double
ENABLE
ERROR
FLOATING
FUNCTION
Facebook
Float
HALF
HINT
Half
Here
INT
INTEGRAL
If
In
Index
Int
Is
It
KERNEL
Keep
Long
MACROS
MAX
MIN
Much
NAME
NB
NOT
Note
PRIVATE
Parameters
Please
PyTorch
QINT
RECORD
RecordScope
SCALAR
SCALARTYPE
SCHAR
SCOPE
SELECTIVE
SUB
Scalar
ScalarType
ScalarTypeToCPPType
Short
Should
Suppress
TEMPLATE
TH
THESE
TODO
TYPE
TYPES
The
There
This
Type
UCHAR
UNDERLYING
USING
Use
VA
VERSION
WITH
WORKAROUND
We
Workaround
You
Your
ability
about
again
aliasing
all
almost
along
also
always
an
and
answer
any
anyone
application
are
arg
argument
arithmetic
as
ask
aten
attribute
avoid
backwards
based
be
because
becomes
behaved
being
below
binary
bit
bitwidth
booleans
build
building
but
byte
c
call
can
care
case
cfg
char
collisions
compatibility
compile
compiler
complex
conditionally
cons
const
constants
constexpr
context
conveniently
correct
cpp
cut
decide
decltype
default
define
defined
definitely
deprecated
differently
dispatch
dispatching
do
doesn
don
double
dtypes
effect
enable
enabled
endif
ensure
enum
error
etc
every
exactly
execution
exist
expensive
experimental
export
fact
fails
false
family
file
floating
floats
fragments
from
function
functions
gen
generally
generate
generated
generation
generic
h
half
handle
have
here
hint
historical
historically
hurt
ident
implementation
implemented
implmeneted
important
in
include
included
including
index
instantiated
instead
integers
integral
internal
involved
just
kByte
kChar
kInt
kQInt
kQUInt
kept
kernel
lazy
like
ll
long
looks
macro
macros
many
max
maybe
method
min
missing
mod
model
mostly
must
name
needed
no
non
not
now
number
numbers
often
one
ones
only
op
operation?
operations
or
out
over
parameter
pass
passing
point
poorly
precision
private
probably
properties
provides
pt
pytorch
qint
qmax
qmin
quant
question
questions
quint
rarer
re
ready
record
reflects
returns
risk
rules
s
safe
scalar
scalarType
selected
set
should
shut
side
situations
sizeof
so
somewhat
specializations
specialized
specific
src
st
standard
statements
static
store
str
sub
such
support
supported
supported?
switch
switching
tag
that
then
this
though
time
toString
toUnderlying
trace
tracer
tracing
type
types
u
underlying
understand
unique
unless
unused
up
usage
use
used
using
var
variable
variations
verbose
via
void
want
warning
warnings
we
well
what
whether
which
width
will
with
work
working
write
you
your
~essentially

?
ASSERT
ATen
BucketizationUtils
CHECK
Device
False
INT
INTERNAL
Int
Long
MAX
ONCE
ResultTypeState
Scalar
ScalarType
So
TORCH
Tensor
This
True
TypeProperties
Undefined
WARN
adopt
and
as
aten
back
bd
be
before
binary
boundaries
but
can
check
common
const
contiguous
converting
copy
cpp
defined
depending
device
dim
dimension
dimensions
dims
due
extra
false
first
flag
got
h
have
in
input
last
less
lower
match
matched
maybe
must
native
non
number
only
operations
or
out
output
performance
please
positive
possible
pre
promotion
pytorch
raw
result
rules
s
same
scalar
searchsorted
set
should
size
src
state
stype
tensor
tensors
than
this
trim
trimmed
type
unsafeGetTensorImpl
update
use
value
we
whether
will
wrapped
wrong

?
ANDROID
ASSERT
ATen
Ac
Activation
Add
Adjust
Bias
Box
CHECK
ConvPackedParamsBase
DIM
During
For
INTERNAL
Initially
Input
IntrusivePtr
LinearPackedParamsBase
MAJOR
MSC
Mobile
NDK
NONE
NULL
Need
Option
PYTORCH
PackedConvWeightsQnnp
PackedLinearWeightsQnnp
PackedWeight
Per
PyTorch
PyTorchQnnpOperator
QNNPACK
QnnPackConvParam
QnnPackPackBMatrix
QnnPackPrePackConvWeights
QnnpackOperatorDeleter
QuantizeUint
RELU
ReluFused
Round
SPATIAL
Self
Since
SpatialDim
TORCH
Tensor
TorchList
Unsupported
VER
Weight
XZP
ac
account
across
activation
adjustment
after
all
allocate
allocated
an
and
any
apply
are
as
assume
aten
available
base
be
bias
bottom
buffer
bufferring
builtin
bytes
call
calloc
calls
can
case
cast
ceil
cfg
ch
change
changes
channel
channels
checking
consecutive
const
contig
contiguous
conv
convolution
count
cpp
cpu
cr
create
currently
d
default
delete
device
dilation
dims
divisor
does
don
double
dw
dwconv
dynamic
elements
empty
endif
enum
error
expect
failed
false
finite
format
free
function
gemm
generate
gives
graph
group
groups
h
height
idx
ifdef
in
include
inference
input
inverse
invoke
irange
isnormal
k
kCPU
kDouble
kFloat
kLong
kPerChannelAffine
kPerTensorAffine
kernel
kernels
kr
left
let
limits
log
long
make
malloc
max
memory
memset
min
mobile
mod
mode
model
move
must
native
nearbyint
nearbyintf
need
needed
needs
new
not
null
nullptr
offset
once
op
operator
or
orig
original
out
output
override
pack
packing
pad
padded
padding
params
per
point
pointer
points
pool
positive
pre
prepack
ptr
pytorch
qmax
qmin
qnnp
qnnpack
qscheme
qtype
quantization
quantize
quantized
r
range
reduce
relu
repr
requant
requantization
requantize
resize
right
round
runtime
save
scalar
scale
scales
scheme
scripted
serialized
set
should
similar
size
sizeof
so
some
src
static
step
stores
stride
structure
suggest
super
support
supported
switch
tensor
then
this
top
transpose
type
u
uint
uintptr
ukernel
unique
unpack
unreachable
unwrap
updated
us
use
usize
utils
value
void
w
we
weight
weights
which
width
with
xzp
zero
zp
zps
zu

ATen
address
aten
c
const
cpp
cpu
do
epi
ft
hi
increment
input
lo
loadu
m
mm
n
native
none
o
output
pytorch
qnnp
qnnpack
quantized
si
src
sse
storeu
u
uintptr
unpackhi
unpacklo
usize
vim
void
vw
vx
vxy
vxyzw
vy
vz
vzw
w
while
y
z
zip

ALWAYS
AND
ATen
Acceptable
BINARY
C
DEFINE
EMULATE
INLINE
MEMBER
Note
ONE
OP
See
Sizeype
UNARY
Vectorized
abs
add
alias
align
and
angle
anonymous
arange
assuming
aten
attribute
b
base
blend
blendv
c
can
case
cast
class
cmpeq
cmpge
cmpgt
cmple
cmplt
cmpne
comparision
conj
const
constexpr
count
cpp
cpu
delete
directly
enable
eq
ft
ge
generated
gt
h
header
helpers
here
idx
imag
in
include
intel
internal
intrinsics
lazy
ld
le
loadu
lt
mask
masks
max
maximum
may
memcpy
min
minimum
mul
namespace
nd
ne
neg
none
not
offset
once
operator
operator~
or
pi
pragma
private
properly
ptr
public
pytorch
real
reinterpret
returned
same
scalar
sel
set
size
sizeof
splats
src
st
static
step
store
style
sub
switch
this
tmp
type
typename
u
union
use
used
using
usize
v
value
values
vbool
vec
vecb
vim
vint
vmask
void
vsx
warning
we
will
with
work
xffffffff
xor

?
ATen
Add
Adjust
Allocate
Any
Applies
B
BYTE
Batched
Bit
BitRowwiseQuantizedSBFloat
CHECK
CPU
Convert
Corner
D
Dimensioned
E
ELEM
Embedding
EmbeddingPackedParamsBase
Example
Expect
Expected
Extend
FBGEMM
FN
FP
FloatOrHalfToFused
FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf
For
Half
IMPL
In
Index
IntrusivePtr
LIBRARY
Lookup
More
NAME
NOTE
NUM
Note
PER
Pack
PackedEmbeddingBagWeight
Prepack
Pull
Python
QEmbeddingPackWeights
QuantizedCPU
Re
SELECTIVE
Set
Since
Sizeo
TODO
TORCH
Tensor
The
This
To
TorchClass
Update
We
Xmax
Xmin
account
added
against
all
along
alongside
already
an
and
apply
are
arr
assert
astype
aten
axis
bag
bags
batch
batched
be
because
been
bias
bit
bits
byte
bytearray
bytes
calculate
case
cast
channel
choose
col
cols
column
columns
consider
const
constexpr
contig
contiguous
corner
cpp
cpu
currently
de
def
determining
dim
dimension
dimensions
due
during
either
elem
element
embedding
embeddings
empty
end
endif
examine
examining
example
expects
extra
fbgemm
first
floating
format
fp
from
function
fused
handling
has
have
helper
here
hex
idx
ifdef
in
index
input
intrusive
inverse
irange
isinf
item
just
kByte
kEpsilon
kPerChannelAffineFloatQParams
kQUInt
last
later
lazy
ll
lower
lrintf
m
make
matrix
max
maximum
memory
might
min
minimum
move
multiple
must
native
nbins
nbit
next
non
np
number
numpy
offset
once
one
only
operator
ops
optimized
options
or
order
out
output
pack
packed
packs
parallel
params
per
point
points
precisely
prepack
ptr
pytorch
qembedding
qembeddingbag
qparams
qscheme
qtype
quantization
quantize
quantized
quantizes
quint
qweight
random
range
rank
ratio
recover
register
reinterpret
replaced
represent
representation
row
rows
sample
scalar
scale
scales
scaling
set
shape
should
similar
size
sizeof
so
squeeze
src
start
static
step
store
stored
stores
suggest
support
supports
temporary
tensor
tensors
then
this
tie
torch
type
u
uint
unless
unpack
unquantized
upper
use
using
usize
value
values
vec
version
we
weight
weights
which
width
will
wise
with
work
works
would
xmax
xmin
zero
zp

ATen
LL
LONG
Limits
MAX
MKL
Since
aten
bit
calculate
cpp
different
h
lazy
linux
max
mkl
need
platforms
programmatically
pytorch
size
sizeof
src
static
varies
we
windows

APIs
ATen
Arc
ConfirmedByOwner
Default
IntrusivePtrTarget
IsOwner
NOT
Owner
OwnerName
RRef
RRefInterface
Returns
String
This
TorchScript
Ty
Type
TypePtr
WorkerId
``OwnerRRef``
``UserRRef``
abstract
an
and
aten
be
been
class
confirmed
contains
copyable
core
counting
cpp
distributed
facing
h
has
id
implement
interface
its
jit
made
messing
movable
name
only
or
owner
prevent
pytorch
reference
returns
rref
shared
src
support
this
trait
ty
type
up
user
will
worker

?
A
ATen
All
ArrayRef
Box
CHECK
Code
Constant
ConvolutionDescriptor
DIM
DataType
Descriptor
DescriptorDeleter
Descriptors
Display
Dtor
FilterDescriptor
Formatter
If
MAX
MIOPEN
MIOpen
MiOpenConvolutionDescriptor
MiOpenConvolutionMode
MiOpenCreateConvolutionDescriptor
MiOpenCreateRNNDescriptor
MiOpenCreateTensorDescriptor
MiOpenDestroyConvolutionDescriptor
MiOpenDestroyRNNDescriptor
MiOpenDestroyTensorDescriptor
MiOpenRNNDescriptor
MiOpenTensorDescriptor
Most
RNNAlgo
RNNBiasMode
RNNDescriptor
RNNDirectionMode
RNNInputMode
RNNMode
Read
Result
STR
Self
String
Subclasses
Tensor
TensorDescriptor
This
Use
\n
access
actually
aka
algorithm
and
any
are
arrays
aten
base
be
bfloat
bias
c
call
case
cast
class
client
const
construct
constructor
contiguous
cout
cpp
ctor
d
dataType
datatype
default
define
defining
desc
descriptor
destructor
dilation
dim
dimA
dimensions
dims
direction
dtor
end
ensure
error
fashion
file
filters
first
fix
fixSizeOneDimStride
fmt
function
generic
getDataType
give
groups
h
half
have
hidden
in
init
initialized
initializing
input
intend
invoke
k
kBFloat
kFloat
kHalf
layers
lazy
let
max
miopen
miopenBFloat
miopenDataType
miopenFloat
miopenGetTensorDescriptor
miopenHalf
miopenInitConvolutionNdDescriptor
miopenSetConvolutionGroupCount
miopenSetFooDescriptor
miopenSetRNNDescriptor
miopenSetTensorDescriptor
miopenStatus
miopenTensorDescriptor
miopenTensorStruct
miopenTypeToString
mode
modifies
modify
must
nbDims
ndimension
need
never
new
nullptr
one
only
or
oss
ostringstream
other
out
pad
place
pointer
points
print
product
pytorch
raw
read
reset
responsible
rnn
runtime
s
scalar
set
should
size
so
src
static
str
stride
strideA
strides
supports
switch
tensors
that
this
throw
time
type
typename
types
undef
underlying
union
unknown
unwrap
up
upscale
use
using
usize
usually
value
void
was
weights
what
will
wrapping
you
z

ATen
CPU
CUDA
CUDALoops
CppTypeToScalarType
Loops
On
TensorIterator
TensorIteratorBase
TensorIteratorDynamicCasting
This
`needs
actual
an
and
are
arg
arguments
arity
assert
aten
being
but
called
cast
casting
casting`
check
checks
compares
computation
constexpr
correct
could
cpp
cuh
currently
decltype
delay
delayed
do
done
down
dtypes
dynamic
expected
false
file
ft
func
function
h
handles
here
identity
includes
including
input
internal
into
iter
iterator
kernel
lazy
map
match
nargs
native
needed
needs
none
not
numbers
operands
output
outside
performance
pushed
pytorch
reasons
result
returns
see
src
static
that
there
this
traits
type
typename
types
used
using
utilties
value
vim
void
we
with

?
ASSERT
AT
ATen
Argument
Arguments
ArrayRef
Backwards
BufWriter
CHECK
Declaration
Display
Do
ERROR
Expected
Fast
First
Formatter
Function
FunctionSchema
HashMap
IValue
If
Make
Option
Position
Result
Schema
String
TORCH
TensorType
TypePtr
Unknown
Validate
Value
\n
accepts?
actual
alias
all
almost
and
any
are
arg
args
argument
arguments
as
aten
back
backward
begin
both
build
but
c
case
check
checkArg
child
clone
cloneWithType
common
compatibility
compatible
conservative
const
consumed
continue
contravariance
contravariant
core
corresponding
count
covariance
covariant
cpp
default
did
directly
doesn
don
duplicated
emplace
end
error
eventually
expected
false
file
files
find
findErrorInKwargs
first
fmt
format
formatTypeMismatchMsg
function
functions
generic
h
has
have
header
identical
in
info
inl
internal
io
isBackwardCompatibleWith
isSubtypeOf
isSubtypeOfExt
isSubtypeOfList
isTensor
k
keyword
kwarg
kwargs
lhs
list
look
map
match
method
mismatch
missing
more
most
msg
name
names
new
none
normalize
not
note
now
nullopt
old
only
operator
operators
out
overload
parent
parser
path
pos
position
positional
present
provide
provided
push
python
pytorch
received
remapped
repr
requires
reserve
returns
rhs
runtime
s
schema
second
seen
should
simpler
since
size
slice
specific
specified
src
start
str
subtype
subtyping
sure
symbols
than
that
their
them
there
they
this
throw
type
types
unconsumed
unknown
unless
update
used
usize
value
vararg
varret
we
were
why
windows
with
work

ATen
Also
NULL
PRIu
PyTorchQnnpOperator
PyTorchQnnpStatus
QNNPACK
Scale
Size
TanH
UINT
allocate
and
as
assume
aten
batch
be
because
below
bits
bytes
c
calloc
channels
const
cpp
cpu
create
delete
enum
error
failed
finite
flags
format
from
ft
goto
in
initialized
input
invalid
isnormal
log
lookup
lrintf
lut
malloc
max
memory
min
must
native
nc
non
none
not
number
offset
only
op
operator
out
output
parameter
params
pixel
point
positive
properly
pytorch
qnnp
qnnpack
quantized
quint
range
scale
scaled
setup
size
sizeof
src
status
stride
structure
success
supported
table
tanh
type
u
ukernel
uninitialized
unsupported
value
vim
we
with
zero
zu

ATen
CHANNEL
ChannelShuffleOperatorTester
OP
SHUFFLE
and
aten
batch
batchSize
cc
channel
cpp
cpu
four
groupChannels
groups
input
inputStride
iterations
many
native
output
outputStride
pytorch
qnnpack
quantized
shuffle
small
src
stride
test
testX
three
unit
usize
with
zero

ALL
AND
AT
ATen
COMPLEX
CrossKernel
DISPATCH
GRAIN
SIZE
Scalar
TYPES
Tensor
apply
aten
b
break
const
continue
cpp
cpu
cross
curr
dim
dims
dispatch
in
index
internal
kernel
native
parallel
position
ptr
pytorch
r
register
result
s
scalar
size
src
start
stride
stub
total
type
while

?
AND
AT
ATen
BFloat
C
CHECK
CeilLog
D
DISPATCH
Expected
FLOATING
H
INDEX
LIKELY
LossNLL
MaybeOwned
Mean
NaN
None
Note
Option
Otherwise
Reduction
Returns
Scalar
ScalarType
See
TORCH
TYPES
Target
Tensor
W
acc
accessor
accumulate
address
all
allow
as
aten
b
backward
batch
batches
be
begin
borrow
bounds
break
but
cascade
case
cast
check
classes
const
constexpr
contiguous
continue
cpp
cpu
cur
d
defined
dim
dimension
either
elem
element
elements
empty
end
expected
first
forward
frame
from
got
grad
gradout
h
hacky
have
idx
ignore
ignored
index
input
j
level
levels
like
linear
load
loss
make
map
mask
max
maybe
mismatch
must
n
native
nll
no
normalize
nullptr
numiter
only
opt
optional
options
or
out
output
outputs
owned
parallel
partial
power
produce
ptr
pytorch
reduction
removal
resize
result
returns
same
sample
scalar
see
shape
should
single
size
source
spatial
src
start
static
step
sum
sums
supported
targets
tensor
tensors
total
tuple
type
undefined
unmodified
utils
val
value
w
weight
wrapper
zero
zeros

?
ATen
Add
All
BASE
C
Clamp
Compute
Copy
FFFFF
GNUC
IGNORE
MINOR
PYTORCH
QNNP
Rounding
SHIFT
Shift
UB
UINT
WORKAROUND
\
aarch
abs
absolute
and
as
asr
assert
aten
attribute
avoid
away
base
behaviour
biased
bit
bits
but
clamped
clang
closest
computations
const
cpp
cpu
define
defined
doesn
elif
endif
factors
fp
from
ft
full
further
gcc
h
ifdef
ifndef
input
integer
lazy
major
midpoints
minor
multiplier
n
native
no
none
operations
performed
point
precise
product
pytorch
qmax
qmin
qnnpack
quantized
requantize
right
rounded
rounding
s
same
sanitize
scalar
scale
scaled
shift
sign
signed
smax
smin
src
static
support
towards
u
ubsan
undefined
unsigned
up
utils
value
values
vim
will
with
work
zero
~
~x

?
ASSERT
AT
ATen
AccumulateType
CASE
COMPLEX
Complex
CppTypeToScalarType
DEFINE
Defines
EXCEPT
Example
FORALL
HALF
INTERNAL
SCALAR
Scalar
ScalarType
TORCH
TYPES
Type
TypeNum
Unrecognized
WITH
\
acc
accscalar
accumulate
accumulation
any
as
aten
bf
case
cfg
char
const
cpp
cuda
cudacc
default
define
dst
export
false
feature
h
half
hipcc
macro
map
pytorch
rules
scalar
src
switch
trait
ty
type
u
undef
using
value

A
AT
Are
Ascending
BFloat
BUFF
BitIndexMath
Bool
Byte
CHECK
CONCAT
CReal
CUTORCH
Can
Char
ComplexDouble
ComplexFloat
DEBUG
DESC
DIMS
DimVector
Dims
Double
Due
ERROR
Extract
False
Float
Half
INT
INVALID
In
Int
IntArrayRef
It
LEN
Long
MAX
NAME
NB
NOTE
NULL
Overlapping
Returns
ScalarType
See
Short
SizeAndStride
Storage
StorageImpl
TH
THArgCheck
THAssert
THC
THCDescBuff
THCState
THCStorage
THCTensor
THCudaBFloat
THCudaBoolTensor
THCudaByteTensor
THCudaCharTensor
THCudaComplexDoubleTensor
THCudaComplexFloatTensor
THCudaDoubleTensor
THCudaHalfTensor
THCudaIntTensor
THCudaLongTensor
THCudaShortTensor
THCudaTensor
THTensor
TORCH
Tensor
The
This
True
TypeMeta
UndefinedTensor
`
`out
`tensor`
aS
all
an
and
any
are
arrays
as
ascending
aten
b
bS
be
bit
break
breaking
but
c
call
can
canUse
case
cased
cast
checking
circuits
comparator
compare
compareSizeAndStride
condition
consider
const
contained
contiguity
contiguous
correct
cpp
cuda
curDimIndex
curDimOffset
d
datapoint
default
device
dim
dimension
dimensions
dims
does
elem
element
elements
endif
exists
export
false
free
function
getDevice
getSizePtr
getStoragePtr
guarantees
guard
h
has
hpp
ident
ifdef
implementations
in
incref
index
indexable
indexable?
indexing?
indices
info
innermost
intrusive
invalid
isSame
keepdim
keywords
later
legacy
let
libc
linearId
macro
math
max
maybe
meta
more
must
n
nDimension
nDimensionLegacyAll
nDimensionLegacyNoScalars
nElement
native
nd
necessary
needs
negative
nested
new
newSize
newStride
next
nicely
no
non
nonSize
noncontiguities
not
numInputs
offset
one
only
operation
ops
optional
or
order
ordering
otherwise
out
output
overlap
overlapping
particular
possibility
preserve
preserved
ptr
ptrdiff
pytorch
qsort
range
raw
reclaim
reduce
reduced
reduction
references
resize
resizeNd
retain
rules
s
same
scalar
scalars
semantics
set
setStorage
shape
should
single
size
sizeLegacyNoScalars
sizeof
sort
sorted
special
specify
squeeze
src
state
static
storage
storageOffset
str
stride
strideLegacyNoScalars
strides
structs
sufficient
switch
tensor
tensors
than
that
thc
then
there
this
toString
type
typeMetaToScalarType
u
unexpected
unsqueeze
unwrap
use
usize
valid
view
void
vs
we
will
with
within
wrap

ATen
AddAssign
BitAndAssign
BitOrAssign
BitXorAssign
CHECK
Can
DivAssign
INDEX
Index
MemoryFormat
MulAssign
Neg
Output
Preserve
Scalar
Self
SubAssign
TORCH
Tensor
TensorOperators
The
These
add
already
and
are
assign
aten
binary
bitand
bitor
bitwise
bitxor
body
but
call
check
checked
const
constructor
copy
cpp
define
defined
diagnostics
dim
div
empty
eq
explicit
export
false
fill
forall
from
ge
gt
h
here
ident
in
index
integral
isIntegral
item
le
like
lt
macro
more
mul
ne
need
neg
not
only
op
operator
ops
or
other
properties
provide
pytorch
remainder
reverse
rhs
rules
scalar
scalars
select
so
src
static
sub
tensors
that
them
this
tilde
toLong
type
useful
user
v
we
with
xor
y
zero

?
ARM
ASSERT
ATen
Create
DeconvolutionOperatorTester
GE
LE
Make
Mode
NEAR
NULL
PrePackConvWeights
PyTorchQnnpOperator
QnnPackConvParam
Runtime
SSE
Static
TRUE
These
This
accumulators
accumulatorsMax
accumulatorsMin
adjustment
adjustmentHeight
adjustmentWidth
allocate
and
are
assert
aten
batch
batchSize
begin
bias
bind
bottom
break
buffer
bytes
c
callers
calloc
case
cast
cbegin
cend
channel
channels
clampedAccumulator
common
compute
const
conv
cpp
cpu
create
d
deconv
deconvolution
default
delete
denominator
device
dilated
dilatedKernelHeight
dilatedKernelWidth
dilation
dilationHeight
dilationWidth
dims
distribution
double
dummy
element
empty
end
error
failed
false
fill
from
ft
generate
generator
getPackedWeights
group
groupInputChannels
groupOutputChannels
groups
h
have
height
ic
initialize
input
inputHeight
inputPixelStride
inputPtr
inputWidth
inputZeroPoint
iteration
iterations
iy
just
k
kernel
kernelHeight
kernelSize
kernelWidth
kernelZeroPoints
kernels
kr
kx
ky
least
left
let
log
long
lrint
malloc
max
memset
min
mode
model
mt
multiple
native
new
nhwc
none
nullptr
oc
offset
only
op
operator
or
output
outputHeight
outputPixelStride
outputScale
outputWidth
outputZeroPoint
ox
oy
packW
padded
padding
paddingBottom
paddingHeight
paddingLeft
paddingRight
paddingTop
paddingWidth
param
params
per
pixel
point
pointer
points
pool
ptr
pull
pytorch
qmax
qmin
qnnp
qnnpack
qnnpackDeConv
quantized
random
randomDevice
real
ref
requantization
right
rng
runStatus
s
scale
scaledAccumulator
scales
setup
size
sizeof
src
static
status
stride
strideHeight
strideWidth
structure
success
switch
test
tester
testq
this
those
thread
top
transpose
type
u
uintptr
ukernel
uniform
unique
unwrap
used
usize
values
vim
void
we
width
will
xA
y
zero
zu

ASSERT
ATen
BLOCKS
But
CUDA
Can
Contents
GET
INT
INTERNAL
If
KERNEL
KernelUtils
LOOP
MAX
NUM
Round
THCUNN
THREADS
TORCH
TYPE
Use
above
after
and
are
aten
be
block
blockDim
blockIdx
blocks
but
can
cannot
case
cast
cause
common
const
constexpr
copied
cpp
cuda
d
detail
device
division
ease
except
export
file
final
from
functions
further
got
greater
grid
gridDim
h
ident
in
increment
index
input
integer
into
iteration
iterations
kernel
launch
loop
looping
macro
many
max
must
n
no
not
number
or
overflow
overflowed
per
porting
positive
prevents
pytorch
requires
rules
schedule
sm
so
specifically
src
static
stride
than
that
there
this
threadIdx
threads
too
type
up
used
value
which

ATen
Adapted
Allow
CHECK
Caffe
D
Expected
FUNC
George
IMPL
IntArrayRef
Luc
META
Non
Option
Originally
Papandreou
Pauline
TORCH
Tensor
UpSampleTrilinear
align
as
aten
backward
batch
be
begin
but
check
common
compute
const
corners
cpp
cpu
d
define
developed
dim
dimension
dimensions
dispatch
double
empty
end
expected
factors
format
from
full
got
grad
h
have
input
integers
interp
kCPU
kernel
lazy
memory
multiply
native
not
optional
options
osize
other
out
output
pytorch
same
scale
scales
set
shape
size
src
static
suggest
tensor
trilinear
upsample
util
value
w
with
zero

AND
AT
ATen
BYTE
DISPATCH
QINT
SUB
Scalar
TYPE
TYPES
Tensor
TensorIteratorConfig
UNDERLYING
When
add
all
allocated
as
aten
bit
build
cast
ceil
check
const
convert
cpp
cpu
dense
dst
elements
empty
false
format
input
iter
keep
kernel
larger
ll
memory
native
non
options
otherwise
out
output
ptr
pytorch
qdata
quant
quantized
reinterpret
repr
same
scalar
size
src
static
suggest
tensor
than
type
underlying
used
val
value
we
width

AM
ARCH
ARM
ATen
AlignedAllocator
Apply
ArgNames
Args
BENCHMARK
BM
Benchmark
BenchmarkFixture
BenchmarkState
CM
COMPUTE
CPUINFO
ColMajor
ColVectorMap
Conv
DEFINE
F
Fire
G
GEMM
GEMMLOWP
GemmArguments
GemmWithOutputPipeline
GemmlowpMultiThreadGemmContext
GemmlowpOutputPipeline
GemmlowpOutputStageBiasAddition
GemmlowpOutputStageClamp
GemmlowpOutputStageQuantizeDownInt
GemmlowpOutputStageSaturatingCastToUint
GemmlowpVectorMap
GemmlowpVectorShapeCol
K
KR
M
MAIN
MR
Make
MapOrder
MatrixMap
MobileNetV
NO
NP
NR
Op
OutputStageBiasAddition
OutputStageClamp
OutputStageQuantizeDownInt
OutputStageSaturatingCastToUint
PYTORCH
Pipeline
PyTorchQnnpConvQuantizationParams
PyTorchQnnpQ
Q
QNNPACK
QUANTIZATION
R
REGISTER
ROW
RUNTIME
RequantizationParams
RowMajor
S
SUM
ScaleByFixedPoint
Self
SetItemsProcessed
SetUp
ShuffleNetV
SqueezeNetV
TEMPLATE
ToUint
WithLhsNonzeroBitDepthParams
XZP
aRowSums
aarch
activation
addition
after
any
arch
arguments
arm
aten
b
base
begin
bench
bias
bind
block
c
cache
cast
cc
cfg
channel
chrono
clamp
clear
clock
compute
const
context
conv
count
cpp
cpu
cpuinfo
d
default
device
distribution
divide
divideRoundUp
down
end
endif
epoch
fast
fill
fixedpoint
gemm
gemmlowp
generate
group
initialize
iterations
k
kc
kcStride
kernel
kr
lazy
m
make
max
mc
min
mobile
mod
mr
mrr
mt
multiplier
n
native
nc
ncStride
neon
net
new
not
now
np
nr
nrr
offset
output
pack
params
pipeline
points
pytorch
qnnp
qnnpack
quantization
quantizationParams
quantize
quantized
random
randomDevice
range
ref
requantization
requantizationParams
reserve
resize
result
rng
round
roundUp
row
rows
s
saturating
scales
seed
set
shift
shuffle
since
single
size
sizeof
squeeze
src
sse
stage
start
state
static
stride
sum
sumrows
sums
super
swizzle
system
tear
threaded
threading
threadingContext
threads
time
tuple
type
u
uint
ukernel
uniform
up
use
usize
v
w
xA
xFE
xzp
zero

ATen
ConvParam
Drop
In
PackBMatrix
PrePackConvWeights
PyTorchQnnpOperator
PyTorchQnnpStatus
Self
Size
This
ThreadPool
allocation
and
aten
batch
be
bias
channel
channels
const
constructor
conv
convolution
cost
cpp
cpu
de
deconv
deconvolution
default
dequantization
dont
drop
dynamic
free
ft
func
h
height
hurt
include
input
kernel
linear
max
memory
min
mode
native
new
none
nullptr
output
packed
param
paying
per
performance
point
points
pytorch
qnnpack
quantization
quantized
requant
requantization
scale
scales
size
src
stride
support
threadpool
u
used
vim
void
we
weights
width
will
yet
zero

?
AND
AT
ATen
BFloat
BOUNDS
Bernoulli
Bool
CHECK
CUDA
Cauchy
Consequently
Contiguous
DISPATCH
DistributionTemplates
Due
Exponential
FLOATING
Fix
For
Generator
Geometric
Half
INTEGRAL
If
LL
LogNormal
MemoryFormat
NB
NoNamesGuard
Normal
Note
OF
ONCE
OUT
Option
Please
RNG
Random
Scalar
ScalarType
See
THC
THCTensorMathPointwise
TODO
TORCH
TYPES
Tensor
TensorIterator
The
This
To
Torchempty
Torchhalf
TypeMeta
Uniform
WARN
When
`
`from`
`random
`to`
`update
abs
actual
add
addcmul
added
advance
after
all
an
and
are
argument
arithmetics
as
assert
aten
available
avoid
b
be
become
becomes
before
bernoulli
boolean
borrowing
bounds
broadcastable
broadcasted
but
calc
can
case
cases
cast
casted
casting
casts
cauchy
changed
check
closest
complex
computation
computes
const
constant
constexpr
copies
cpp
cu
current
deprecated
deprecation
deviation
different
digits
discrete
distribution
double
elements
empty
ensure
equal
equals
error
exceed
expandable
expected
expects
exponential
export
false
features
find
fix
floating
following
found
fp
from
from`
full
function
ge
gen
generating
generator
generic
geometric
got
greater
guard
h
half
handles
happen
has
have
hence
here
ident
imaginary
implementation
in
inc
includeBool
inconsistent
incur
infer
input
instance
instead
integral
internal
into
isFloatingType
isIntegralType
issues
item
iter
kernel
lambda
left
less
like
limit
limitations
limits
log
look
lowest
macro
max
may
mean
median
min
minus
mode
moves
mul
must
n
name
namedinference
names
native
next
no
non
normal
not
now
nullary
number
numbers
only
op
operation
opt
options
or
other
out
output
outside
overlap
overwritten
performed
please
plus
point
precision
previous
previously
produces
propagate
purpose
pytorch
random
range
real
reference
release
removed
requirement
requires
reshape
reshapes
resize
resolve
result
ret
right
rules
s
same
samples
scalar
shape
sigma
signature
size
so
sqrt
src
standard
static
support
supported
tensor
th
than
that
these
third
this
throw
toString
to`
type
typeMetaToScalarType
types
u
uniform
update
use
used
uses
valid
value
values
var
variance
version
view
violates
warn
warning
was
we
which
while
will
within
won

?
AT
ATen
CHECK
Calculate
ChooseQuantizationParams
Contiguous
DISPATCH
Data
Delegate
FBGEMM
Half
Helper
It
MemoryFormat
Option
PerChannelAffine
PerChannelAffineQuantizer
PerTensorAffine
PerTensorAffineQuantizer
Preserve
QEngine
QINT
QNNPACK
QScheme
QTensor
QuantizedCPU
Quantizers
Returns
Scalar
ScalarType
Sizes
Storage
TODO
TORCH
TYPES
Tensor
TensorList
This
To
\
`
`torch
activation
add
affine
all
and
approach
aten
axis
back
backend
be
best
bins
bit
calculate
can
cast
channel
choose
clone
comparison
concrete
const
contig
contiguous
copy
cpp
cpu
cur
dense
dequantize
dequantized
device
different
doing
double
dst
element
empty
endif
ensure
equal
equalTo
error
fake
false
features
find
force
format
found
function
globalContext
greedy
have
ifdef
implemented
input
inverse
irange
item
kCPU
kFloat
kPerChannelAffine
kPerChannelAffineFloatQParams
kPerTensorAffine
keep
kernel
left
list
local
logic
loop
loss
make
max
memcmp
memcpy
memory
method
min
minimize
move
must
n
native
nbytes
nearbyint
need
non
norm
now
nudge
nullopt
offset
only
optima
optimized
optional
options
or
other
overlapping
params
per
point
points
power
preserve
ptr
push
pytorch
qEngine
qmax
qmin
qparams
qscheme
qtensor
qtensorimpl
quant
quantization
quantize
quantized
quantizer
range
ratio
reduce
reinterpret
remainder
right
row
s
same
scalar
scale
scales
set
similarly
size
sparsity
specific
sqrt
src
static
stepsize
storage
strided
strides
suggest
support
tensor
tensors
thr
toDouble
toLong
toQIntType
tries
tuple
type
types
underlying
unsafeGetTensorImpl
use
uses
utils
value
virtual
void
we
while
width
will
works
xmax
xmin
z
zero

AND
AT
ATen
CHECK
COMPLEX
CONTIGUOUS
Col
DISPATCH
FLOATING
FORMAT
Im
It
LEGACY
MEMORY
Scalar
TORCH
TYPES
Tensor
and
aten
backward
batch
batched
but
check
checks
col
contiguous
cpp
cpu
dilation
dim
elt
empty
equals
expected
false
got
grad
height
im
input
kHalf
kernel
length
like
n
native
out
output
pad
padding
plane
ptr
pytorch
resize
scalar
select
shape
size
src
stride
type
width
zero

ATen
B
CPU
Conv
FN
IMPL
LIBRARY
LinearOpContext
NAME
None
RegisterOpContextClass
SCHEMA
SELECTIVE
Scalar?
SerializationTypeConv
SerializationTypeLinearPrePack
SerializationTypeTransposeConv
TORCH
Tensor
Tensor?
TransposeConv
W
Y
aten
clamp
class
classes
const
context
conv
convolution
cpp
createConv
createLinearClampPrePackOpContext
d
dClampPrePackOpContext
dOpContext
dPrePack
dTransposeClampPrePackOpContext
def
dilation
ft
getstate
groups
internal
intrusive
lazy
linear
m
max
min
move
native
none
op
output
padding
pickle
prepack
prepacked
ptr
pytorch
setstate
src
state
static
stride
torch
transpose
unpack
vim
xnnpack

ARM
ATen
Add
Android
BitAnd
BitOr
BitXor
BlendRegs
But
Clang
Div
Due
For
However
IEEE
ISA
Implement
Implements
Index
IndexMut
Most
Mul
NB
NDK
NaN
Once
Only
Option
Ordering
Output
PartialCmp
PartialEq
Right
STL
Self
SizeType
Sleef
Step
Sub
TODO
This
ValueType
Vectorized
VectorizedFloat
Very
We
`maximum`
`minimum`
aarch
abs
acos
add
affect
align
all
an
and
android
angle
any
arange
arch
are
as
asin
asm
assert?
atan
aten
b
base
be
because
been
bitand
bitor
bitxor
blend
blendv
boolean
bug
bugs
building
c
calc
can
case
cast
ceil
cfg
cgi?id
clamp
cmp
com
compiling
conj
const
contains
convert
copysign
cos
cosh
count
cpp
cpu
currently
difference
different
div
do
done
dont
dst
either
endian
eq
erf
erfc
erfinv
etc
even
exp
expm
false
faster
faults
floor
fmadd
fmod
follow
frac
ge
github
gt
h
has
here
high
https
hypot
idx
igamma
igammac
imag
implementation
in
index
indexing
input
integer
intrinsics
isnan
issues
kimishpatel
kind
lane
le
let
lgamma
like
likely
little
llvm
loadu
log
low
lt
map
mask
max
maximum
memcpy
memset
midway
min
minimum
mobile
mod
mul
n
native
ndk
ne
nearest
need
neg
neon
new
nextafter
not
now
numbers
offers
only
operation
operator
opting
or
org
other
partial
performance
perhaps
pi
pow
pragma
problems
propagates
ptr
pytorch
qint
r
real
reasons
reciprocal
refers
reinterpret
removed
required
requires
res
round
rsqrt
s
seg
set
should
show
sign
sin
since
sinh
size
sizeof
slow
so
some
specialize
sqrt
src
static
step
store
sub
subtraction
such
support
supported
switch
tan
tanh
that
this
tmp
transcedentals
trunc
type
u
uint
unroll
unwrap
use
v
vabsq
vaddq
val
value
values
vandq
vbslq
vceqq
vcltq
vcvtq
vdivq
vdupq
vec
vectorized
veorq
version
versions
vfmaq
vgetq
vld
vmaxq
vminq
vmulq
vmvnq
vnegq
void
vorrq
vreinterpretq
vrndq
vsetq
vsqrtq
vst
vsubq
want
way
we
which
will
with
work
would
xFF
xFFFFFFFF
yet
zero
zeros

ASSERT
ATen
CHECK
Cannot
DEBUG
INTERNAL
MemoryFormat
ONLY
Option
ResizeCommon
TORCH
Tensor
This
Unsupported
`
`out
an
and
are
argument
as
aten
be
break
caused
cpp
dim
ensure
format
h
has
have
may
memory
named
names
native
optional
or
passing
please
precondition
pytorch
requires
resize
same
size
src
storage
stride
tensor
that
tried
usize
value
with

ATen
BCSRMatrix
C
FBGEMM
FN
IMPL
IntrusivePtr
K
LIBRARY
LinearPackedParamsBase
LinearPackedSerializationType
NAME
PYTORCH
PackedLinearWeight
PackedLinearWeightQnnp
QLinearUnpackWeightInt
QNNPACK
QuantizedCPU
R
SELECTIVE
TODO
TORCH
Tensor
The
TorchClass
affine
ao
aten
axis
bias
blob
block
cast
cfg
channel
cpp
cpu
device
empty
features
from
implemented
in
kCPU
kDouble
kFloat
kInt
kLong
kPerChannelAffine
kPerTensorAffine
kQInt
lazy
linear
m
make
move
native
once
orig
origin
out
output
packW
packed
params
pattern
per
points
ptr
pytorch
qint
qlinear
quantized
register
reinterpret
scale
scales
scheme
size
sparse
src
static
toType
tuple
uncomment
unpack
w
weight
zero
zp

ASSERT
ATen
Args
AutoDispatchBelowAutograd
CPU
CUDA
ConcatKernel
DecrementKernel
Default
Dict
DispatchKey
Dispatcher
EXPECT
Error
ErrorKernel
FALSE
For
IValue
IncrementKernel
KernelForSchemaInference
KernelFunc
KernelWithCache
KernelWithConstructorArg
KernelWithDictInputWithOutput
KernelWithDictInputWithoutOutput
KernelWithDictOutput
KernelWithIntInputWithOutput
KernelWithIntInputWithoutOutput
KernelWithIntListInputWithOutput
KernelWithIntListInputWithoutOutput
KernelWithIntListOutput
KernelWithIntOutput
KernelWithMultipleConstructorArgs
KernelWithMultipleOutputs
KernelWithOptInputWithMultipleOutputs
KernelWithOptInputWithOutput
KernelWithOptInputWithoutOutput
KernelWithTensorInputByReferenceWithOutput
KernelWithTensorInputByReferenceWithoutOutput
KernelWithTensorInputByValueWithOutput
KernelWithTensorInputByValueWithoutOutput
KernelWithTensorListInputWithOutput
KernelWithTensorListInputWithoutOutput
KernelWithTensorListOutput
KernelWithTensorOutput
KernelWithTupleInput
KernelWithZeroOutputs
KernelWithoutInputs
KernelWithoutOutput
KernelWithoutTensorInputs
List
OperatorKernel
Option
RegisterOperators
Return
Self
String
TRUE
Tensor
Tensor?
The
Type
all
and
any
are
arg
args
argument
arguments
assert
aten
b
backwards
base
based
be
because
boxed
boxing
c
cache
callBoxed
callOp
callOpUnboxed
called
calling
calls
can
captured
catch
catchAllKernel
class
compatibility
concat
const
constructor
core
correctly
counter
cpp
cpu
decrement
default
dict
differences
different
dispatch
does
don
dummy
dummyTensor
error
expect
expectCallsConcatUnboxed
expectCallsIncrement
expectThrows
extractDispatchKey
fail
fails
fallback
false
final
findSchema
findSchemaDifferences
first
foobar
from
functor
given
guts
has
have
in
increment
infers
initial
input
insert
int?
invoke
isNone
jit
kept
kernel
kernels
key
lazy
list
make
makeStack
matches
mismatch
mismatched
mismatching
mode
move
multiple
must
my
never
new
no
non
not
note
now
nullopt
number
offset
one
only
op
operator
operators
opt
optional
options
output
outputs
parseSchema
place
prefix
present
pytorch
reference
registered
registering
registrar
registrars
registration
result
returning
returns
right
schema
schemas
second
set
should
singleton
size
specified
specifying
src
stack
static
str
str?
tensor
test
text
that
then
there
third
this
time
toGenericDict
toInt
toIntVector
toString
toTensor
toTensorVector
toTypedDict
torch
tup
tuple
type
types
unboxed
value
void
vs
was
way
with
without
work
zero

?
AND
ASSERT
AT
ATen
AVX
Byte
C
CHECK
CPU
ChannelsLast
CheckAlmostAllZeroStrides
Compute
CppTypeToScalarType
DISPATCH
DN
Defines
Di
F
FLOATING
Float
For
GRAIN
Generic
Helper
HelperInterpBase
HelperInterpCubic
HelperInterpLinear
HelperInterpNearest
INTERNAL
INTERP
In
Index
Indices
Input
Inputs
Internally
Interp
InterpLinear
Interpolate
InterpolateNInterp
Interpolation
Linear
MSVC
MemoryFormat
NCHW
NCKHW
NCL
ND
NHWC
Nd
Nearest
Once
Option
Please
SIZE
SSE
Scalar
ScalarType
ScaleType
Single
Supports
TORCH
TYPES
Temporarily
Tensor
TensorIterator
TensorIteratorConfig
The
There
Unit
UpSampleKernel
Upsample
Using
Vectorized
We
`grad
`output`
above
add
align
all
almost
already
also
an
and
apply
are
area
argument
as
assumed
aten
automatically
back
backward
base
basic
batch
batches
be
begin
below
bicubic
bilinear
both
build
but
c
call
can
case
cases
cast
catch
channel
channels
char
check
class
coefficients
coeffs
compile
compiler
computation
computations
compute
computed
config
consistency
const
constant
constexpr
containing
context
contiguous
coordinates
copy
corners
correspond
corresponding
cpp
cpu
cubic
d
declare
depth
device
dim
dimN
dimension
dimensional
dimensions
dims
dispatch
double
dst
either
element
emplace
empty
end
equal
eval
except
expected
factor
factorize
factorized
fallback
false
first
fit
fixed
floorf
following
format
from
function
generic
given
got
grad
greater
h
handle
have
height
here
hint
id
ids
idx
ih
implementation
implemented
in
independently
index
indexr
indices
init
input
input`
internal
interp
interpolate
interpolated
interpolation
into
iter
its
iw
j
keep
kernel
lambda
last
let
like
linear
loadu
loop
lot
max
maximum
memory
method
methods
min
mode
modes
more
multiplied
n
native
nbatch
nd
ndim
ndims
nearest
need
neighbor
new
non
not
number
od
offset
oh
one
ones
operations
opt
optimization
optimizations
optimize
optional
or
original
oshape
otherwise
out
output
ow
parallel
per
pixel
possible
practice
ptr
put
pytorch
real
recursion
recursive
recursively
refer
register
rely
remove
reshape
reshaped
restrided
s
same
scalar
scale
scales
see
separable
separately
shape
shift
should
simply
single
size
sizeof
slice
slowdown
so
source
spatial
special
specialize
specific
src
static
step
store
stride
strided
strides
structs
structure
such
supports
tensor
tensors
than
that
then
these
this
those
threads
time
treat
trilinear
type
typename
typically
u
uint
unexpected
unrolling
up
upsample
upsampling
use
used
uses
using
value
values
vec
vectorized
verify
w
way
we
weight
weights
whether
which
width
with
wt
wts
zero

ALWAYS
AND
ATen
Acceptable
BINARY
C
DEFINE
EMULATE
INLINE
MEMBER
Note
ONE
OP
See
Sizeype
UNARY
Vectorized
abs
add
alias
align
and
angle
anonymous
arange
assuming
aten
attribute
b
base
blend
blendv
c
can
case
cast
class
cmpeq
cmpge
cmpgt
cmple
cmplt
cmpne
comparision
conj
const
constexpr
count
cpp
cpu
delete
directly
enable
eq
ft
ge
generated
gt
h
header
helpers
here
idx
imag
in
include
intel
internal
intrinsics
lazy
ld
le
loadu
lt
mask
masks
max
maximum
may
memcpy
min
minimum
mul
namespace
nd
ne
neg
none
not
offset
once
operator
operator~
or
pi
pragma
private
properly
ptr
public
pytorch
real
reinterpret
returned
same
scalar
sel
set
size
sizeof
splats
src
st
static
step
store
style
sub
switch
this
tmp
type
typename
u
union
use
used
using
usize
v
value
values
vbool
vec
vecb
vim
vint
vmask
void
vsx
warning
we
will
with
work
xffff
xor

ADD
ALIGN
ATen
Add
After
Args
B
BEGIN
BHI
CMP
CODE
CSEL
Check
Compute
DIRECTIVES
DUP
ELF
END
EOR
EXT
Each
FADD
FMUL
FUNCTION
First
GNU
HS
IGNORE
In
It
LD
LDP
LDR
LO
LS
LSL
Load
MOV
Mul
NE
Now
R
RET
S
SCVTF
SMLAL
SRI
ST
STP
SUB
SUBS
Shift
Similarly
Store
TOS
TRANSPOSE
TRN
Then
This
Thus
To
USUBL
We
When
\
\temp
\vin
aarch
accumulators
address
after
align
all
and
are
aten
avoid
b
back
base
be
because
being
beyond
block
blocks
buffer
byte
bytes
c
can
ch
channel
const
contain
conv
cpp
cpu
create
d
doing
dq
dynamic
element
endif
endm
extra
fetched
first
frame
from
ft
function
gemm
h
have
id
ids
ifdef
ifndef
implementation
in
inbetween
increment
index
indx
iteration
just
k
lazy
load
local
loop
macro
make
may
mr
mrxnr
multiplier
n
native
nd
needed
needs
neon
never
next
none
nonzero
note
nr
nrs
nrxmr
nth
number
offset
one
other
out
output
overlap
packed
packedA
params
parts
passed
per
point
pointer
points
processed
processing
produce
produces
progbits
ptr
pytorch
qnnp
qnnpack
quantization
quantized
r
restrict
result
row
s
say
section
should
so
sp
sparse
src
st
stack
static
stored
storing
stride
sure
temp
that
this
tranpose
tranposed
transpose
u
ukernel
union
using
usize
v
value
values
variables
via
vim
vin
void
w
we
which
will
with
zero

A
ASSERT
ATen
Adapter
All
Arguments
Buffer
CHECK
CREATE
Command
CommandBuffer
Construction
DEBUG
DELETER
DEVICE
Descriptor
DescriptorSet
Device
Do
Drop
EXTENSION
Error
Factor
Failed
Forward
GPU
Gpu
Handle
INFO
INTERNAL
IndexSequence
Indices
Invalid
KHR
Layout
NAME
ONLY
Object
Only
PORTABILITY
Pipeline
PyTorch
QUEUE
Resource
STRUCTURE
SUBSET
Select
Self
Set
Shader
ShaderDescriptor
ShaderLayoutSignature
ShaderWorkGroup
Signature
TORCH
TYPE
Tensor
The
This
Unknown
VK
VUID
VkDevice
VkDeviceCreateInfo
VkDeviceQueueCreateInfo
VkExtensionProperties
VkPhysicalDevice
VkQueue
Vulkan
VulkanImpl
VulkanImplInterface
VulkanImplRegistrar
WARN
WorkGroup
acquire
adapter
affinity
all
allocate
an
and
api
are
arguments
around
as
associated
aten
available
back
base
be
bind
bloat
break
buffer
but
cache
cast
catch
char
com
command
compute
consequential
const
constexpr
context
copy
count
cpp
create
currently
debugging
declaration
decltype
descriptor
destruction
destructor
device
dispatch
doc
does
drop
enabled
endif
epilogue
exception
expensive
explicit
extension
extensionName
extensions
family
first
flush
forward
function
global
gpu
group
h
hack
handle
holds
html
https
ifdef
in
independent
index
info
initialize
its
layout
lazy
local
logical
lunarg
mac
make
matters
members
minimize
more
move
multi
native
need
new
not
nullptr
object
one
only
onto
ops
or
order
our
out
pProperties
parameter
performance
performant
pertains
physical
pipeline
pool
portability
precursor
priorities
prologue
properties
ptr
purge
push
pytorch
queue
raised
relevant
requested
resource
retrieve
runtime
select
sequence
set
shader
short
signature
simply
size
solution
src
state
static
strcmp
subset
support
technically
tensor
tensors
term
this
trio
try
u
unique
use
user
usize
view
vkCreateDevice
vkEnumerateDeviceExtensionProperties
vkGetDeviceQueue
vkQueueWaitIdle
vkspec
void
vulkan
way
we
were
what
with
work

?
ARE
ATen
Absolute
Alias
All
An
Args
C
CHECK
Checks
Complex
Copies
Dispatch
Distribution
Expected
FIXME
FUNC
For
Generator
HALF
HELPERS
Here
IMPL
If
In
Kernel
M
META
Missing
NB
NOT
NOTE
Negation
Note
NumPy
OBLIGED
Option
OutImpl
PI
Please
QUARTER
Runs
Scalar
Stub
THESE
TO
TORCH
Tensor
TensorIterator
TensorIteratorBase
TensorIteratorConfig
The
These
This
Unary
UnaryFn
UnaryFnWithScalar
UnaryOps
Unlike
XLA
YOU
`
`conj
`copy
`logical
`~`
abs
absolute
acos
acosh
actual
actually
add
affected
alias
all
allows
alternate
always
an
and
angle
arange
arccos
arccosh
arcsin
arcsinh
arctan
arctanh
are
args
as
asin
asinh
atan
atanh
aten
b
be
because
bernoulli
bit
bits
bitwise
boolean
both
build
but
c
calc
calls
can
canCast
case
cast
cauchy
ceil
check
clamp
class
clone
complex
conj
conjugate
conjugation
consistent
const
constexpr
contiguous
copy
copying
corresponding
cos
cosh
cpp
create
declare
default
define
defined
deg
desired
device
devices
different
differently
digamma
dim
dispatch
does
don
done
double
dtypes
elegant
elements
empty
entr
eps
equal
equivalent
erf
erfc
erfinv
everything
example
exp
expected
expects
expit
explicitly
expm
exponent
exponential
export
false
fat
fill
fix
flat
flexible
floating
floor
follows
foreseeable
format
former
found
frac
frexp
from
full
func
function
functional
functions
future
gammaln
gammanln
geometric
given
got
greater
h
handled
has
have
helper
ident
imag
imaginary
implemented
implementing
implements
in
include
includeBool
inf
infinite
input
instead
integer
integral
intended
into
invert
isFloatingType
isIntegralType
item
iter
its
just
kBool
kInt
kaiser
kernel
kind
latter
layout
lazy
least
lgamma
like
log
logical
logit
look
macro
make
makes
mantissa
mask
mathematically
maybe
mem
memory
meta
might
more
most
mul
multinomial
must
mvlgamma
n
namedinference
names
nan
native
ndtr
neg
negative
never
new
no
non
normal
not
now
number
numbers
once
only
op
operation
operations
operator
operators
ops
opt
optTypeMetaToScalarType
options
or
other
otherwise
out
output
outputs
overlap
part
passed
pattern
physical
pi
pinned
place
please
point
polygamma
pos
positive
pow
preprocessing
preserve
produce
promotes
promotion
propagate
psi
pytorch
rad
random
range
re
real
reciprocal
recursion
reduce
redundant
remove
replacement
resize
resolve
result
results
returns
round
rsqrt
rules
running
s
safe
same
scalar
scalarTypeToTypeMeta
scipy
select
sense
set
sgn
should
sigmoid
sign
signature
signbit
simple
sin
sinc
since
sinh
some
something
source
special
specialized
sqrt
square
src
static
structured
stub
sub
suggest
sum
support
supported
supports
tan
tanh
tensor
tensors
than
that
them
then
these
this
through
toValueType
torch
trigamma
trunc
try
trying
tuple
type
types
typical
u
ultimately
unary
uniform
unique
unsqueeze
updated
use
using
usual
value
variant
version
view
void
we
which
window
with
won
work
wrapped
write
writing
you
your
zero

AN
AND
ANY
API
APIs
ASSIGN
AT
ATen
ATenNVRTC
BASE
C
CALL
CPP
CPU
CREATE
CUDA
CUDAHooks
DIRECTLY
DRIVER
ERROR
FORALL
FUNCTION
GPU
HCC
HIP
HIPOCCUPANCYMAXACTIVEBLOCKSPERMULTIPROCESSOR
INSTEAD
IS
IT
If
JIT
LazyNVRTC
MEMBER
NOTE
NVRTC
Normal
OF
PLATFORM
ROCm
Stub
TO
TORCH
TRY
The
To
VERSION
While
above
accomplish
accordingly
add
all
and
apis
as
aten
base
be
because
below
both
build
but
caffe
can
certain
cfg
compiled
contains
cpp
cu
cuCtxGetCurrent
cuDevicePrimaryCtxGetState
cuGetErrorString
cuLaunchKernel
cuLinkAddData
cuLinkComplete
cuLinkCreate
cuLoadModule
cuModuleGetFunction
cuModuleLoadData
cuModuleLoadDataEx
cuModuleUnload
cuOccupancyMaxActiveBlocksPerMultiprocessor
cuda
decltype
define
detail
directly
does
doesn
driver
dynamic
edit
either
even
export
extern
forall
former
from
full
function
functional
getCUDAHooks
getDeviceWithPrimaryContext
getNVRTC
globalContext
gte
h
have
hipGetErrorString
hipModuleOccupancyMaxActiveBlocksPerMultiprocessor
hipOccupancyMaxActiveBlocksPerMultiprocessor
hipoccupancymaxactiveblockspermultiprocessor
ident
in
initialized
installed
later
lazily
lazy
libcaffe
libcuda
libnvrtc
libraries
library
link
list
load
loading
long
lt
machines
macro
macros
maps
memory
missing
name
need
needed
new
non
not
nvrtc
nvrtcAddNameExpression
nvrtcCompileProgram
nvrtcCreateProgram
nvrtcDestroyProgram
nvrtcGetCUBIN
nvrtcGetCUBINSize
nvrtcGetErrorString
nvrtcGetLoweredName
nvrtcGetPTX
nvrtcGetPTXSize
nvrtcGetProgramLog
nvrtcGetProgramLogSize
nvrtcVersion
one
operations
or
our
out
pin
pointers
provides
pytorch
renamed
require
rules
runtime
s
sometimes
specifically
src
static
strips
stub
supported
tensor
that
they
this
torch
typedef
unsupported
use
used
uses
via
want
we
which
work
yet

ATen
PyTorchQnnpOperator
Size
aten
batch
buffer
c
channels
char
clamped
components
const
conv
cpp
cpu
d
deconv
dilation
divide
divisor
doz
dwconv
ft
fxdiv
group
groups
h
height
image
index
indirection
init
input
kernel
left
maxpool
min
native
none
offset
op
output
padding
pixel
pointer
pooling
pytorch
qnnp
qnnpack
quantized
quotient
remainder
result
size
src
start
step
stride
tile
tiled
top
usize
vim
void
width
y
zero

?
A
ASSERT
ATen
Activation
Allocate
Arguments
B
Batch
C
CHANNEL
CHECK
CPU
Calling
Can
ChannelsLast
Conv
ConvDimChecks
ConvPackedParamsBase
ConvTranspose
ConvertToChannelsLast
D
DIM
DoNothing
Expected
FBGEMM
FUSED
Fix
For
ForBC
Get
GetBiasData
GetQuantizationParams
Given
H
However
IMPL
INTERNAL
Ideally
If
Input
IntrinsicsGuide
IntrusivePtr
K
Kimish
LIBRARY
M
MAX
MakeConvOutputShape
MakeDeConvOutputShape
MakeEmptyAffineQuantizedChannelsLast
MakeFbgemmConvParam
MemoryFormat
NAME
NCDHW
NCHW
NHWC
NULL
Number
ONCE
OUT
On
Original
Output
PYTORCH
Packed
PackedConvWeight
PackedConvWeightsQnnp
PrePackConvWeights
QConv
QConvInt
QNNPACK
QuantizationGranularity
Quantized
QuantizedCPU
REASONABLE
RELU
Re
ReLU
ReQuantizeOutput
SELECTIVE
SPATIAL
See
Set
SmallVector
So
SpatialDim
SpatialDimPlusTwo
Still
String
Such
TENSOR
TODO
TORCH
Tensor
The
This
To
TorchList
Unknown
Update
W
WARN
We
Weight
Weights
Y
Your
act
activation
activationLimits
activations
actual
actually
adjust
affine
after
again
all
allocating
allocation
always
an
and
apply
are
arguments
as
assertion
aten
axis
b
backend
backward
based
batch
be
been
begin
behavior
below
beyound
bias
buffer
but
calculate
can
cannot
cast
cfg
ch
change
channel
channels
check
checks
col
com
compatibility
compatible
compute
consistent
const
contig
contiguous
conv
convolution
cpp
cpu
d
dInt
dSqueezeDim
dTensor
de
decltype
deconv
device
dilation
dim
dimension
dimensions
dims
do
does
doing
done
double
elements
empty
end
epi
example
expand
expected
failed
false
fbgemm
fbgemmConv
fbgemmSupportedCPU
feature
first
fit
force
format
fp
freed
from
front
full
func
fused
generate
getOutputChannels
getPackedWeights
github
globalContext
got
greater
groups
h
has
have
here
holding
https
id
idx
image
implementation
improving
in
input
inputChannels
instead
instruction
intel
into
intrusive
irange
issues
just
kCPU
kConv
kFloat
kInt
kNoOpObj
kPerChannelAffine
kPerTensorAffine
kQInt
kQUInt
kReasonableMaxDim
kReluFused
kSpatialDim
kernel
kernels
landingpage
lands
large
last
layout
lazy
let
m
maddubs
maintaining
make
match
matrix
max
maximum
memory
memset
might
min
mind
mini
mm
mobile
model
module
more
multiplier
multiply
must
name
native
ndimension
necessary
new
nhwc
not
now
nullopt
nullptr
number
obj
offset
offsets
once
only
op
operates
operator
opportunistically
ops
options
or
orig
original
out
output
outputChannels
overflows
owned
pack
packB
packed
packing
pad
padding
parallel
param
params
passed
per
perf
pin
please
point
pointer
points
possible
pre
preserve
proc
pthreadpool
ptr
pytorch
qbias
qconv
qint
qnnp
qnnpack
qnnpackConv
qnnpackDeConv
quant
quantization
quantize
quantized
quantizing
quint
reasonable
regardless
reinterpret
release
releaseWeightsWhenPrepacking
relu
remove
removed
replaced
requant
requantization
requantize
reset
resetting
resize
result
returned
robust
row
same
sanity
saturates
scale
scales
scheme
second
semantic
shape
should
sites
size
so
software
squeeze
src
static
status
stride
success
support
supports
task
tasks
tensor
text
than
them
then
these
this
thread
threads
throw
thus
times
torch
transpose
u
uint
unique
unpack
unsqueeze
unwrap
upconversions
update
use
used
uses
usize
utils
value
values
vectors
vpmaddubsw
w
want
was
we
weight
weights
will
with
written
wt
your
zero
zeros
zp

AT
ATen
Add
BitAnd
BitOr
BitXor
C
CMP
Comparison
Complex
Div
ERROR
EXC
Ensure
Exploit
F
FROUND
GT
INT
Implement
LT
MM
Most
Mul
NEAREST
NEQ
NO
NaN
OQ
Output
PartialEq
Q
See
Self
SizeType
Sleef
Step
Sub
TO
UNORD
UQ
V
ValueType
Vectorized
VectorizedComplexFloat
We
ZERO
`O`
`Q`
ab
abi
abs
ac
acos
ad
add
ai
align
all
an
and
angle
any
arange
arrays
asin
atan
aten
avx
b
base
bc
bd
be
because
bi
bitand
bitor
bitxor
blend
blendv
c
can
case
cast
castsi
ceil
cfg
change
cmp
com
compile
compiled
complex
conj
const
convert
cos
cosh
count
cpp
cpu
d
details
di
div
do
does
epi
eq
erf
erfc
exp
expf
expm
fact
false
feature
floor
gcc
github
h
hadd
half
here
hsub
https
hypot
igamma
igammac
im
imag
improve
index
initialize
instruction
instructions
isnan
issues
iz
let
ln
load
loadu
log
loop
m
map
mask
max
maximum
memcpy
memory
min
minimum
mm
mod
more
mul
multiplication
ne
neg
new
nextafter
not
number
numbers
one
ones
op
operand
operator
ops
or
os
other
output
performance
permute
pow
predicate
ps
ptr
pytorch
raise
re
real
reciprocal
reinterpret
ret
root
round
rsqrt
set
setr
setzero
sgn
sign
sin
sincosf
sinh
size
sizeof
so
sqrt
src
step
store
storeu
sub
sum
super
supported
switch
tan
tanh
that
this
tmp
trigonomic
trunc
type
u
uninitialized
unpacklo
unwrap
use
using
v
val
value
values
vec
vectorized
void
we
while
windows
would
xAA
xB
xC
xCC
xCF
xD
xF
xFC
xFFFFFFFF
xor
xxyy
xy
y
z
zero

?
A
ASSERT
ATen
Add
After
Allocate
Aq
B
Bq
But
C
CHANNEL
CHECK
CONTIGUOUS
CPU
Calculate
Call
Calling
ChooseQuantizationParams
Consider
Creates
Currently
D
Dequantize
Do
DoNothing
FBGEMM
FN
FORMAT
FP
FindMinMax
Fix
GEMM
Get
Here
IMPL
INTERNAL
If
Input
IntrusivePtr
K
LEGACY
LIBRARY
Linear
LinearPackedParamsBase
M
MEMORY
NAME
NoTranspose
Note
OUT
On
Option
PYTORCH
PackAWithQuantRowOffset
PackBMatrix
PackedLinearWeight
PackedLinearWeightFp
PackedLinearWeightsQnnp
Packs
Pass
Process
PyTorch
QLinearDynamicFp
QLinearDynamicInt
QNNPACK
QuantizationGranularity
Quantize
Quantizes
ReQuantizeForFloat
ReluFused
SELECTIVE
Sizeo
Still
TODO
TORCH
Tensor
The
Therefore
This
TorchClass
Update
We
Your
`pmat`
above
across
activation
actually
add
added
adjust
affine
after
again
allocate
allocating
allocation
also
an
and
apply
arbitrary
are
arrays
as
assertion
aten
avoid
b
back
batch
be
begin
below
bias
bit
buffer
built
but
cache
calculated
call
called
cannot
case
cast
cblas
cfg
changed
channel
channels
col
cols
column
columns
compute
consistent
const
constexpr
contig
contiguous
correctness
cpp
cpu
dequant
dequantization
device
different
dim
dimension
dimensions
do
doNothingObj
does
doesn
doing
dynamic
eagerly
elements
empty
end
ensure
equal
examples
executed
fail
failed
fallback
false
fbgemm
fbgemmPacked
fbgemmSupportedCPU
feature
floating
following
force
fp
freed
friendly
from
function
further
gemm
generate
generated
getPackedWeights
getRowOffsetBuffer
given
globalContext
guarantee
hand
has
have
here
id
in
index
input
installation
integer
interface
internally
into
intrusive
irange
item
jit
just
kCPU
kFloat
kInt
kPerChannelAffine
kPerTensorAffine
kQUInt
kernel
kimishpatel
larger
lazy
ld
ldc
left
len
let
like
linear
loudly
m
machines
make
manages
matrices
matrix
max
min
mobile
models
move
multiplication
must
nCol
nRow
native
needed
nextop
no
not
nullptr
numCols
numRows
number
numerics
offset
offsets
only
op
operation
operator
operators
optimizations
optimized
options
or
orig
original
out
outProcess
output
outputProcObj
ownership
pack
packA
packB
packed
packing
parallel
params
pass
past
path
per
performed
pipeline
pmat
point
pointer
pointers
points
power
pre
precision
preserve
provide
pt
pthreadpool
ptr
pytorch
qint
qlinear
qmax
qmin
qnnp
qnnpack
qnnpackLinearDynamic
qparams
quant
quantization
quantize
quantized
quint
range
rather
re
really
reduce
regardless
register
release
releaseWeightsWhenPrepacking
relu
repopulating
requant
requantization
requires
reset
resetting
respectively
result
resulting
results
reuse
row
rows
rowwise
runStatus
s
same
scalars
scale
scales
scheme
set
should
signed
since
size
smat
so
sparsity
src
static
statistics
status
stride
strong
success
support
task
tasks
tensor
term
than
that
these
this
thread
threadpool
threads
through
throw
thus
tiles
trans
u
uint
unique
unpack
unsigned
use
used
using
usize
utils
value
values
ve
vec
view
w
was
way
we
weight
weights
well
whole
will
with
within
won
wt
zero
zp

ATen
PyTorchQnnpAvgPoolQuantizationParams
add
address
adds
assert
aten
bias
c
const
cpp
cpu
cvtepi
cvtps
cvtsi
decrement
do
epi
epu
extract
ft
gavgpool
hi
input
lo
load
loadl
loadu
m
max
min
mm
mul
n
native
none
output
packs
packus
params
point
ps
pytorch
qnnpack
quantization
quantized
scale
setzero
shift
si
src
srl
srli
sse
storel
stride
u
uintptr
ukernel
unpackhi
unpacklo
up
usize
vacc
vbias
vi
vim
vout
vscale
vscaled
vxi
vzero
while
zero

API
ASSERT
Assumes
Aten
BUFF
CPU
CU
CUDA
Checks
Collapses
DESC
Declarations
DispatchKey
Empty
FILE
GENERIC
INTERNAL
In
IntArrayRef
LEN
Make
NOTE
NULL
Note
PRId
Please
Pointer
Resize
STOP
Scalar
See
Skips
Slow
So
StorageImpl
TH
THArgCheck
THC
THCDescBuff
THCState
THCStorage
THCTensor
THCudaBFloat
THCudaBoolTensor
THCudaByteTensor
THCudaCharTensor
THCudaCheck
THCudaComplexDoubleTensor
THCudaComplexFloatTensor
THCudaDoubleTensor
THCudaHalfTensor
THCudaIntTensor
THCudaLongTensor
THCudaShortTensor
THCudaTensor
THTensor
TORCH
Tensor
TensorImpl
These
Thinking
This
TypeMeta
UndefinedTensorImpl
You
abstraction
access
alias
all
already
and
are
arg
args
as
aten
available
backwards
be
being
break
buf
called
cannot
char
check
checkGPU
clearFlag
clone
compatibility
const
contiguous
continue
copy
cpp
creation
cudaGetDevice
curDev
current
cwrap
d
debug
define
device
dim
dimension
dimensions
dims
directly?
dispatch
distinct
do
documentation
doing
don
dst
end
endif
especially
everything
exist
false
first
firstIndex
flag
four
free
freeCopyTo
ft
function
functions
general
generic
genericized
getDevice
getStoragePtr
h
happen
have
header
here
hpp
ifndef
in
including
incref
indices
init
input
intrusive
isContiguous
isSameSizeAs
lazy
least
likely
list
make
may
measure
methods
might
must
n
nDimension
nDimensionLegacyAll
nDimensionLegacyNoScalars
nElement
nTensors
narrow
native
need
needs
new
newClone
newContiguous
newFoldBatchDim
newNarrow
newSelect
newSize
newStride
newTranspose
newWithSize
newWithStorage
newWithTensor
no
none
not
nullptr
offset
one
only
out
output
probably
process
ptr
ptrdiff
put
pytorch
range
raw
read
reclaim
release
removed
resize
resizeAs
resizeNd
retain
same
select
set
setFlag
setStorage
should
simply
single
size
sizeDesc
sizeLegacyNoScalars
sliceIndex
snprintf
some
specific
squeeze
src
start
state
static
still
storage
storageOffset
str
stride
strideLegacyNoScalars
strides
support
tensor
tensorDev
tensors
these
they
this
three
transpose
tricks
type
types
un
unless
unsafeReleaseTensorImpl
unsigned
unsqueeze
use
used
va
valid
value
values
via
view
vim
violation
void
vs
want
wrap
you
z

ATen
INTERNAL
PYTORCH
QNNP
aten
avgpool
buffer
const
cpp
cpu
declare
export
function
h
ident
increment
kc
ks
macro
mp
mpavgpool
n
name
native
neon
params
pytorch
qnnp
qnnpack
quantization
quantized
rules
src
sse
u
ukernel
union
up
upavgpool
usize
void
xm
y
zero

ATen
allowlist
assert
aten
bc
const
contains
core
cpp
d
mod
op
pytorch
registration
src
super
test
use

ATen
Clamp
NULL
PRIu
PyTorchQnnpOperator
PyTorchQnnpStatus
QNNPACK
allocate
aten
batch
be
because
below
bytes
c
calloc
channels
clamp
clamping
compute
const
cpp
cpu
create
delete
enum
error
failed
flags
format
goto
initialized
input
invalid
log
max
memory
min
must
native
nc
non
not
number
op
operator
out
output
parameter
params
pixel
properly
pytorch
qnnp
qnnpack
quantized
quint
range
setup
size
sizeof
src
status
stride
structure
success
type
u
ukernel
uninitialized
usize
with
zero
zu

API
ATen
ConstQuantizerPtr
IntrusivePtr
Quantizer
Tensor
TensorList
This
TypeDefault
actually
are
aten
class
cpp
enable
exposing
frontend
function
h
in
ll
native
pytorch
remove
src
temporary
them
type
typedef
we

ASSERT
ATen
Compute
Create
NE
NEAR
Sigmoid
SigmoidOperatorTester
Verify
and
assert
aten
batch
batchSize
begin
bind
c
channels
const
cpp
cpu
create
default
delete
destroy
device
distribution
end
exp
fill
generate
h
initialize
input
inputScale
inputStride
inputZeroPoint
isnormal
iteration
iterations
max
min
mt
native
nc
nullptr
operator
output
outputRef
outputScale
outputStride
outputZeroPoint
point
pool
pytorch
qmax
qmin
qnnp
qnnpack
quantized
random
randomDevice
ref
reference
results
rng
scale
scaledSigmoidX
setup
sigmoid
sigmoidOp
sigmoidX
size
src
status
stride
success
test
tester
testq
this
thread
u
uniform
usize
xA
y
zero

ASSERT
AT
ATen
CHECK
ChannelsLast
D
DISPATCH
It
MemoryFormat
Non
Option
QINT
Scalar
TORCH
TYPES
Tensor
affine
aten
b
but
c
case
cast
channels
compute
const
contig
contiguous
copy
cpp
cpu
d
dim
empty
equals
expected
factors
format
frame
functions
got
h
height
idata
index
input
irange
just
memcpy
memory
native
nbatch
nearest
neighbor
nhwc
nullopt
o
odata
options
osize
out
output
point
pos
ptr
pytorch
quantized
qupsample
reinterpret
scalar
scale
scales
size
sizeof
source
special
src
static
suggest
tensor
type
typename
underlying
upsample
value
w
width
with
yaml
zero

?
@ezyang
ADD
AT
ATen
After
Apex
Arg
ArgList
Args
ArrayRef
Autocast
AutocastCPU
AutocastCUDA
BCELoss
BCEWithLogits
BCEWithLogitsLoss
Banned
Base
Bfloat
CHECK
CPU
CUDA
Cast
CastPolicy
Common
Cores
CppFunction
Currently
DIFFERENT
DeviceType
Dimname
DimnameList
DispatchKey
Do
Does
ERROR
Ed
ExcludeDispatchKeyGuard
Explicit
F
FN
FUNC
General
Heuristic
IMPL
If
Imitate
In
IntArrayRef
IntList
Interior
It
KERNEL
Keep
LIBRARY
Less
List
Logic
Many
Move
NAME
NS
Non
OP
Option
Otherwise
Our
Overload
POLICY
Patch
Policies
Potential
Python
RAW
REDISPATCH
REGISTER
Redispatch
Registered
Ret
Reuses
Run
SELECTIVE
SIGNATURE
Saving
Scalar
ScalarOpt
ScalarType
See
TODO
TORCH
Template
Templates
Tensor
TensorImpl
TensorList
TensorLists
Tensors
The
Therefore
This
Treats
UndefinedTensorImpl
Unexpected
Unpack
ValType
VariableTypeEverything
We
WeakIntrusivePtr
WeakrefType
When
WrapFunction
WrapFunctionIntoFunctor
Wrapper
\n
about
above
accept
acos
across
actual
adaptive
add
addbmm
addcdiv
addcmul
adding
addmm
addmv
addr
affine
against
ahead
all
allocated
already
also
among
an
and
another
answer
any
append
appending
apply
appropriate
are
are\n
arg
args
argument
arguments
as
asin
assuming
atan
aten
autocast
autocasting
autograd
aware
back
backward
baddbmm
banned
based
batch
be
because
before
behalf
behavior
being
below
benefit
best
bf
bilinear
binary
bmm
boxing
bug
but
cache
cached
call
calling
calls
can
case
cases
cast
casted
casting
casts
cat
catch
categories
cdist
cell
chain
chokes
class
clear
clever
closely
codegenned
com
combine
commas
commenting
common
compare
const
contain
context
controls
conv
convolution
convolutions
copies
copy
copying
core
correspond
cosh
cosine
could
cpp
cpu
cross
cublas
cuda
cudnn
cumprod
cumsum
current
d
debatable
decisions
declaration
declare
declared
decrement
deduce
default
defined
definition
deleted
deprecated
depth
detailed
determine
device
different
difficult
dim
dimname
directly
discussion
dispatch
dispatcher
dist
div
divergent
does
doesn
don
dot
double
dropout
element
eligible
embedding
emplace
enabled
end
ensure
entropy
enum
equal
erfinv
error
etc
even
ever
example
excluded
executed
exits
exp
explicit
explicitly
explicity
expm
exposed
exterior
extern
extract
extracts
fake
fallback
field
figure
final
find
first
firstarg
flag
flags
flip
floating
following
footprint
forward
fp
frobenius
from
function
functional
functions
fused
gelu
getIntrusivePtr
going
grad
grid
group
gru
guaranteed
guess
guts
h
handling
happened
happens
has
have
header
here
heuristic
hinge
history
hit
how
https
huber
ident
ignored
ignores
imitating
implement
implicit
implicitly
in
incoming
increment
index
ineligible
infer
input
inside
instance
instantiation
interfere
interior
intermittent
internal
into
intrusive
invokes
its
just
kBFloat
kDouble
kFloat
kHalf
keeps
key
kl
kthvalue
layer
layers
lazy
leaf
leak
leaves
let
level
library
like
linalg
linear
list
lists
ll
load
local
log
logits
logits\n
loss
lost
lower
lstm
m
macro
make
makeFallthrough
manager
manually
map
margin
matches
matching
math
matmul
max
may
memory
method
methods
mirrors
mistakenly
mkldnn
mm
mod
mode
model
models
moment
more
most
mse
multi
multilabel
mv
my
names
native
nearest
necessary
need
nesting
new
next
nextArg
nll
nn
no
nogroup
non
norm
not
now
nuclear
nullopt
occur
one
ones
only
op
operations
opportunity
ops
opt
optimization
optional
or
other
otherwise
our
out
output
outside
over
overload
overloads
overrides
pad
parameter
pass
pasted
patched
paths
pattern
pdist
per
performs
pitfall
place
point
pointer
poisson
policy
pool
possible
pow
precede
precision
prelu
present
prioritize
prioritizes
process
prod
promote
promoted
promotion
properly
provide
proxy
ptr
push
put
pytorch
quantize
questions
random
ranking
rare
rather
re
reciprocal
record
recording
redispatch
redispatching
reduce
reflection
region
register
registered
registering
registration
relu
renorm
repeated
repr
requires
reserve
respect
returns
reuse
right
rnn
rsqrt
rules
running
runtime
s
safe
said
same
sampler
save
saved
scalar
scatter
second
section
see
set
setting
several
shallow
should
shouldn
side
sigmoid
signature
significantly
similarity
simple
sinh
size
smooth
sneak
so
soft
softmax
softplus
some
something
sort
source
specializations
specialized
specified
src
stack
stackoverflow
static
still
strategy
strawman
streamline
structure
stuff
substitutions
such
sum
super
support
sure
surface
switch
tail
tan
tanh
tbc
templates
tensor
tensordot
tensors
than
that
their
them
then
therefore
these
they
think
this
thnn
those
thread
throw
time
tls
topk
torch
touch
tracks
traits
transpose
trilinear
triplet
try
tuple
type
typelist
typename
types
u
unaltered
unary
unchanged
unknown
unordered
unpredictable
unsafe
unsafeGetTensorImpl
unwrap
update
upsample
use
used
useful
user
uses
using
uuid
val
value
ve
vec
view
we
weak
weakref
weights
were
what
which
whose
why
widest
will
with
would
wouldn
wrap
wrapped
wrapper
wrappers
write
xla
you

ARCH
ARM
ATen
CPUINFO
GemmMicrokernelTester
NEON
REQUIRES
SSE
TEST
aStride
aZeroPoint
aarch
any
arch
arm
aten
azp
bZeroPoint
bzp
c
cStride
cc
cfg
conv
cpp
cpu
div
eq
ft
gt
iterations
k
kr
m
mod
mr
n
native
neon
none
np
nr
only
pytorch
qmax
qmin
qnnpack
quantized
src
sse
strided
subtile
super
test
u
ukernel
use
usize
vim

AARCH
ARCH
ARM
ATen
BLOCK
CPUINFO
GEMM
GemmBlockSparseMicrokernelTester
MR
NEON
NR
OP
PACKED
Q
REQUIRES
ROW
SIZE
SIZEXCOL
SPARSE
SSE
TEST
aStride
aZeroPoint
aarch
any
arch
arm
aten
azp
bZeroPoint
block
bzp
c
cStride
cc
cfg
col
colBlockSize
compute
cpp
cpu
div
dq
eq
export
ft
gemm
gt
ident
iterations
k
kernel
lt
m
macro
mod
mr
n
native
neon
none
nozp
nr
op
packA
packed
packedA
packeda
prepacking
pytorch
qmax
qmin
qnnpack
quantized
row
rowBlockSize
rules
size
sizexcol
sparse
src
sse
strided
subtile
super
test
u
ukernel
use
usize
vim

ATen
ClampingParams
LIKELY
PYTORCH
PyTorchQnnpU
QNNP
assert
aten
builtin
c
clamp
const
cpp
cpu
do
dup
ft
increment
lane
low
max
min
n
native
neon
none
output
params
prefetch
pytorch
qnnpack
quantized
src
u
uint
uintptr
ukernel
usize
vget
vim
vld
vmax
vmaxq
vmin
vminq
vout
voutput
vst
vx
vy
while
y

ATen
Apply
ApplyRelu
ConvPackedParamsBaseInterface
DIM
Dilation
Groups
Option
OutputPadding
Padding
SPATIAL
Stride
Tensor
TorchJitCustomClassHolder
TorchList
Transpose
Unpack
apply
aten
const
conv
cpp
cpu
dilation
groups
h
input
native
output
packed
padding
params
point
pytorch
quantized
relu
scale
src
stride
trait
transpose
unpack
zero

@zasdfgbnm
AT
ATen
CUDNN
ConvPlaceholders
CuDNN
Deprecated
ENABLED
ERROR
MaybeOwned
Note
Option
Placeholder
Scalar
See
TODO
Tensor
add
allow
allowTF
alpha
aten
backward
benchmark
bias
borrow
cfg
compatibility
compiled
const
convolution
cpp
cuDNN
cudnn
defined
deprecated
deterministic
dilation
dim
disabled
forward
from
future
globalContext
grad
groups
hacky
here
in
input
mask
maybe
mod
native
not
only
operators
opt
optional
out
output
owned
padding
philosophy
preprocessor
pytorch
raw
relu
removal
remove
reshape
size
src
stride
super
support
tensor
tf
this
transpose
use
weight
with
wrapper
z

ALSO
AND
AT
ATen
BTW
But
CHECK
COMPLEX
CONTIGUOUS
Col
DISPATCH
Edward
FLOATING
FORMAT
Force
From
If
Im
In
Input
It
LEGACY
Let
MEMORY
Note
Now
Our
Output
Scalar
So
Suppose
THNN
TORCH
TYPES
Tensor
The
To
We
Weight
accept
add
adj
all
an
and
another
arises
as
aten
avoid
aw
b
backward
backwards
batch
batched
because
been
both
bounds
bounds?
but
bw
c
can
case
check
checks
clearly
col
compute
computing
consider
contiguous
contributions
convolution
could
cpp
cpu
cw
d
desired
diagram
dilated
dilation
dim
directly
do
down
dw
easy
effective
elt
empty
end
equals
error
expected
false
fix
formulas
going
got
grad
gradient
gradients
handle
have
height
how
im
implementations
in
input
inside
just
kHalf
kernel
known
labeled
like
ll
might
n
native
non
notion
once
one
only
out
output
over
pad
padding
parameters
performing
plane
principle
problem
ptr
pytorch
raised
re
real
redundant
resize
right
rub
runs
s
same
say
scalar
see
seemingly
select
set
shape
should
simple
size
slide
src
standard
stencil
stride
surely
take
that
that?
there
this
those
transposed
trouble
type
understand
up
us
useful
using
vol
w
wanted
we
weight
well
what
which
why
width
with
within
would
you
zero
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

API
ATen
InitialTensorOptions
NOTE
Represents
TensorOptions
This
are
aten
be
before
changed
cpp
defaults
designed
devices
dtypes
etc
ever
explicit
false
grad
h
in
initial
kCPU
kFloat
kStrided
known
layout
library
not
options
pytorch
requires
src
stable
tensor
this
used

CONCAT
TH
THC
THCGenerateAllTypes
THCTypeIdx
THCTypeIdxByte
THCTypeIdxChar
THCTypeIdxDouble
THCTypeIdxFloat
THCTypeIdxHalf
THCTypeIdxInt
THCTypeIdxLong
THCTypeIdxShort
aten
const
cpp
export
h
ident
idx
macro
pytorch
rules
src
thc
type
usize

?
AND
AT
ATen
Allow
Assuming
AutoDispatchBelowADInplaceOrView
BFloat
CHECK
CONV
Calculated
CheckedFrom
ConvolutionMM
D
DISPATCH
FLOATING
GRAIN
Given
Kernel
MaybeOwned
Note
Option
Output
SALT
Scalar
ScalarType
See
TODO
TORCH
TYPES
Tensor
TensorArg
TxHxW
Unfold
acc
accessor
actual
addmm
arg
as
aten
away
backward
baddbmm
batch
be
bias
bmm
borrow
buffer
but
c
can
cast
channel
check
checkContiguous
checked
const
contiguous
conv
copy
cpp
cpu
d
dAccCPU
dCopyCPU
deciding
defined
depth
detach
determine
dim
dimensions
div
empty
end
entire
exact
expected
false
fgrad
finput
forward
frame
from
functions
generated
got
grad
greater
group
grouped
groups
h
hacky
height
in
individual
input
instead
k
kernel
make
mask
maybe
mm
mode
multiple
must
n
native
ndim
need
new
non
none
not
opt
optional
options
or
other
out
output
owned
pad
padded
padding
parallel
parameters
per
permute
plane
planes
pytorch
removal
reshape
resize
rtn
s
scalar
shape
should
size
slow
small
src
start
stride
sum
support
sz
tensor
tfinput
than
too
transpose
tuple
tweight
type
undefined
unsqueeze
update
upstream
valid
variable
view
w
way
we
weight
width
wrapper
zero

ATen
HARDSIGMOID
HardsigmoidOperatorTester
OP
aten
batch
batchSize
cc
channels
cpp
cpu
ft
hardsigmoid
input
inputScale
inputStride
inputZeroPoint
iterations
native
none
output
outputStride
point
pytorch
qmax
qmin
qnnpack
quantized
scale
small
src
stride
strided
test
testQ
u
unit
usize
vim
with
zero

?
A
AND
AT
ATen
B
Batch
Bias
CHECK
CONTIGUOUS
Calculated
D
DISPATCH
Define
Do
ERROR
Extract
FLOATING
FORMAT
For
Force
GEMM
GEMV
Given
Helpers
It
K
LEGACY
Long
M
MEMORY
Matrix
MaybeOwned
NaiveConvolutionTranspose
NoTranspose
Note
Option
Output
Resize
Scalar
ScalarType
See
TORCH
TYPES
Tensor
Transpose
Unpack
Weight
acc
accumulation
after
alpha
always
and
are
assumes
aten
back
backward
batch
be
because
beta
bias
bit
borrow
buffer
but
can
cast
channel
check
col
column
columns
com
confusing
const
contains
contiguous
conv
cpp
cpu
cpublas
cublas
cuda
d
defined
dilation
dim
dimf
dimh
dims
dimw
do
docs
either
elt
empty
equals
ever
expected
false
fill
from
gemm
gemv
gets
got
grad
greater
gt
hacky
height
http
im
in
increased
incx
incy
input
into
irange
k
kernel
kh
kw
lda
like
lt
m
major
mask
matrices
matrix
maybe
modules
mulitply
must
n
native
ndim
needs
non
note
nullable
nvidia
ones
only
opt
optional
options
or
other
out
output
owned
pad
padding
parameters
per
plane
planes
ptr
pytorch
removal
resize
sample
scalar
scale
see
select
shape
shared
should
size
slow
small
smaller
src
static
stride
temporary
tensor
than
this
too
trans
transpose
tuple
type
u
weight
width
with
wrapper
y
zero

?
ATen
ArrayRef
CHECK
Dimname
FUNC
IMPL
Integer
Integers
It
META
Note
Numpy
Pow
Scalar
Signed
TORCH
Tensor
TensorIterator
TensorIteratorBase
This
Unsigned
allowed
allows
always
are
aten
attempts
b
base
be
because
better
binary
borrowing
build
but
c
cast
casted
casts
causes
check
circuit
common
compatibility
complex
const
conversion
copy
cpp
cpu
cuda
d
declare
define
defined
deprecated
device
dim
directly
dispatch
doesn
equal
exp
exponent
exponents
figure
fill
functions
given
giving
h
handle
has
ignore
in
inside
integer
integral
isComplex
isComplexType
isIntegral
isIntegralType
item
kComplexDouble
kDouble
lazy
maybe
names
native
need
negative
non
not
only
op
operation
options
otherwise
out
output
overflow
overload
overloads
pow
power
powers
powi
pytorch
redispatch
redispatches
requires
result
results
returned
s
scalar
set
short
signed
since
src
static
stub
tensor
ternary
this
toComplexDouble
toDouble
toLong
truncated
type
ubsan
unary
unsigned
use
using
void
way
which
while
wrapped
zero

ABCDEFGHIJKLMNOPQRSTUVWXYZ
ANY
ASDF
ASSERT
ATen
Add
Check
ClassType
CompilationUnit
Constant
ConstantString
DEFAULT
Default
Device
Dict
EXPECT
EnumHolder
EnumType
Error
Exercise
Expected
FAIL
FALSE
Future
FutureError
HasSubstr
HashAliasedIValues
IVALUE
IValue
IntType
Is
List
My
NE
Object
Post
Scalar
Scalars
Snippets
Stream
StringType
StrongTypePtr
THAT
THROW
TODO
TRUE
Tensor
TensorType
Tensors
Test
This
TorchkBool
Torchones
Torchrand
Torchzeros
Tuple
We
`true`
abcdefghijklmnopqrstuvwxyz
actual
addAttribute
addCallback
alias
all
already
ambiguous
an
and
asdf
assembly
assign
aten
attr
b
bar
basic
baz
because
booleanTrue
bug?
c
callbacks
calledTimes
calledTimesA
calledTimesB
cases
catch
checking
class
clear
clone
cls
comparison
completed
complex
const
construct
construction
container
conversion
copy
copyFrom
copyTo
count
cpp
create
cu
d
device
dict
different
dlist
doesn
double
elem
element
elements
elementwise
empty
enum
eq
eqTensor
equal
equality
equals
err
error
exception
exceptions
expect
expected
export
false
first
following
foo
four
future
getSubValues
gmagogsfm
goodbye
has
hasAttribute
hasError
hash
hashing
have
hello
ident
identity
ii
in
innerDict
innerDictNotEqual
input
insert
inspect
integer
internal
internalToPointer
intrusive
isAliasOf
isBool
isBoolean
isComplexDouble
isComplexDoubleList
isDouble
isDoubleList
isInt
isIntList
isNone
isPtrType
isScalar
isString
isTensor
isTuple
item
iv
ivalue
ivalues
j
kCUDA
lhs
list
m
macro
make
makeMoreSampleIValues
makeSampleIValues
markCompleted
more
moreSampleIValues
move
moveFrom
moveTo
multi
name
neither
nested
no
none
not
nullStr
nullopt
nullptr
numAttributes
o
obj
objType
object
one
operator
or
order
pairs
pointer
print
probably
ptr
pytorch
rand
randn
rawTensor
rawTensorCopy
remove
removed
res
returns
rhs
rules
runtime
s
same
sample
sampleIValues
sampleInputs
sampleTargets
scalar
seem
set
setError
setSlot
shared
should
size
slots
source
src
ss
str
stream
strings
stringstream
sub
subvalues
swap
tCopy
tReference
ten
tensor
test
test?
testEquality
testing
that
this
three
throw
toBool
toComplexDouble
toComplexDoubleList
toComplexDoubleVector
toDouble
toDoubleList
toDoubleVector
toFuture
toGenericDict
toInt
toIntList
toIntVector
toScalar
toTensor
toTuple
tp
try
tryRetrieveErrorMessage
tuple
tv
type
typedef
undefinedTensor
unsafe
unsafeRemoveAttr
unsafeToTensorImpl
use
v
value
values
variant
void
what
will
work

ATen
MM
PyTorchQnnpConvDynamicQuantizationParams
SHUFFLE
add
aten
b
c
channel
const
cpp
cpu
cvtepi
cvtsi
cvtss
dq
epi
ft
gemm
index
input
k
kernel
load
loadl
loadu
m
madd
mm
mr
mul
multipliers
native
none
nr
output
params
pi
point
points
predecrement
ps
pytorch
qnnpack
quantization
quantized
set
setzero
shift
shuffle
si
src
srl
sse
storel
storeu
stride
sub
u
uintptr
ukernel
unpacklo
usize
va
vacc
vb
vbias
vim
vmultiplier
void
vout
vxa
vxb
vzero
w
zero

?
ALL
AND
AT
ATen
AbsMaxOps
AbsMinOps
Acc
Bool
CHECK
COMPLEX
DISPATCH
FLOATING
For
HasType
INFINITY
LinearAlgebraKernel
NormOneOps
NormOps
NormTwoOps
NormZeroOps
Scalar
ScalarType
ScalarValueType
TORCH
TYPES
TensorIterator
Type
Vectorized
WARNING
above
addr
alpha
and
as
aten
be
behavior
beta
binary
cast
change
char
complex
const
could
cpp
cpu
dim
dispatch
do
double
elem
expects
false
fill
future
ignore
ignored
imag
imaginary
in
infinity
infs
init
input
isComplexType
isFloatingPoint
iter
kBFloat
kHalf
kernel
kernels
limits
linalg
loop
lu
must
nans
native
nelems
norm
not
ord
out
output
outputs
pivots
propagate
ptr
pytorch
reduce
register
reinterpret
returns
scalar
should
size
so
src
static
strides
stub
swap
them
this
torch
touch
trait
type
ubsan
undefined
unpack
unpacked
using
val
values
vec
we
zero

CONCAT
NAME
Real
TH
THVector
Vector
We
and
are
aten
cpp
declarations
dispatch
dynamic
export
functions
generate
going
h
ident
macro
only
pytorch
rules
src
th
use
want

ATen
Add
Apply
ArgNames
Args
BENCHMARK
Benchmark
BenchmarkState
C
CharacteristicArguments
MAIN
NO
PYTORCH
Q
QNNPACK
SetBytesProcessed
SetItemsProcessed
SkipWithError
add
addOperator
arguments
aten
b
batchSize
begin
bench
bind
bytesPerIteration
c
cast
cc
channels
characteristic
const
cpp
cpu
create
delete
device
distribution
end
endif
failed
flags
generate
ifndef
initialize
inplace
itemsPerIteration
iterations
lazy
max
min
mt
n
native
nc
nullptr
operator
point
pool
pytorch
qnnp
qnnpack
quantized
random
randomDevice
range
ref
rng
scale
setup
sizeof
src
state
static
status
stride
success
thread
u
uniform
usize
y
zero

ATen
All
Analytic
CHECK
Differentiable
Double
Expected
Float
Functions
GradMode
HardshrinkBackward
In
Like
Python
RVO
Staying
TORCH
Tensor
The
To
True
VariableType
WeightNorm
WeightNormFusedBackward
additional
already
alternative
an
and
are
argument
as
assume
aten
autograd
avoid
back
backward
bcast
be
blocking
broadcast
but
called
can
check
checks
clarity
consider
construct
contiguous
correct
cpp
creating
cuda
define
defined
derivative
device
differentiable
dim
dimension
dimensions
dispatch
does
dynamic
enabled
etc
ever
except
expect
expected
faithful
fine
first
function
fused
grad
graph
half
have
here
in
inlining
instead
interface
itself
last
later
look
might
must
native
non
norm
norms
not
now
object
only
ops
optimizations
optimize?
or
output
over
path
per
performed
pow
primitive
probably
pytorch
route
s
same
saved
scalar
separate
shaped
should
single
size
so
src
statement
succeed
sum
sums
supplies
tensor
that
these
this
through
transpose
tuple
type
use
used
using
v
view
w
we
weight
well
why
will
within
would

APPLY
APPLYX
AT
Assumes
CAL
CODE
CONTIG
COUNTERS
Calculate
Collapse
Contig
DIM
DIMENSION
DOUBLE
ERROR
EXPR
Expected
FLOAT
GRAIN
HYPER
IS
IVDEP
MEMORY
MSC
MSVC
NAN
NULL
Noop
OFFSET
OMP
ORDIN
OUTPUT
OVERHEAD
P
PARALLEL
PRAGMA
PREAMBLE
Pragma
REAL
SIMD
SIZE
Scalar
Size
TENSOR
TH
THFree
THRESHOLD
THTensor
THTensorApply
TYPE
Tests
The
UNCERTAIN
UPDATE
Used
VER
\
`scatterAdd`
`scatter`
accreal
advanced
adveanced
all
allocated
and
apart
apply
aten
be
begin
break
check
clang
contig
count
counter
cpp
d
decrease
define
defined
dim
dimension
doesn
easy
element
elements
elif
end
endif
expected
export
first
flag
following
from
ft
generic
getStoragePtr
hasFinished
have
hpp
ident
ifndef
in
inconsistent
index
indexes
internal
isContiguous
isSameSizeAs
isnan
iter
iterate
ivdep
lazy
len
length
line
local
loop
loops
macro
memory
nDimensionLegacyAll
nElement
nan
nested
none
not
number
numbers
offset
omp
one
order
parallel
plus
pragma
pragmas
ptrdiff
pytorch
r
real
reduce
reduction
region
rest
rp
rules
same
scatter
searching
seg
shape
simd
size
sizeLegacyNoScalars
smaller
src
srcp
start
static
step
storage
strategy
stride
sum
support
tensor
tensors
th
than
thread
tmp
tp
undef
understand
update
val
vim
while

?
AT
ATen
Arguments
Assumes
D
DISPATCH
Data
Depending
Dist
DistanceOpsKernel
F
FLOATING
GRAIN
General
Generally
However
IDistCalc
Inf
LttDistCalc
ODistCalc
One
PDistCalc
SIZE
SSE
Scalar
Special
TDistCalc
TODO
TYPES
Tensor
The
There
This
To
Two
Vectorized
VectorizedScalar
We
ZDistCalc
Zero
abs
accounts
actually
add
added
agg
aggregated
all
also
always
an
and
apply
are
aren
assumption
aten
avoid
b
backward
be
because
before
better
blendv
bogus
break
but
calc
call
can
case
cases
cast
cdist
ceil
checking
collapsed
column
columns
combs
compiler
compite
complext
component
compute
conceptually
const
contiguous
could
count
cpp
cpu
curr
current
d
derivative
diff
different
differently
dimension
dispatch
dist
do
does
don
done
double
down
earlier
end
etc
every
explanitory
expression
faster
few
fill
finish
first
floating
floor
form
forward
four
from
function
functions
fused
general
gets
grad
gradient
gs
guaranteed
guaranteeing
has
have
how
idist
implementation
implementations
independentaly
index
inefficient
inf
infers
inlines
inner
input
instead
instructions
internal
into
isinf
issues
iterate
j
k
kernel
last
less
loadu
locking
logic
loop
lttdist
m
main
map
max
maximum
min
minimum
mod
modify
much
multiply
n
native
no
nonempty
norm
odist
only
other
outer
outside
over
parallel
parallelize
parallelizes
parallelizing
pass
pdist
perf
pnorm
point
pow
pretty
probably
process
ptr
pvec
pytorch
r
range
red
reduce
register
remainder
represent
requires
res
result
reuse
s
same
satisfies
scalar
second
section
separate
shortcircuited
should
sign
size
so
special
specific
sqrt
src
standard
start
static
still
store
stride
structs
stub
sum
summed
super
supports
tdist
tells
tensor
than
that
there
these
this
those
thus
truncation
tuples
type
up
use
used
uses
using
val
value
vec
very
way
we
what
while
with
zdist

ASSERT
ATen
Compute
Create
GE
LE
Max
MaxPoolingOperatorTester
NE
Pooling
Re
Setup
Verify
and
assert
aten
batch
batchSize
begin
bind
bottom
c
channel
channels
const
cpp
cpu
create
d
default
delete
destroy
device
dilated
dilatedPoolingHeight
dilatedPoolingWidth
dilation
dilationHeight
dilationWidth
distribution
end
fill
first
ft
generate
h
height
in
index
initialize
input
inputHeight
inputPixelStride
inputWidth
iteration
iterations
iy
left
max
maxPoolingOp
maxValue
min
mt
native
next
nextBatchSize
nextInputHeight
nextInputWidth
nextOutputHeight
nextOutputRef
nextOutputWidth
nhwc
none
nullptr
once
operator
output
outputHeight
outputPixelStride
outputRef
outputWidth
ox
oy
paddedInputHeight
paddedInputWidth
paddedNextInputHeight
paddedNextInputWidth
padding
paddingBottom
paddingHeight
paddingLeft
paddingRight
paddingTop
paddingWidth
pixel
pool
pooling
poolingHeight
poolingSize
poolingWidth
px
py
pytorch
qmax
qmin
qnnp
qnnpack
quantized
random
randomDevice
ref
reference
results
right
rng
second
setup
setupu
size
src
status
stride
strideHeight
strideWidth
success
test
tester
testu
this
thread
time
top
u
uniform
usize
vim
width
xA
y

?
ASSERT
ATen
AVX
All
Args
ArgsTuple
Base
Basic
BinaryOpsKernel
Both
CPU
Cb
Copying
Explicitly
Fill
For
Func
GCC
GRAIN
INDEX
INTERNAL
If
IndexSequence
Indices
Instead
It
Iterate
Loop
Loops
May
Or
README
Range
S
SIMD
SIZE
Scalar
See
Supports
TORCH
TensorIterator
TensorIteratorBase
The
These
This
TupleOutput
TupleOutputBaseCase
Use
VecFunc
Vectorized
We
`S`
`Scalar`
`apply`
`basic
`cpu
`func
`gpu
`handle
`multiple
`needs
`thrust
`tuple`
above
added
all
also
and
apply
arbitrary
are
arg
args
argument
arity
as
aten
available
b
base
basic
be
both
but
can
case
cast
casting
casting`
cb
char
check
checking
checks
class
compiled
compiler
complete
const
constexpr
contiguous
corresponding
could
cpp
cpu
currently
d
decltype
default
dereference
details
different
directories
dynamic
elementwise
enable
enabled
ever
example
exception
execute
explicitly
extend
extended
false
file
files
follows
forward
func
function
functions
future
generate
give
grain
h
handle
help
helps
idx
implementation
implemented
in
index
indicated
input
instead
intended
intrinsics
invocation
iter
iterating
kernel
kernels
lambda
let
like
loadu
loop
loop`
make
manipulated
match
may
md
member
members
method
more
move
multiple
multiplication
must
n
native
needs
new
ninputs
no
not
noutputs
now
ntensors
nullptr
older
one
only
op
operation
opt
or
other
out
output
outputs
outputs`
over
passed
position
provides
ptr
pytorch
range
recursive
relies
requires
result
returned
returns
s
same
scalar
sending
sequence
serial
set
similar
since
single
size
sizeof
slices
so
some
src
store
stride
strides
structure
subdirectory
support
supported
t`
temporary
tensor
that
then
there
this
traits
tuple
tuple`
type
typename
types
u
unroll
unwrap
used
uses
using
usize
value
values
variadic
vec
vectorization
vectorized
versions
void
vop
we
with
without
work
write
you

ATen
C
FFFF
Size
UINT
abs
adjusted
assert
aten
bits
c
clamped
concat
const
cpp
cpu
even
fp
ft
hi
hl
input
lh
ll
lo
load
mask
max
min
multiplier
n
native
neg
none
output
point
precise
product
psimd
pytorch
qmax
qmin
qnnp
qnnpack
quantized
requantization
requantize
rounding
s
scale
scaled
shift
splat
src
store
u
vim
vmultiplier
vrounding
vshift
vsmax
vsmin
vzero
w
xy
xyzw
y
z
zero
zw

ALWAYS
AND
ATen
C
DEFINE
INLINE
MEMBER
ONE
OP
Sizeype
Sleef
TERNARY
Vectorized
VsxDblMask
abs
acos
acosd
add
alias
align
and
angle
arange
asin
asind
atan
atand
aten
attribute
b
base
blend
blendChoiceDbl
blendv
c
calc
case
cast
ceil
class
cmp
cmpeq
cmpge
cmpgt
cmple
cmplt
cmpne
comparision
conj
const
constexpr
copysign
copysignd
cos
cosd
cosh
coshd
count
cout
cpp
cpu
d
delete
div
double
dump
enable
endl
eq
erf
erfc
erfcd
erfd
erfinv
estimated
exp
expd
expm
floor
fmod
fmodd
frac
ft
ge
generated
gt
h
helpers
here
hypot
hypotd
idx
igamma
igammac
ignore
imag
include
internal
intrinsics
isnan
lazy
ld
le
lgamma
lgammad
loadu
log
logd
lt
madd
map
mapbi
mask
masks
max
maximum
may
memcpy
min
minimum
mul
namespace
nan
nd
ne
neg
nextafter
nextafterd
none
nor
offset
once
one
operator
or
other
pd
pi
pow
powd
pragma
private
ptr
public
pytorch
re
real
reciprocal
reinterpret
ret
returned
rint
round
rsqrt
scalar
sel
set
sign
sin
sind
sinh
sinhd
size
sizeof
sleef
splats
sqrt
src
st
static
step
store
sub
switch
tan
tand
tanh
tanhd
this
tmp
trunc
type
typename
u
ubsan
undefined
union
used
using
usize
v
value
values
vbool
vd
vec
vecb
vfloat
vim
vmask
void
vsx
xor
zero

ASSERT
AT
ATen
Argument
C
CHECK
Calculated
ChannelsLast
Check
D
DISPATCH
Expected
Expecting
FN
Fast
Given
Got
IMPL
INTERNAL
In
Includes
Input
Keep
LIBRARY
MaxPool
MemoryFormat
NAME
NHWC
Output
PYTORCH
Q
QEngine
QINT
QMaxPool
QNNPACK
QnnpackOperatorDeleter
Quantized
QuantizedCPU
SELECTIVE
Scalar
TORCH
TYPES
Tensor
affine
amenable
and
anonymous
are
args
argument
arr
arrays
as
aten
batch
batches
be
bottom
can
case
ceil
center
cfg
channels
check
col
const
contig
contiguous
cpp
cpu
create
createStatus
d
dH
dW
define
description
device
dh
dilated
dilation
dilationH
dilationW
dim
dimc
dimensional
dimensions
dimh
dimw
dispatch
dw
empty
end
endif
expected
failed
false
flags
format
functions
globalContext
got
greater
h
half
height
iC
iData
iH
iW
ic
ifdef
ih
in
inC
inH
inW
index
initQNNPACK
input
iw
kCPU
kH
kQUInt
kSpatialDim
kSqueezeDim
kW
kernel
kernels
kh
kw
last
layout
lazy
left
limits
list
local
loop
lowest
m
max
maxpool
memory
min
mode
more
must
namespace
native
nbatch
ndim
ndimension
nest
nhwc
no
non
not
nullopt
nullptr
o
oC
oData
oH
oSizes
oW
oh
one
operator
options
or
outC
outH
outW
output
ow
pH
pW
padH
padW
padding
parallel
params
path
ph
pixel
point
pointers
pool
pooling
preserve
pthreadpool
ptr
pw
pytorch
qEngine
qmaxpool
qnnp
qnnpack
qpool
quantized
quint
qx
qxd
qy
qyd
rank
registry
removed
resulting
right
row
runStatus
sH
sW
scalar
scale
setup
setupStatus
sh
shape
should
size
small
smaller
spatial
special
squeeze
src
start
static
status
stride
strideH
strideW
strides
stub
success
suggest
supported
sw
tcntr
tensor
than
that
this
thread
threadpool
toQIntType
too
top
type
typename
u
underlying
uniq
unique
unsqueeze
use
val
valid
vectorization
w
we
well
while
width
with
y
yaml
zero

AT
ATen
CHECK
D
DISPATCH
ERROR
Embedding
INDEX
Index
Note
See
TODO
TORCH
TYPES
Tensor
TensorArg
TorchList
add
after
all
arg
aten
back
backward
be
because
begin
c
cannot
check
checkDim
checkScalarTypes
com
come
const
contig
contiguous
continue
coo
counts
cpp
cpu
d
dense
details
dim
double
embedding
empty
end
features
freq
from
github
grad
gradients
here
idx
implement
improving
index
indices
inside
irange
issues
item
k
kInt
kLong
loop
max
more
must
native
new
norm
not
operations
optional
options
our
padding
parallel
perf
perform
ptr
push
pytorch
renorm
reset
reshape
row
scalar
scale
section
select
size
slice
sort
sorted
sparse
src
start
supported
tensor
that
type
unique
unsafe
use
values
vec
view
we
weight
weights
with
zeros

AFFINE
ATen
All
Bool
CHECK
Can
MemoryFormat
Need
Option
PER
QScheme
SYMMETRIC
Scalar
ScalarType
TENSOR
TODO
TORCH
Tensor
The
This
Unsupported
We
`
an
appended
are
aten
be
comparator
compatible
const
cpp
cpu
define
dequantize
dequantize`
dimvector
dq
efficient
eq
export
forall
format
ge
gt
has
have
ident
implementation
inefficient
infer
lazy
le
lt
macro
make
memory
more
must
name
named
native
ne
nullopt
only
op
operators
optional
other
out
per
pytorch
qscheme
quantized
quantizer
resize
rules
schemes
size
src
static
strides
sure
tensor
tensors
that
torch
unsafeGetTensorImpl
uses
value
variant
will
with

?
ASSERT
ATen
Call
Compute
LUTMicrokernelTester
LutUKernelFunction
PyTorchX
Verify
assert
aten
begin
bind
const
cpp
cpu
default
device
distribution
end
false
fill
ft
generate
h
inplace
iteration
iterations
kernel
lut
micro
microkernel
mt
n
native
none
optimized
position
pytorch
qnnpack
quantized
random
randomDevice
ref
reference
results
rng
src
test
tester
this
u
uniform
usize
vim
xA
xData
y
yRef

A
ATen
B
Bint
CHECK
CPU
Calculate
Comments
Consider
D
Didn
FBGEMM
FN
Given
HandleWeightsSaturation
IMPL
IntrusivePtr
It
JIT
K
LIBRARY
Legacy
LinearPackedParamsBase
NAME
Not
Note
Option
PYTORCH
PackAWithQuantRowOffset
PackBMatrix
PackedGemmMatrixFP
PackedLinearWeight
PackedLinearWeightFp
PackedLinearWeightsQnnp
QEngine
QLinearPackWeightFp
QLinearPackWeightInt
QNNPACK
QScheme
QuantizedCPU
Refer
SELECTIVE
TODO
TORCH
Tensor
The
This
TorchClass
Transpose
TypeMetaData
Update
We
Weight
XQ
across
actually
and
are
as
aten
b
be
below
bias
bint
boundaries
but
calc
call
called
cast
cfg
channel
col
column
columns
const
contig
contiguous
cpp
cpu
create
created
ctx
currently
custom
defined
deleteFn
details
dim
dimensional
dll
during
elements
endif
engine
entirely
expected
false
fbgemm
feature
find
fine
first
flows
fp
freed
from
function
functor
further
globalContext
got
groups
hack
has
have
here
ifdef
in
includes
initQNNPACK
input
instead
intrusive
invocation
irange
item
j
kFloat
kPerChannelAffine
kPerTensorAffine
lazy
ld
legacy
linear
m
make
manages
matrix
might
mingzhe
more
move
nCol
nRow
native
ndimension
new
not
nullopt
nullptr
offsets
once
only
op
operation
operator
optimizations
optional
options
ownership
pack
packed
packing
params
per
perfectly
pmat
point
points
pre
prepack
prepacked
problematic
ptr
pytorch
qEngine
qint
qlinear
qnnpack
qscheme
qtype
quant
quantized
rank
register
regular
reinterpret
removed
resize
ret
row
rows
safe
saturate
scalar
scale
scales
set
should
size
smat
src
static
step
sum
supported
sure
tensor
tensors
term
that
this
through
tie
toString
trans
translation
transpose
type
u
unique
unit
using
usize
utils
value
vec
very
w
we
weight
weights
well
whereas
with
within
wrapped
wt
zero
zeros

ATen
Display
Div
Formatter
Output
Range
Result
Self
aten
begin
core
cpp
div
divisor
end
fmt
h
new
other
out
pytorch
range
size
src
type

?
AND
AT
ATen
Allow
AutoDispatchBelowADInplaceOrView
BFloat
CHECK
Calculated
CheckedFrom
ConvolutionMM
D
DISPATCH
FLOATING
Given
Kernel
MaybeOwned
NoGradGuard
Note
Option
Output
Scalar
ScalarType
See
TORCH
TYPES
Tensor
TensorArg
acc
accessor
actual
addmm
arg
as
aten
away
backward
batch
be
bias
borrow
buffer
but
c
can
cast
channel
check
checkContiguous
const
contiguous
conv
copy
cpp
cpu
d
defined
detach
dim
dimensions
div
empty
end
exact
expected
false
fgrad
finput
forward
frame
from
generated
got
grad
greater
group
hacky
height
input
k
kCPU
kernel
make
mask
maybe
mm
mode
n
native
ndim
ndimension
new
no
non
not
opt
optional
options
or
other
out
output
owned
pad
padded
padding
parallel
parameters
per
plane
planes
pytorch
removal
reshape
resize
rtn
s
scalar
shape
should
size
slow
small
src
start
stride
stub
sum
sz
tensor
tfinput
than
this
thnn
too
transpose
tuple
tweight
type
undefined
unfolded
unsqueeze
update
valid
variable
view
weight
width
wrapper
zero

?
A
AND
AT
ATen
B
Batch
Bias
CHECK
CONTIGUOUS
Calculated
D
DISPATCH
Define
Do
ERROR
Extract
FLOATING
FORMAT
For
Force
GEMM
GEMV
Given
Helpers
It
K
LEGACY
Long
M
MEMORY
Matrix
MaybeOwned
NaiveConvolutionTranspose
NoTranspose
Note
Option
Output
Resize
Scalar
ScalarType
See
TODO
TORCH
TYPES
Tensor
Transpose
Unpack
Weight
acc
accumulation
after
alpha
always
and
are
args
assumes
aten
back
backward
batch
be
because
beta
bias
bit
borrow
buffer
but
can
cast
channel
check
col
column
columns
com
condition
confusing
const
contains
contiguous
conv
cpp
cpu
cpublas
cublas
cuda
d
defined
depth
dilation
dim
dimd
dimf
dimh
dims
dimw
do
docs
either
elt
empty
equals
ever
expected
false
fgrad
fill
finput
from
gemm
gemv
gets
got
grad
greater
gt
hacky
have
height
http
in
increased
incx
incy
indirectly
input
internal
into
just
k
kernel
kh
kt
kw
lda
like
lt
m
major
mask
matrices
matrix
maybe
message
mode
modules
mulitply
must
n
native
ndim
needs
non
note
nullable
number
nvidia
ones
only
opt
optional
options
or
other
out
output
owned
padding
parameters
per
plane
planes
ptr
pytorch
removal
resize
sample
scalar
scale
see
select
shape
shared
should
size
slow
small
smaller
src
static
stride
temporary
tensor
than
this
too
trans
transpose
tuple
type
u
vol
weight
width
with
wrapper
y
zero

?
ATen
FunctionTraits
INDEX
IsContiguous
NB
STRIDE
Traits
`s`
all
an
and
are
arg
argument
arguments
arity
assert
aten
be
bounds
called
const
contiguous
corresponds
cpp
cpu
enable
eval
exists
first
function
h
index
input
isize
lazy
n
native
no
nullptr
number
or
other
out
output
pytorch
result
s
scalar
see
sizeof
so
src
static
stride
strides
there
traits
type
typename
typically
using
value
void
will

?
API
ATen
Access
Activation
BUFFER
Block
Buffer
C
CHECK
COMBINED
Command
Compute
D
DESCRIPTOR
FN
IMAGE
IMPL
IntArrayRef
It
KERNEL
Kernel
LIBRARY
LIKELY
Layout
Not
OK
Object
Option
Padding
Parameter
Pool
Read
SAMPLER
STORAGE
ShaderDescriptor
Stage
Stride
TORCH
TYPE
Tensor
UNIFORM
VK
VULKAN
Vulkan
Write
access
adapter
adaptive
an
and
api
appropriate
arg
async
aten
barriers
batch
be
block
buffer
but
bypasses
cannot
cast
ceil
cfg
channels
check
command
const
context
convert
count
cpp
d
descriptor
dilation
dim
dimensional
dispatch
divisor
downcast
empty
expects
extents
false
final
format
ft
gpu
group
handle
has
height
image
implemented
implied
include
input
inserts
ivec
keep
kernel
lazy
lifetime
local
m
managed
max
memory
mode
native
necessary
none
normalize
not
object
only
ops
options
output
override
pad
padding
parameter
pool
pooling
pytorch
queue
range
resource
safe
shader
shape
size
src
static
stream
stride
submit
suggest
synchronization
tensors
track
triggers
u
uniform
uvec
v
vTensor
vec
vim
vulkan
width
work

ATen
CHECK
DFTI
DftiErrorClass
DftiErrorMessage
ERROR
Exceptions
FFT
INT
MKL
NO
aten
cpp
error
h
mkl
ostringstream
pytorch
runtime
src
ss
status
str
throw

ATen
CustomBehavior
Device
DispatchKeySet
HasContiguityPolicy
MemoryFormat
MetalTensorImpl
OpaqueHandle
OpaqueTensorImpl
Self
SmallVector
TensorImpl
TypeMeta
aten
base
const
contiguity
contiguous
cpp
custom
d
device
dim
false
format
h
handle
has
key
maybe
memory
metal
name
native
new
opaque
policy
pytorch
set
src
stride
strides
tensorimpl
this
type
u
vec
wrap

ATen
Check
ConvContext
ConvParam
ConvUKernelFunction
Doesn
PyTorchQ
PyTorchQnnpConvQuantizationParams
PyTorchQnnpOperator
PyTorchQnnpStatus
Q
QnnpackDeleter
Run
Setup
Support
ThreadPool
all
always
aten
batch
batches
block
bs
buffer
c
cc
channel
channels
compute
const
context
conv
cpp
cpu
d
de
deconv
deconvolution
decovolution
delete
dims
error
failed
function
going
group
groups
height
image
index
indirect
indirection
input
invalid
invoke
k
kc
kernel
kr
ks
log
m
matter
max
min
mr
n
native
nhwc
no
nr
op
operator
output
packed
parameters
params
pixel
point
points
pthreadpool
pytorch
qnnp
qnnpack
quantization
quantized
range
requantization
round
s
scale
scales
setup
size
sizeof
src
start
status
stride
success
threadpool
tiled
u
uintptr
ukernel
up
usize
vars
void
w
weights
what
width
zero

?
ANEURALNETWORKS
ANeuralNetworksMemory
ANeuralNetworksModel
ANeuralNetworksOperandType
ATen
All
BUFFER
CHECK
ERROR
Get
IMMEDIATE
It
Keep
MEMORY
Maybe
Memory
Model
NNAPI
NO
NULL
NUMBERED
NnapiWrapper
No
SOURCE
Serialized
SerializedModel
SerializedOperand
SerializedOperation
SerializedValue
SourceType
TODO
Unknown
addOperand
addOperation
aligned
and
are
args
arguments
aten
avoid
basically
be
bounds
break
buf
buffer
buffers
byte
bytes
c
calls
case
check
checking
const
consumed
count
cpp
default
dimension
dimensionCount
dimensions
eliminate
end
ensure
enum
error
export
false
format
h
ident
identifyInputsAndOutputs
implemented
index
input
integer
just
len
length
list
load
loader
macro
made
memories
memory
model
models
multiple
next
nnapi
not
number
offset
operand
operands
operation
operations
out
output
outputs
overflow
pad
padded
phys
physical
physically
point
pointer
ptrs
pytorch
rely
required
res
result
returned
rules
scale
ser
serialized
setOperandValue
size
sizeof
small
source
src
stored
sum
switch
these
too
ty
type
u
usize
value
values
version
void
yet
zero
zeroPoint

ATen
CHECK
Check
ConstantPadNd
Length
Only
Pad
Scalar
TORCH
Tensor
The
affine
all
and
are
aten
back
be
but
c
calling
can
clone
const
constant
copy
cpp
diff
dim
dimension
dimensions
emplace
empty
equals
even
false
fill
format
has
idx
in
inp
input
instead
invalid
irange
just
kPerTensorAffine
kPerTensorSymmetric
length
memory
more
must
narrow
native
nd
negative
new
no
non
none
nullopt
number
optimize
options
output
pad
padding
pads
per
plus
point
positive
pytorch
qscheme
quantized
result
resulted
scale
shape
should
size
src
suggest
supported
tensor
than
twice
usize
value
we
which
while
your
zero

ALL
AND
ASSERT
AT
ATen
AutoDispatchBelowAutograd
COMPLEX
CPU
DISPATCH
Derived
Device
FIXME
Ideally
NoTracerDispatchMode
Option
Scalar
ScalarOps
ScalarType
TYPES
Tensor
TensorIterator
This
Types
When
also
and
are
aten
avoidable
be
but
cast
compute
core
cpp
cpu
currently
device
directly
dptr
element
empty
everything
false
fast
fill
filling
going
guard
h
have
implement
implemented
in
inplace
isBoolean
isComplex
isFloatingPoint
isIntegral
kBFloat
kBool
kCPU
kComplexDouble
kDouble
kHalf
kLong
let
manipulate
mode
no
not
now
nullopt
number
opt
or
out
part
pass
ptr
pytorch
result
s
scalar
set
should
skip
src
static
tensor
tensors
there
this
through
toTensor
tracer
track
type
types
unsafeGetTensorImpl
unwrap
value
want
was
way
we
which
without
wrapped

A
AArch
ARM
ARMv
ATen
Assumes
Both
Conversion
Convert
Doing
FP
Floating
However
In
Large
Leverage
NEON
Product
PyTorchQnnpConvQuantizationParams
Rounding
Signed
So
The
This
VLD
We
aarch
absolute
add
adding
addition
after
aligned
always
an
and
any
as
assume
aten
available
be
because
before
better
both
builtin
c
can
cause
channel
clamp
clamped
clamping
consider
const
conversion
cpp
cpu
cycle
don
dup
elements
endif
even
exactly
floating
ft
gemm
generally
gets
has
have
having
high
higher
ifdef
in
index
input
instead
instruction
integer
into
involve
just
k
kernel
lane
large
last
latency
less
lieu
limited
low
magic
make
max
min
mr
multiple
multiply
must
n
native
nearest
necessary
need
neon
none
not
nr
number
offers
only
operation
operations
order
output
overflow
padded
params
per
performed
point
points
predecrement
probably
pytorch
qmax
qmin
qnnpack
quantization
quantized
range
representation
represented
requantization
result
results
round
rounded
rounding
roundings
s
saturated
saturates
scale
scaled
scales
shift
smaller
so
specifically
src
stage
statistically
stride
sub
substracing
than
that
then
they
this
throughput
thus
ties
towards
trick
u
uint
uintptr
ukernel
unbiased
use
using
usize
va
vacc
vaddq
value
values
vb
vcombine
vcvtnq
vcvtq
vdupq
vextq
vfmagic
vfmax
vfmin
vget
vim
vimagic
vld
vmaxq
vminq
vmlal
vmov
vmulq
void
vout
voutput
vqaddq
vqmovn
vqmovun
vreinterpret
vreinterpretq
vs
vshl
vst
vsubl
vsubq
vxa
vxb
w
we
well
which
will
with
works
would
zero

?
@note
A
API
ASSERT
ASan
AT
ATen
Add
An
Arc
Args
Array
ArrayRef
As
AtomicBool
Attempted
Attempting
Attribute
Attributes
Blob
Bool
BoolList
But
C
CHECK
CPU
CUDA
Capsule
Check
ClassType
Complex
ComplexDouble
ComplexDoubleList
ComplexHolder
ConditionVariable
ConstantString
CurClass
DEBUG
DEFINE
DEPRECATED
DataPtr
DataPtrs
Deduplicate
Default
Defined
Device
DeviceIndex
DeviceType
Dict
DictImpl
Dimname
Do
Double
DoubleList
Elem
ElemType
Enum
EnumHolder
EnumType
Error
Errors
Event
Exception
ExceptionPtr
Expected
Explicitly
ExtractTensors
FIXME
FakeType
False
FalseType
Foo
For
From
Future
FutureError
Futures
GIL
Generator
GeneratorImpl
GenericDict
GenericList
Get
GetPyObject
GuardedUnsignedLongUniqueDummy
HashAliasedIValueMap
HashMap
However
INDEX
INFO
INTERNAL
IValue
IValueWithDataPtrs
IValues
If
Immutable
In
IndexSequence
Indices
InferredType
Input
Instead
Int
IntList
Into
IntrusivePtr
IntrusivePtrTarget
It
Its
K
Keep
Key
LOG
Layout
Leaving
List
ListImpl
MESSAGE
MemoryFormat
Methods
MultiStreamGuard
Mutex
NOTE
No
None
Note
NullType
O
ONLY
Object
Only
Option
OptionalArray
OptionalDeviceGuard
Optionally
Output
Please
PyObject
PyObjectHolder
PyObjectHolderInterface
Python
QScheme
Quantizer
RRef
RRefInterface
RefCell
ReferenceWrapper
Remove
Resizing
Scalar
ScalarType
See
Self
Semantics
Shared
Since
Skipping
Slot
Start
Storage
StorageImpl
Str
Stream
String
StringView
StrongTypePtr
TO
TODO
TORCH
Tag
TaggedCapsule
TaggedCapsuleType
Tensor
TensorList
The
There
Therefore
These
They
This
Thus
ToIvalue
ToStr
TorchCustomClassHolder
TorchJitCompilationUnit
TorchList
TorchScript
TorchisCustomClass
Tried
Tries
True
TrueType
TryToInferType
Trying
Tuple
TupleType
Type
TypePtr
U
UB
UNLIKELY
Undefined
UndefinedTensorImpl
UniqueLock
Unknown
Unlike
Use
User
V
VALUE
Value
VirtualGuardImpl
Wait
We
Wrappers
You
\n
\nOriginal
`
`obj
above
abundantly
access
accessor
account
acquire
acquiring
actual
actualDataPtrs
add
addCallback
adding
additional
address
after
again
all
allow
allowed
allowing
allows
already
also
always
among
an
and
another
any
apply
appropriate
are
args
around
arrayn
as
assert
assume
async
aten
attempt
attr
attribute
attributes
automatically
b
back
base
based
be
because
before
begin
behavior
benchmarks
binds
blob
block
blocked
both
bounding
boxed
but
c
cached
call
callback
callbacks
called
caller
calling
callsite
can
cannot
capsule
careful
case
cases
cast
catch
cause
cb
cbs
characters
check
checkCustomClassType
checks
child
childFut
children
choose
class
classType
clear
clearToNone
collect
com
comment
commented
compacting
compare
comparisons
compilation
compiler
complete
completed
completes
completion
complex
computed
concrete
conditional
consistent
const
constexpr
constructible
constructor
contain
contained
continue
convert
convertible
converts
copy
core
correctness
correspond
could
cpp
cpu
create
createInstance
createVectorFromList
created
creating
cu
current
currentDevice
currently
custom
customized
cv
dataPtrs
deduplicate
deep
deepcopy
default
define
defined
dependency
deprecated
derive
destroy
destroyed
destructor
detail
detectable
device
deviceCount
deviceGuard
deviceType
devices
dict
did
didn
difference
directly
discoverable
disjunction
distinct
distinction
do
doesn
don
double
due
dummy
duplicate
duplicates
dynamic
effectively
efficient
either
elem
element
elements
emplace
empty
enable
end
endif
ensure
ensureIsSubsetOfDevices
ensures
enum
environment
eptr
equality
erase
error
errors
even
event
events
example
exception
excessDevices
execute
executed
exists
expand
expanded
expected
explanatory
explicit
export
extract
extractDataPtrs
extracted
extraction
fairly
fake
false
far
fast
field
finished
first
fixed
flag
following
force
forgetting
format
formatSetOfDevices
forward
from
ft
fully
function
functions
functor
future
futures
gap
generator
generic
getCustomClassType
getDevice
getDevicesOfDataPtrs
getMethod
getPyObject
getSlot
getStream
getStreamFromGlobalPool
getSubValues
getTypeOfDevices
getTypePtr
github
given
got
guarded
h
happen
happens
has
hasError
hash
hashmap
have
held
here
hold
holder
holding
hot
how
https
identity
idx
ie
immediately
implementation
implementing
important
in
including
inconsistent
incref
index
indices
infer
information
inherit
inherited
init
inl
input
insert
inserter
instance
instantiate
instead
intends
intermediateFut
internal
into
introspecting
intrusive
inversion
invocable
invoke
invokeCallback
invoking
isBlob
isBool
isBoolList
isCapsule
isComplexDouble
isComplexDoubleList
isDeviceUsed
isDouble
isDoubleList
isEnum
isFuture
isGenerator
isGenericDict
isInt
isIntList
isList
isNone
isObject
isPyObject
isQuantizer
isRRef
isStorage
isString
isTensor
isTensorList
isTuple
isn
issues
item
items
its
itself
iv
ivalue
just
kCPU
kernels
key
know
lambda
lazily
lazy
lead
leads
let
lib
libtorch
like
likelike
list
lists
ll
lock
log
logic
long
looks
lookup
lookups
lot
lvalue
macro
make
manipulate
manually
map
mark
markCompleted
marked
members
memcmp
memo
message
method
methods
might
mismatch
module
more
move
moveToIntrusivePtr
moved
msg
much
multiple
must
mutex
name
named
native
necessarily
necessary
need
needed
needs
negation
new
no
non
none
not
note
notify
now
null
nullopt
nullptr
numSlots
obj
object
objects
omitting
once
one
only
op
operation
operator
ops
option
optional
or
order
original
oss
ostringstream
other
our
out
output
overload
padding
parameter
parent
parentFut
pass
passed
payload
perf
performing
pickling
pointed
pointer
pointers
possible
potential
potentially
preparation
present
prevent
primitive
principle
private
propagation
proper
properly
provide
provided
provider
ptr
ptrs
push
py
python
pytorch
qualified
quantizer
queried
r
random
rare
raw
reasons
reclaim
record
recordDataPtrOnStream
recorded
recover
ref
refcounting
reference
references
regardless
registered
registration
release
remove
removing
reportToTensorTypeError
required
requires
reserve
reside
residing
resize
resizeObject
responsible
restore
result
rethrow
retrieve
returned
rhs
right
rref
rules
running
runtime
s
safety
same
second
see
seen
sequence
set
setError
setErrorInternal
setSlot
sets
setting
shallow
shared
should
shouldn
shrink
signature
silent
simple
since
singleton
size
sizeof
skip
slight
slot
slots
slow
so
sort
sortAndDeduplicateDevices
sorted
sourceIdx
special
specialized
spew
src
srcs
state
static
statically
steps
still
storage
store
stored
str
stream
streamGuard
streams
stuff
subset
successfully
superset
support
supposed
sure
swap
synchronization
synchronize
synchronizeWithCurrentStreams
tag
tagKind
tagged
take
targetIdx
tell
templated
tensor
tensors
testify
than
that
thats
their
them
then
there
this
through
throw
thus
tie
toBlob
toBool
toBoolList
toCapsule
toComplexDouble
toComplexDoubleList
toComplexDoubleVector
toCustomClass
toDevice
toDimname
toDouble
toDoubleList
toDoubleVector
toFuture
toGenerator
toGenericDict
toIValue
toInt
toIntList
toIntVector
toIntrusivePtr
toLayout
toList
toListRef
toMemoryFormat
toObject
toOptionalStringRef
toPyObjectHolder
toQScheme
toQuantizer
toRRef
toScalar
toScalarType
toStorage
toStream
toString
toStringRef
toStringView
toTensor
toTensorList
toTensorVector
toTuple
toTypedDict
toTypedList
toX
trait
trim
try
tryRetrieveErrorMessageInternal
trying
tuple
tuples
ty
type
typename
types
u
unboxed
unchecked
undefined
union
unique
unit
unlock
unnamed
unordered
unpack
unqualified
unsafe
unsigned
until
unwrap
up
upcast
updated
us
use
used
usedDevices
user
userObj
users
using
usize
usual
util
v
val
vals
value
values
vec
vectors
versa
version
vice
view
virtual
void
wait
waited
want
warning
was
way
we
well
were
what
which
why
will
win
with
without
works
worsen
wrapper
writing
yaml
you
~Tensor

ATen
CPU
DimnameList
DispatchKey
DispatchKeySet
Factory
GetDefaultMobileCPUAllocator
If
MemoryFormat
Remove
Size
Storage
TODO
Tensor
TensorImpl
There
TypeMeta
accept
allocate
allocator
allocators
and
are
as
aten
because
but
byte
bytes
const
contiguous
copy
cpp
custom
destination
directly
either
empty
final
format
function
h
hand
in
input
integers
into
intrusive
itemsize
make
maybe
memory
mobile
modified
multiply
namedinference
names
native
need
needed
nelements
no
nonempty
not
only
onto
operation
options
or
other
our
padded
padding
propagate
ptr
pytorch
reallocate
requested
resizable
resize
same
size
source
src
storage
tail
temporary
tensor
then
there
this
use
usize
utils
with

ATen
INTERNAL
PYTORCH
QNNP
aarch
aten
c
channel
const
conv
cpp
cpu
declare
export
function
h
ident
index
kc
ks
macro
mr
name
native
neon
nr
output
params
pytorch
qnnp
qnnpack
quantization
quantized
rules
src
sse
stride
u
ukernel
union
usize
void
w

ASSERT
AT
ATen
CHECK
ChannelsLast
D
DISPATCH
It
MemoryFormat
Non
Option
QINT
Scalar
TORCH
TYPES
Tensor
affine
aten
b
but
c
case
cast
channels
compute
const
contig
contiguous
copy
cpp
cpu
d
depth
dim
empty
equals
expected
factors
format
frame
ft
functions
got
h
height
idata
index
input
irange
just
memcpy
memory
native
nbatch
nearest
neighbor
nhwc
none
nullopt
o
odata
options
osize
out
output
point
pos
ptr
pytorch
quantized
qupsample
reinterpret
scalar
scale
scales
size
sizeof
source
special
src
static
suggest
tensor
type
typename
underlying
upsample
value
vim
w
width
with
yaml
zero

@note
A
ASSERT
ATen
Adds
BATCH
BatchDim
BatchDims
BatchDimsRef
Batched
BatchedTensor
BatchedTensorImpl
BitSet
BufWriter
CHECK
CPUs
Can
Checks
Contiguous
Creates
CustomBehavior
DEBUG
DIMS
DispatchKey
DispatchKeySet
Display
Error
Example
For
Formatter
Given
HasContiguityPolicy
INTERNAL
If
It
LEVELS
MAX
MemoryFormat
Most
NB
NOTE
NUM
NYI
ONLY
Override
PDEP
Please
Result
Returns
SIZE
STACK
See
Self
SmallVector
Store
TENSOR
TODO
TORCH
Tensor
TensorImpl
That
The
There
They
This
Use
VMAP
We
`
`bdims`
`dim`
`level`
`maybeGetBatchedImpl`
`non
`tensor`
`value
access
actual
actualDim
add
adjust
all
an
and
answer
are
as
asking
assert
assertions
assume
assumes
aten
back
backed
base
batch
batchdim
batched
bdim
bdims
be
begin
being
bit
bitset
bitset?
bt
bunch
but
call
called
can
cast
centralized
cfg
check
checkInvariants
clean
codebase
com
compatible
const
construct
contiguity
contiguous
continue
corresponds
count
count`
cpp
create
createBatchDimBitset
createVmapLevelsBitset
created
custom
debug
defined
definition
detail
device
dim
dimension
dimensionality
dimensions
dims
do
does
dropped
ed
effectively
elements
emplace
end
equivalent
error
exactly
example
exposed
false
few
find
fmt
following
format
from
gives
got
h
has
have
hit
holds
https
identifier
in
incompatible
increasing
index
indexed
indicating
inherited
inplace
inside
instruction
internal
invariant
invariants
io
irange
isBatchedTensor
isn
j
kBatchDimsStackSize
kVmapMaxTensorDims
kVmapNumLevels
key
less
let
level
levels
list
lvl
make
makeBatched
maximum
maybe
maybeGetBatchedImpl
mean
means
memory
messages
methods
might
move
must
name
ndim
necessary
nested
never
new
newer
non
normal
not
nth
nullptr
number
occur
offset
older
ones
only
operation
operations
or
order
other
out
over
people
performs
physical
place
places
policy
possible
present
prev
private
probably
public
publically
pytorch
querying
questions
range
rd
reduction
reference
refresh
regular
represent
represents
require
resize
restrict
result
returning
returns
rzou
s
said
see
set
should
size
so
some
src
stack
stackoverflow
static
storage
stored
stride
strides
sum
support
supports
tensor
tensorimpl
tensors
term
than
that
then
there
they
this
throw
torch
treated
tuple
type
u
unchecked
underlying
unsafe
unsafeGetBatchedImpl
unsafeGetTensorImpl
unwrap
up
us
use
user
valid
value
visible
vmap
vmapped
vmaps
was
we
whenever
which
will
with
wrap
wraps
zero
~

API
ATen
AVAILABLE
For
LaunchParams
MPSCNN
MPSCNNUtils
MPSImage
MTLSize
String
Thus
To
align
always
arrayKernel
aten
be
can
compute
const
cpp
featureChannels
floor
grid
h
iOS
image
in
input
ios
just
kernel
left
look
looks
macos
match
metal
mps
mpscnn
native
need
negative
non
nonArrayKernel
numberOfImages
offset
original
pad
padding
per
pixel
pt
pytorch
s
set
should
size
src
that
this
threadgroup
threadgroups
threads
top
up
values
we
with

?
ARM
ATen
Arguments
C
Columns
Depthwise
DepthwiseConvKernel
H
Input
IntArrayRef
Iterate
LIKELY
NEON
Non
Output
Padding
Rows
Stride
TILE
TODO
Tensor
W
Winograd
\
aarch
accesses
all
args
aten
avoid
b
batch
bias
block
bounds
breaking
c
calc
calculate
case
cfg
col
cols
const
conv
convolution
cpp
cpu
d
declare
define
defined
depthwise
dimension
dimensions
dispatch
empty
end
endif
fast
groups
h
half
hxw
ih
in
inplace
input
iw
junk
k
kernel
lane
layer
lazy
load
low
m
n
native
neon
non
not
oblock
operator
options
oth
otw
out
output
over
pad
padded
padding
parallel
path
plus
potentially
ptr
pytorch
regime
register
ret
row
rows
size
spilling
src
start
static
stride
stub
tile
tiles
transform
transpose
undefined
using
usize
v
val
vbias
vdupq
vextq
vfmaq
vfmsq
vget
vld
vmlaq
vmlsq
vmuladdq
vmulq
vmulsubq
vs
vsetq
vst
w
wd
weight
winograd
zeros

ATen
MM
SHUFFLE
address
aten
c
const
cpp
cpu
cvtsi
decrement
do
epi
ft
hi
ii
increment
input
k
last
lo
loadl
loadu
m
mm
n
native
none
o
output
pytorch
qnnp
qnnpack
quantized
shufflelo
si
src
srl
sse
u
uintptr
unpackhi
unpacklo
usize
vim
void
vshift
vw
vx
vxy
vxyzw
vy
vz
vzw
w
while
xm
y
z
zip

ARM
ATen
GemmMicrokernelTester
NEON
REQUIRES
TEST
aStride
aarch
any
arch
arm
aten
c
cStride
cc
cfg
cpp
cpu
div
eq
gt
iterations
k
kr
m
mod
mr
n
native
neon
np
nr
psimd
pytorch
qmax
qmin
qnnpack
quantized
rmin
sgemm
src
strided
subtile
super
test
u
ukernel
use
usize

ATen
CppFunction
Dimname
DimnameList
IMPL
IntList
LIBRARY
Named
NamedRegistrations
Scalar
TORCH
Tensor
These
abs
absolute
acos
acosh
add
addcdiv
addcmul
addmm
addmv
alias
align
all
and
angle
any
arccosh
are
as
asin
asinh
atan
atanh
aten
backward
because
behavior
bernoulli
bitwise
bmm
but
cat
cauchy
cdist
ceil
chunk
clamp
clone
coalesced
complex
conj
contiguous
copy
core
correction
cos
cosh
cpp
cummax
cummin
cumprod
cumsum
d
deg
dense
detach
device
diagonal
digamma
dim
dimname
div
divide
dot
dropout
ellipsis
empty
eq
equal
erf
erfc
erfinv
exercised
exp
expand
expm
exponential
fallback
fill
flatten
floating
floor
forward
frac
from
full
fused
fw
ge
geometric
grad
gt
hypot
idx
igamma
igammac
imag
implicitly
index
indices
inference
ints
isfinite
isinf
isnan
item
kthvalue
layout
lazy
le
leaf
lgamma
like
local
log
logaddexp
logcumsumexp
logical
logsumexp
lt
m
makeFallthrough
makeNamedNotSupported
manually
marked
masked
matmul
max
mean
median
min
mm
mode
mul
mv
named
names
nanmedian
narrow
ne
neg
nextafter
nonzero
normal
not
nr
old
ones
or
out
output
pinned
point
polygamma
pool
pow
preserve
primal
prod
pytorch
rad
rand
randn
random
real
reciprocal
refine
registered
registrations
relu
rename
requires
reshape
resize
result
retain
right
round
rsqrt
rsub
same
scalar
sections
select
set
sigmoid
sign
signbit
signed
sin
sinh
size
slice
softmax
some
sparse
split
sqrt
square
squeeze
src
static
stride
strided
sub
suite
sum
supported
supporting
sure
tan
tanh
tensor
tensors
test
them
these
they
threshold
transpose
trunc
type
unbind
unflatten
uniform
unsafe
using
values
vander
var
version
were
weren
with
xor
zero
zeros

ATen
Box
CHECK
In
IntrusivePtr
LinearPackedParamsBase
LinearPackedSerializationType
Option
PYTORCH
PackedLinearWeightQnnp
PytorchQnnpOperator
QNNPACK
QScheme
QnnpackBCSRMatrix
QnnpackOperatorDeleter
Refacto
ReluFused
Self
Seperate
Static
TODO
TORCH
Tensor
This
across
ao
apply
as
aten
base
bcsr
be
bias
block
can
case
cfg
compy
const
copy
cpp
cpu
default
does
dynamic
exist
expects
false
features
fill
from
generic
h
in
input
just
linear
matrix
native
needed
new
not
nullptr
op
operator
optional
orig
out
output
point
points
prepack
present
pytorch
qnnpack
quantization
quantized
reference
relu
requantization
scale
scales
scheme
separate
size
so
sparse
sparsity
specific
src
that
u
unimplemented
unpack
utilities
utils
w
we
weight
with
zero
zeros

ASSERT
AT
ATen
CHECK
Cannot
Copy
Device
DeviceGuardImplInterface
ERROR
Explicit
Layout
MemoryFormat
No
Note
Option
Preserve
ScalarType
See
Storage
TORCH
Take
Tensor
TensorConversions
TensorImpl
TensorList
TensorOptions
This
Unsupported
Viewing
While
XLA
according
across
actual
all
and
argument
as
aten
b
back
backend
backends
backward
based
being
blocking
both
but
bytes
c
call
coalesce
const
converting
converts
copy
corresponding
cpp
cpu
cuda
current
d
delete
dense
device
different
doesn
element
empty
ensure
expects
explicit
flag
format
ft
function
getDevice
getDeviceGuardImpl
got
grad
graph
hacky
has
have
having
implementation
important
in
independently
index
input
itemsize
kMkldnn
kSparse
kStrided
key
layout
lazy
like
loops
make
mask
may
memory
merge
meta
might
mkldnn
move
native
need
new
non
none
not
nullopt
number
offset
op
opt
optional
options
or
other
out
overlapping
per
pin
pinned
please
primarily
push
pytorch
r
redundant
removal
representing
requires
scalar
scalarTypeToTypeMeta
set
setter
size
sparse
specified
src
storage
strided
strides
suggest
support
supported
supports
sync
tell
tensor
tensors
that
this
through
time
type
unsafeGetTensorImpl
unset
updates
value
vanilla
view
vim
with
wrapper

API
ASSERT
ATen
After
An
Any
Appends
Arc
Args
Assigns
Calling
Changes
Checks
ConstReference
Constructs
Create
Erases
Example
GenericList
IValue
Identity
If
Increase
Index
IndexMut
InitializerList
Inserts
InternalConstReferenceType
InternalReferenceType
IntrusivePtr
IntrusivePtrTarget
Invalidates
Invariant
It
Iterator
Kernels
List
ListElementConstReferenceTraits
ListElementReference
ListImpl
ListIterator
ListType
Lists
May
Moves
Option
Ord
Ordering
Output
PartialEq
PartialOrd
PyTorch
RandomAccessIteratorTag
ReferenceWrapper
Removes
Resizes
Returns
ReverseIterator
See
Self
SizeType
Some
String
TODO
Test
The
There
This
Type
TypePtr
Uses
ValueType
We
You
`rhs`
`this`
access
acts
additional
afterwards
all
allocation
allow
allows
also
always
an
and
another
any
append
appended
are
args
arguments
as
assign
assigning
assigns
aten
attempting
b
back
backwards
base
be
because
before
begin
behavior
being
both
bounds
breaking
but
call
can
cannot
cap
capacity
cbegin
cend
checking
class
clear
cmp
comparison
compatibility
concrete
const
constructed
contain
contained
container
contains
copies
copy
core
count
cpp
current
currently
decrement
deep
default
difference
directly
do
element
elementType
elements
emplace
empty
end
eq
equal
equality
equals
erase
except
exception
exists
extract
fastEqualsForContainer
first
following
friends
from
generic
given
greater
h
has
how
however
implementation
in
increment
index
information
initial
insert
inserted
instance
instead
internal
internally
into
intrusive
invalid
invalidate
invalidated
iter
iterator
iterators
kernel
last
less
lhs
list
lists
location
lst
make
may
maybe
memory
minus
mod
most
move
moving
mul
never
new
no
not
nullptr
number
object
offset
one
only
operations
operator
ops
optimizations
optional
or
original
other
our
out
overload
part
partial
past
placeholder
plus
pointer
pointers
pointing
pop
pos
position
prefix
prim
ptr
public
push
pytorch
random
range
re
read
ref
reference
references
referring
reflected
rely
represents
reserve
resize
results
returned
returns
reverse
rhs
runtime
s
same
see
separate
set
setting
share
should
size
some
specified
src
storage
store
stores
super
supposed
sure
swap
switch
tag
than
that
there
this
three
thrown
traits
type
typed
types
undefined
underlying
unsafe
us
use
used
user
usize
v
valid
value
values
vec
versa
vice
why
will
with
within
without
works
wraps
you
zero

ASSERT
ATen
Assigning
C
ChannelsLast
Check
FALSE
H
If
MemoryFormat
QTensor
TRUE
Tensor
Tie
W
We
added
affine
and
api
as
assume
aten
axis
be
before
blob
breaking
c
cast
ch
channel
channels
check
clamp
const
correct
cpp
cpu
create
d
defined
dequant
dequantization
dequantize
device
double
downsize
during
element
elements
empty
expect
expected
failed
format
from
ft
greater
idx
image
inv
item
kCPU
kDouble
kFloat
kLong
kQInt
kQUInt
last
max
memory
min
mode
native
nearbyint
new
none
ones
per
perchannel
point
points
ptr
pytorch
qint
qr
qtensor
quant
quantization
quantize
quantized
quint
qval
qx
r
rand
randint
range
repr
reqr
requantization
requantize
result
round
rounding
rqr
s
scale
scales
size
src
static
tensor
test
than
that
toType
u
usize
val
value
values
vim
will
wrong
zero

?
ASSERT
ATen
CHECK
Cannot
DimMask
DimVector
EQUALITY
Expected
Got
INDEX
INTERNAL
IdxVec
IntArrayRef
Long
Maximum
None
OPTION
Option
ReduceOpsUtils
Resize
Return
Scalar
ScalarType
TORCH
TYPE
Tensor
TensorIterator
This
We
You
all
allreduce
an
and
are
argument
as
aten
avoid
b
back
be
begin
bitset
bound
but
c
call
called
case
check
common
computational
const
containing
cpp
create
cross
cuda
d
defined
depending
device
dim
dimension
dimreduce
dims
don
efficiency
empty
end
ensure
equal
equality
erase
exists
expected
export
false
fill
flip
found
from
function
generalize
got
gpu
h
has
have
ident
identity
improves
in
includeBool
including
indices
infinities
infinity
input
insert
inside
integer
irange
isIntegralType
kBFloat
kFloat
kHalf
keepdim
kernel
launches
layout
likely
lim
limits
list
lower
lowest
lowp
macro
make
mask
match
max
minimum
mismatched
mixed
must
name
namedinference
names
native
ndim
ndimension
new
no
non
nonempty
not
nullopt
op
operator
opt
option
options
or
out
output
performs
possible
precision
product
promotion
propagate
provided
push
pytorch
reduce
reducing
reduction
replacement
resize
resizing
restride
result
results
returning
review
rules
same
scalar
scalarType
setup
shape
should
size
special
squeeze
src
stride
strided
strides
templated
tensor
tensors
that
this
toString
toType
tried
trivial
type
types
u
undefined
upcast
upper
using
value
values
vec
viewed
want
was
with
you
zero

AT
ATen
CONJUGATE
Conjugate
Divide
E
ERROR
EVEN
FFT
FftNormMode
Fourier
Hence
Hermitian
In
K
MKL
NOTE
No
Normalization
Note
Real
STORAGE
ScalarType
See
SpectralOps
SpectralOpsUtils
Symmetry
Tensor
That
The
Therefore
This
Transform
True
about
also
and
assuming
assumption
aten
avoid
back
base
be
being
both
c
calculates
cases
com
complex
conjugate
const
cpp
cuFFT
cuda
cufft
declare
default
developer
dfti
dim
dimension
dimensional
dimensionsal
dims
dispatch
doc
docs
due
en
enum
even
expected
fft
fill
fills
flag
floor
following
frequency
from
ft
full
function
given
h
half
halved
have
html
http
https
in
incompatible
index
infer
information
intel
j
k
last
let
libraries
loses
maps
mirror
mkl
mod
multi
n
native
none
normalization
nvidia
one
onesided
only
operator
optional
or
ostringstream
other
out
parameter
pytorch
real
redundancy
reference
returned
root
roughly
satisfies
shape
should
sided
signal
size
software
some
sqrt
src
ss
storage
str
strides
stub
such
symmetry
takes
that
this
transform
transformed
twosided
type
types
unwrap
us
used
using
values
void
we
will
with

?
ATen
BackwardFn
C
CHECK
CONTIGUOUS
Expected
FORMAT
ForwardFn
GroupNormBackwardKernel
GroupNormKernel
HxW
LEGACY
MEMORY
MaybeOwned
Note
Option
Ported
See
TORCH
Tensor
Y
add
addcmul
affine
and
aten
backward
batch
be
beta
bias
borrow
but
c
cbegin
cend
channels
const
contiguous
cpp
cudnn
dX
dY
dbeta
declare
define
defined
deprecated
device
dgamma
dim
dispatch
divisible
dx
dy
empty
enabled
eps
equal
forward
from
gamma
got
grad
group
groups
h
hacky
hxw
in
input
integers
kEmpty
layout
like
make
mask
math
maybe
mean
memory
momentum
mul
multiply
native
norm
nullopt
number
opt
optional
options
or
out
outputs
owned
param
pin
pytorch
removal
repo
reshaped
rstd
running
shape
size
src
tensor
training
tuple
type
value
var
view
void
weight
wrapper
xla

ASSERT
ATen
Compute
Create
Hardswish
HardswishOperatorTester
NE
NEAR
Verify
and
assert
aten
batch
batchSize
begin
bind
c
channels
const
cpp
cpu
create
default
delete
destroy
device
distribution
end
fill
ft
generate
h
hardswish
hardswishOp
hardswishX
initialize
input
inputScale
inputStride
inputZeroPoint
isnormal
iteration
iterations
max
min
mt
native
nc
none
nullptr
operator
output
outputRef
outputScale
outputStride
outputZeroPoint
point
pool
pytorch
qmax
qmin
qnnp
qnnpack
quantized
random
randomDevice
ref
reference
results
rng
scale
scaledHardswishX
setup
size
src
status
stride
success
test
tester
testq
this
thread
u
uniform
usize
vim
xA
y
zero

ATen
Arc
Auto
BUILD
C
Checking
DebugInfoGuard
Drop
GradMode
Guard
IS
InferenceMode
JIT
LocalDispatchKeySet
MOBILE
Note
Propagate
RecordFunction
RecordFunctionTLS
Saves
Self
Sets
Special
TLS
Thread
ThreadLocalDebugInfo
ThreadLocalState
ThreadLocalStateGuard
Whether
XPLAT
according
across
after
all
and
are
args
as
async
aten
autograd
be
being
boundaries
boundary
bumpRecordAllFunctions
bumped
callback
callbacks
calling
care
cfg
change
checkRecordAllFunctions
contains
corresponding
cpp
created
current
debug
decltype
default
defined
dispatch
does
doesn
drop
enabled
endif
engine
false
force
forceCurrentDebugInfo
fork
forward
frequency
from
function
functions
given
global
grad
h
handling
has
hence
high
in
inference
info
into
its
keep
key
launch
let
local
mode
move
new
non
not
optimization
or
parallel
pass
passing
pre
preserve
preserved
preserving
prev
previously
propagate
pytorch
re
record
releaseRecordAllFunctions
reset
resets
restore
returned
returns
rf
s
samping
sampled
sampling
set
setThreadLocalState
simply
specified
src
state
takes
task
that
them
there
thread
tls
unwrap
value
values
variables
was
whether
with
wrap

?
AT
ATen
CHECK
ChannelsLast
DISPATCH
MemoryFormat
Option
QINT
Scalar
TORCH
TYPES
Tensor
affine
aten
avgpool
b
be
ceil
channel
const
contiguous
count
cpp
cpu
d
dD
dH
dW
dd
define
device
dh
dispatch
divisor
downcast
dw
either
empty
end
fast
format
has
include
input
inputDepth
inputHeight
inputWidth
ints
kD
kH
kW
kd
kernel
kh
kw
last
make
memory
mode
must
nInputPlane
native
nbatch
ndimension
nhwc
not
nullopt
omitted
options
or
output
outputDepth
outputHeight
outputWidth
override
pad
padD
padH
padW
padd
padding
padh
padw
parallel
path
point
pool
pooling
pytorch
qavg
quantized
safe
scalar
scale
shape
single
size
src
start
stride
stub
suggest
three
tie
tuple
type
value
zero

ATen
C
CHECK
Cannot
CheckedFrom
Device
List
MaybeOwned
MemoryFormat
Note
Operators
Option
Remove
TODO
TORCH
Tensor
Tensor?
TensorList
TensorOptions
The
This
To
UNLIKELY
We
adaption
addressed
an
and
arg
argName
argument
arguments
as
assemble
aten
b
beginning
borrow
both
but
c
cannot
changed
check
com
common
const
constructed
core
cpp
creation
d
default
defined
delete
device
directly
dispatcher
expected
expects
explicit
extract
failure
false
following
format
from
function
github
going
grad
h
hacky
handle
has
https
implementation
implemented
in
instead
isn
issues
kernel
layout
make
marked
may
maybe
memory
method
methodName
name
nullopt
once
op
opt
optional
options
owned
parameters
pin
pinned
please
pytorch
redundant
registration
removal
remove
requires
schema
separate
set
setter
src
take
takes
taking
tensor
tensors
these
this
through
unwrap
update
value
want
weight
with
without
wrapper
yet

ASSERT
AT
ATen
Copy
Copying
DispatchKeySet
Enum
Expose
For
Frontend
ImplType
IntrusivePtr
NOTE
Note
OpaqueTensorImpl
PyTorch
QTensor
QTensorImpl
Quantized
Quantizer
QuantizerPtr
Return
See
Self
Shallow
Storage
TODO
Tensor
TensorImpl
Tensors
TypeMeta
VariableVersion
We
`
`allow
`version
allow
and
another
aten
base
cast
change
change`
check
compatible
const
contiguous
copies
copy
counter
counter`
cpp
dest
detach
documentation
doesn
fields
from
function
h
has
in
information
into
intrusive
key
ll
make
metadata
more
move
name
new
offset
one
or
parameters
please
pointer
pytorch
quantization
quantized
quantizer
refer
refresh
s
scheme
see
set
shallow
specific
specifies
src
static
storage
stores
strides
tensor
tensorimpl
that
this
ty
type
u
usage
use
version
which
why
with

?
ATen
ClampingParams
LIKELY
PYTORCH
PyTorchQnnpU
QNNP
assert
aten
builtin
c
clamp
const
cpp
cpu
do
epi
epu
ft
increment
load
loadl
loadu
m
max
min
mm
n
native
none
output
params
prefetch
pytorch
qnnpack
quantized
si
src
sse
storel
storeu
u
uintptr
ukernel
usize
vim
vout
voutput
vx
vy
while
y

ATen
Alt
Array
BINARY
Broadcast
CLAMP
Conversions
Currently
DEFINE
EMULATE
FloatVecReturnType
IntVecReturnType
MEMBER
OP
Self
SizeType
The
This
ValueType
Vbool
VecInternalMaskType
VecInternalType
Vectorized
VectorizedQint
Vint
When
acceptable
add
align
already
always
an
and
are
argument
arithmetic
as
aten
b
bandwidth
be
bound
c
carried
cases
cast
classes
cmpeq
cmpge
cmpgt
cmple
cmplt
cmpne
const
constexpr
constructor
converters
count
cout
cpp
cpu
defines
dequantize
doing
dump
efficient
elementwise
endl
expected
file
floating
follows
from
full
function
h
in
inp
inverse
iterations
kernels
lazy
ld
let
loadu
loop
madd
max
maximum
memcpy
min
minimum
mod
mul
multiplier
new
offset
operations
operator
operators
or
other
out
over
packs
point
precision
premul
ptr
pytorch
qint
quantize
quantized
reinterpret
relu
requantize
returned
rhs
rint
scale
signed
simply
six
size
sizeof
special
specified
splats
src
st
static
store
sub
subtract
super
that
these
tmp
type
types
u
underlying
union
unpackh
unpackl
unwrap
use
usize
usually
v
val
vals
value
values
vec
vecBi
vecBshi
vecb
vecf
veci
vecs
vecshi
vectorized
vectors
vf
vfloat
vi
vint
vmask
vmax
vmin
void
vsx
we
widening
will
with
writing
xor
zero
zp

AT
ATen
AVX
Actually
Apparently
As
Buck
Build
C
CHECK
COMPILING
CPU
CPUCapability
CROSS
CXX
Caffe
Cribbed
Cross
DEFAULT
DEFINITION
DNN
ENABLED
FBCODE
FLAGS
FLAGS`
FULL
GCC
GNUC
GetBuildOptions
Git
HAVE
HIP
Hash
ISA
Intel
It
LAPACK
MACOSX
MINOR
MKL
MKLDNN
MLC
MSC
MSVC
MacOSX\n
Magic
Maratyszcza
NNPACK
NO
No
OPENMP
OSS
OpenMP
PyTorch
R
Reference
Returns
String
TODO
TORCH
Too
VER
VSX
Version
XLA
\n
`CXX
`get
actually
article
aten
bad
blog
break
buf
buffer
build
built
c
can
capability
case
char
check
clang
com
common
compilers
compiling
config
configuration
const
conveniently
cplusplus
cpp
cpu
cxx
default
defined
describing
detailed
developer
dnn
do
documentation
does
empty
en
enabled
enabled\n
endif
environment
etc
false
field
first
flags
flags`
found
from
gcc
getCUDAHooks
github
guide
h
hasCUDA
hash
how
html
https
ideep
ifdef
in
info
intel
issues
j
k
kowalczyk
linux
macros
major
minor
mkl
mkldnn
msvc
native
no
not
nullptr
number
one
only
openmp
options
ostringstream
override
pair
patch
patchlevel
picked
populate
possible
predefined
provided
pytorch
questions
record
reference
result
second
settings
show
showConfig
software
src
ss
stackoverflow
str
switch
that
us
usage
used
usually
v
variable
ver
verbose
version
version?
way
we
which
with

?
API
Aten
BUFF
CPP
CPU
DESC
Debug
Declarations
DimVector
DispatchKey
Empty
FILE
GENERIC
INVALID
In
IntArrayRef
It
LEN
Make
NB
NOTE
NULL
Note
PRId
Please
Pointer
Resize
STOP
Scalar
See
Slow
So
StorageImpl
TH
THArgCheck
THBFloat
THBoolTensor
THByteTensor
THCharTensor
THComplexDoubleTensor
THComplexFloatTensor
THDescBuff
THDoubleTensor
THFloatTensor
THHalfTensor
THIntTensor
THLongTensor
THQUANTIZED
THShortTensor
THSizeDesc
THStorage
THTensor
Tensor
TensorImpl
These
Thinking
This
TypeMeta
UndefinedTensorImpl
You
abstraction
access
alias
already
an
and
are
as
aten
available
backwards
be
being
break
buf
call
cannot
cast
char
check
clone
compatibility
const
contiguous
copy
cpp
creation
current
cwrap
d
dD
define
desc
dim
dimension
dimensions
directly?
dispatch
distinct
do
documentation
doing
don
dst
endif
especially
et
everything
exist
false
firstIndex
four
free
freeCopyTo
ft
functions
general
generic
genericized
getSizePtr
getStoragePtr
h
happen
have
header
here
hpp
ifdef
ifndef
in
including
incref
indices
init
intrusive
isContiguous
isSameSizeAs
isTransposed
la
lazy
legacy
les
likely
lua?
make
max
may
measure
methodes
methods
might
must
n
nDimension
nDimensionLegacyAll
nDimensionLegacyNoScalars
nElement
narrow
native
need
new
newClone
newContiguous
newNarrow
newSelect
newSize
newStride
newTranspose
newWithSize
newWithStorage
newWithTensor
no
none
not
nullptr
offset
one
only
out
probably
ptr
ptrdiff
put
pytorch
quantized
range
raw
read
reclaim
release
resize
resizeAs
resizeNd
retain
select
set
setStorage
should
simply
single
size
sizeDesc
sizeLegacyNoScalars
sliceIndex
snprintf
some
squeeze
src
static
still
storage
storageOffset
storageoffset
str
stride
strideLegacyNoScalars
strides
stringify
support
tensor
terminated
that
these
they
this
three
torch
transpose
treat
tricks
type
types
un
undef
unless
unsafeReleaseTensorImpl
unsqueeze
use
used
usize
value
values
via
vim
violation
void
vs
want
wrap
you
z

ATen
Apply
ArgNames
Args
Average
BENCHMARK
Benchmark
BenchmarkState
C
Global
H
IH
IW
ImageNetArguments
MAIN
NO
PYTORCH
Pooling
QNNPACK
SetBytesProcessed
SkipWithError
W
arguments
aten
average
b
batchSize
begin
bench
bind
cc
channels
const
cpp
cpu
create
delete
device
distribution
end
endif
failed
flags
generate
global
globalPoolingOperator
ifndef
image
initialize
input
inputHeight
inputPixelStride
inputWidth
iterations
lazy
mt
native
net
nullptr
nwc
operator
output
outputPixelStride
point
pool
pooling
pytorch
qnnp
qnnpack
quantized
random
randomDevice
range
ref
rng
scale
setup
sizeof
src
state
static
status
success
thread
u
uniform
usize
zero

ALL
AND
ASSERT
AT
ATen
BFloat
BYTE
Bool
CHECK
COMPLEX
DISPATCH
Deterministic
Enabling
Even
Float
Func
GPU
GRAIN
Given
Half
INDEX
INTERNAL
Implements
IndexKernel
IndexToOffset
Indexer
Mask
NOTE
Note
Number
OffsetCalculator
Operations
Parallel
Perhaps
QINT
SIZE
SUB
Scalar
ScalarType
See
Self
TODO
TORCH
TYPES
Tensor
TensorIterator
The
This
Unlike
Vectorized
When
`?
`cpu
`index
`iter`
`put
access
accumulate
accumulation
add
again
algorithm
algorithms
and
are
arg
as
aten
atomic
available
balanced
be
benchmark
better
bit
bounds
but
bytes
cache
can
case
cast
char
chosen
cntr
column
const
constant
constexpr
contiguous
copy
could
cpp
cpu
deterministic
deterministicAlgorithms
dim
dimension
dirty
dispatch
does
dst
dummy
duplicate
elem
element
elements
enable
enabled
every
execution
false
fill
flip
follows
func
globalContext
grain
handle
have
here
idx
implements
in
index
indexed
indexer
indexers
indices
innecessarily
input
internal
investigate
iter
iterated
its
j
kBFloat
kBool
kHalf
kernel
kernel`
launch
legacy
less
let
linear
load
location
loop
major
make
mask
masked
modify
more
much
must
n
native
nb
ndim
necessary
needs
new
non
nondeterministic
nonzero
not
ntensor
ntensors
number
numbers
offset
ones
only
op
or
order
original
otherwise
out
overcome
overhead
parallel
parallelization
prefix
ptr
ptrdiff
put
put`
pytorch
quantized
range
register
reinterpret
relative
result
returns
safe
same
samll
scalar
scatter
select
serial
set
size
sizeof
small
so
source
specialization
src
static
strategy
stride
strides
stub
sum
supported
take
tensor
than
that
this
though
thread
threads
through
tried
tweak
tweaked
u
unsigned
unwrap
use
uses
val
value
values
vec
version
was
we
which
whole
with
work
would
yet
zero

ATen
Box
CHECK
CONFIG
DESCRIPTOR
DFTI
Descriptors
DftiCreateDescriptor
DftiDescriptor
DftiDescriptorDeleter
DftiFreeDescriptor
LONG
MKL
VALUE
aten
be
been
can
cpp
desc
error
h
has
init
initialized
invoke
mkl
ndim
not
nullptr
once
only
precision
pytorch
raw
reset
runtime
signal
src
throw
type

ATen
Apply
ArgNames
Args
BENCHMARK
Benchmark
BenchmarkState
C
CharacteristicArguments
Hardsigmoid
MAIN
NO
PYTORCH
QNNPACK
SetBytesProcessed
SetItemsProcessed
SkipWithError
arguments
aten
b
batchSize
begin
bench
bind
bytesPerIteration
c
cast
cc
channels
characteristic
const
cpp
cpu
create
delete
device
distribution
end
endif
failed
fill
flags
generate
hardsigmoid
hardsigmoidOperator
ifndef
initialize
input
itemsPerIteration
iterations
lazy
max
min
mt
n
native
nc
nullptr
operator
output
point
pool
pytorch
qnnp
qnnpack
quantized
random
randomDevice
range
ref
rng
scale
setup
sizeof
src
state
static
status
stride
success
thread
u
uniform
usize
xA
zero

ATen
BS
COL
CONNECTED
Dynamic
FULLY
FullyConnectedSparseOperatorTester
Mode
OP
ROW
SPARSE
TEST
aarch
any
arch
arm
aten
batch
batchSize
cc
cfg
colBlockSize
connected
cpp
cpu
dynamic
export
expr
ft
fully
input
inputChannels
inputStride
integration
iterations
macro
native
none
op
output
outputChannels
outputStride
prepacked
pytorch
qmax
qmin
qnnpack
quantized
rowBlockSize
rules
small
sparse
src
stride
test
testQ
unit
vim
with
zero

ASSERT
AT
ATen
CHECK
DISPATCH
FN
Got
IMPL
INTERNAL
LIBRARY
NAME
PYTORCH
QEngine
QINT
QLeakyRelu
QNNPACK
QRelu
QnnpackOperatorDeleter
QuantizedCPU
Relu
SELECTIVE
Scalar
TODO
TORCH
TYPES
Tensor
TensorIterator
True
Vectorized
WARN
affine
argument
aten
batch
cfg
channels
clamp
const
contig
contiguous
cpp
cpu
create
createStatus
define
device
dispatch
elems
empty
endif
failed
flags
format
ft
globalContext
ifdef
ignored
initQNNPACK
inplace
input
iter
kCPU
kQUInt
kernel
lazy
leaky
m
max
memory
min
native
nc
ndimension
negative
negval
none
not
now
nullptr
op
operator
out
output
point
pthreadpool
ptr
pytorch
qEngine
qnnp
qnnpack
qrelu
quantize
quantized
quint
qx
qy
relu
result
runStatus
scalar
scale
setup
setupStatus
six
size
slope
src
static
status
stride
stub
success
suggest
support
supported
tensor
threadpool
type
u
unary
underlying
uniq
unique
using
usize
val
value
vec
vim
yet
zero

ALWAYS
ASSERT
AT
ATen
BIT
CHECK
DISPATCH
FN
For
Got
IMPL
INTERNAL
LIBRARY
NAME
NUM
Naive
Output
PYTORCH
QEngine
QINT
QNNPACK
QSigmoid
QnnpackOperatorDeleter
QuantizedCPU
SCALAR
SELECTIVE
See
TORCH
TYPE
TYPES
Tensor
The
This
affine
and
aten
batch
but
cfg
channels
com
const
constexpr
contig
contiguous
cpp
cpu
create
createStatus
define
dequantize
device
dispatch
double
elems
empty
endif
execute
failed
flags
format
ft
globalContext
https
ifdef
implemenentation
initQNNPACK
input
kCPU
kQInt
kQUInt
lazy
m
max
memory
min
native
nc
ndimension
none
nullptr
op
operator
optimizations
output
outputs
point
potential
pthreadpool
ptr
pytorch
qEngine
qint
qmax
qmin
qnnp
qnnpack
qsigmoid
quantize
quantized
quint
qx
qy
routine
runStatus
scalar
scale
set
setup
setupStatus
sigmoid
signed
size
src
stackoverflow
static
status
stride
stub
success
suggest
tensor
threadpool
type
types
u
uniq
unique
unsigned
uses
usize
vim
zero

ATen
address
aligned
assume
aten
builtin
c
const
cpp
cpu
do
ft
hi
ii
increment
input
k
lane
last
lo
m
n
native
neon
none
o
output
pytorch
qnnp
qnnpack
quantized
s
src
u
uint
uintptr
usize
val
vim
vld
vmov
void
vreinterpret
vshift
vshl
vst
vw
vx
vxy
vxyzw
vy
vz
vzip
vzw
w
while
xm
y
z
zip

ATen
Builtin
Builtins
C
CASE
DEFINE
FORALL
InternedStrings
MOBILE
Make
NS
Pair
SYMBOLS
String
Symbol
SymbolInfo
Symbols
TODO
The
\
acquire
actually
all
already
also
and
are
assemble
assert
aten
back
be
because
but
bypass
c
can
case
cast
char
compare
const
core
cpp
custom
customString
d
default
define
defined
display
domain
end
endif
error
expected
find
first
fly
found
friendly
from
fromQualString
global
globalStrings
guard
has
have
in
info
interned
know
lifetime
lock
map
maps
must
mutex
name
namespace
namespaces
need
npos
ns
org
ostringstream
pos
prefix
prefixed
printf
push
pytorch
qual
qualString
read
runtime
s
second
size
so
something
src
ss
statements
static
str
strings
stringstream
strlen
style
substr
switch
sym
symbol
symbols
that
their
this
throw
toQualString
toUnqualString
trouble
u
undef
unique
unqual
usable
user
value
we
whose
with

?
ATen
BFloat
By
CHECK
CPU
Can
DNN
Float
For
Ideally
Ideep
Layout
Legacy
MKL
MKLDNNConversions
Mkldnn
NOTE
Option
ScalarType
Strided
TODO
TORCH
Tensor
`ideep
algorithm
already
an
and
are
as
aten
avx
back
backward
be
begin
bf
bfloat
build
but
bw
can
cfg
check
compatibility
consider
cont
contain
contiguous
conv
convert
converted
converts
convolution
cost
cpp
cpu
d
dense
desc
device
dilation
dim
dimension
dimensions
dims
direct
directly
disabled
do
doing
dq
dtensor
empty
enabled
end
expected
expects
extra
false
feature
feed
first
fly
form
format
forward
from
groups
h
has
having
ideep
implicit
in
init
input
instead
itensor
its
jitted
kStrided
kernel
layout
m
may
memory
mkldnn
mod
module
move
native
ndims
needs
new
non
not
note
number
o
only
opt
optTypeMetaToScalarType
optimized
options
or
original
padding
path
perf
pinned
ptr
public
pytorch
reorder
reshape
result
scalar
seeing
serialization
should
special
squash
src
stensor
stride
strided
strides
super
support
tensor
tensor`
this
time
type
use
value
vl
w
wdims
we
weight
weights
will
with
writing
~

AT
ATen
AVX
Add
Additional
Align
Array
Broadcast
Clip
Conversions
Convert
Currently
Default
ERROR
FBGEMM
FE
FloatVecReturnType
However
If
IntVecReturnType
Load
Mul
NOTE
Not
Note
Only
Output
Pack
Pray
Q
QuantizeAvx
RequantizeAvx
Self
SizeType
TONEAREST
The
These
This
VLEN
ValueType
Vectorized
VectorizedQint
VectorizedQuantizedConverter
VectorizedQuint
Vectorizedqi
When
above
acceptable
act
add
advantage
aligned
all
always
an
and
any
are
argument
arithmetic
as
assert
assume
aten
attribute
autovectorize
avoid
avx
away
awful
b
back
bandwidth
base
based
be
because
behavior
bound
building
but
c
can
cannot
carried
cases
cast
castsi
cfg
clamp
clamped
clang
classes
clipped
com
compiler
const
constexpr
constructor
converter
converters
converts
count
cout
cpp
cpu
current
currently
cvtepi
cvtepu
cvtps
decay
decltype
default
defined
defines
dequantize
derive
does
doing
dst
dump
efficient
elem
elementwise
emits
en
endif
endl
enum
epi
epu
even
exactly
expected
extract
fall
fbgemm
feature
file
first
floating
fmadd
following
follows
format
from
full
function
future
github
greater
h
halfway
has
https
ifdef
implement
implementation
implementations
in
inp
installed
intel
inverse
iterations
j
kernels
lane
largest
layout
least
len
let
limits
linkage
load
loadu
loop
low
m
mask
max
maximum
may
memcpy
memory
min
minimum
mm
mod
mode
most
moving
mul
mullo
multiplier
nearbyint
need
needed
neg
new
node
not
off
only
operations
operator
operators
or
os
other
out
over
pack
packed
packs
packus
per
performance
permute
permutevar
point
practice
precision
premul
probably
problem
ps
ptr
pytorch
qint
quantization
quantize
quantized
quint
qvals
reference
reinterpret
relax
relu
representable
requantize
requirement
res
result
results
returned
retval
revisit
rhs
round
rounded
rounding
rounds
rv
same
sat
saturate
scale
scaled
second
set
setzero
shifted
should
shuffle
si
simply
six
size
sizeof
smaller
so
software
special
specified
srai
src
srli
static
store
storel
storeu
sub
subtract
super
supported
take
than
that
them
these
this
tmp
trait
transformed
type
typename
types
u
uint
underlying
unpacked
unpacklo
unwrap
us
use
used
user
using
usize
usually
uw
v
val
vals
value
values
ve
vec
vecs
vectorized
vectorizedqi
vectors
version
void
w
we
widening
will
windows
with
writing
xff
xy
xyzw
y
z
zero
zeros
zp
zw

ATen
Infinity
Integer
aten
cpp
div
division
h
pytorch
r
rounding
rtn
src
y

?
ATen
CHECK
ContextConv
ContextLinear
Conv
D
FreeOrigWeightAndBias
IntrusivePtr
LinearOpContext
LinearOpContextInterface
OpContext
Option
Original
Run
Scalar
Self
SerializationTypeConv
SerializationTypeLinearPrePack
SerializationTypeTransposeConv
TORCH
Tensor
TorchJitCustomClassHolder
TransposeConv
XNNPackConv
XNNPackLinearOpContext
XNNPackTransposeConv
and
aten
base
been
bias
context
conv
convolution
cpp
create
d
dOpContext
dOpContextInterface
dPrePack
dilation
false
free
freed
globalContext
groups
h
have
input
internal
intrusive
kMax
kMin
linear
make
max
min
move
native
new
op
orig
output
padding
pytorch
releaseWeightsWhenPrepacking
reset
src
stride
trait
transposed
tuple
type
u
unpack
weight
xnnpack

API
CU
CUDA
FILE
GENERIC
THC
THCState
THCTensor
THCTensorScatterGather
THCudaLongTensor
TORCH
aten
cpp
define
dim
endif
gather
generic
h
ifndef
index
lazy
pytorch
src
state
static
tensor
void

ATen
Args
ArrayRef
C
F
For
Here
If
IterArgs
NB
These
This
TorchList
Use
Variadic
You
YourStructName
able
adding
all
allows
an
apply
are
arg
args
argument
arguments
as
aten
autogenerated
be
because
call
can
cast
circuit
class
commonly
consider
const
container
convenient
conversion
copies
core
cpp
csrc
deduction
default
defaults
different
do
efficiently
enable
enabled
examples
false
forward
forwarding
function
functions
h
handling
handy
have
here
homogenous
implicit
in
initializer
instead
interested
into
invoke
like
list
ll
make
manually
may
might
more
most
need
not
one
operator
order
otherwise
overloaded
overloads
perfect
possibly
process
provide
pytorch
recursing
see
sensible
short
some
specify
src
static
structures
take
than
that
them
these
this
through
torch
types
uniformly
use
used
using
utils
value
variadic
we
which
won
write
you
your

?
API
ATen
Access
Buffer
Command
FN
IMPL
LIBRARY
Pool
Read
Shape
Stage
TORCH
Tensor
Transfer
VULKAN
Vulkan
Write
access
an
and
api
appropriate
arg
async
aten
barriers
buffer
but
bypasses
cfg
command
const
context
convert
copy
cpp
gpu
implied
inserts
lazy
m
native
necessary
only
ops
options
output
pool
pytorch
queue
shape
src
static
stream
submit
synchronization
tensors
triggers
v
vTensor
view
vulkan

AT
ATen
AffineGridGenerator
C
CHECK
CUDNN
CheckedFrom
CudnnDataType
ENABLED
ERROR
H
Note
See
SpatialTransformerDescriptor
Tensor
TensorArg
W
affine
aten
backward
c
cfg
checkContiguous
checkSize
compiled
contig
contiguous
cpp
cuDNN
cudnn
cudnnSpatialTfGridGeneratorBackward
cudnnSpatialTfGridGeneratorForward
dataType
desc
descriptor
empty
forward
generator
getCudnnDataType
getCudnnHandle
grad
grid
h
inputSize
n
native
not
options
philosophy
preprocessor
ptr
pytorch
resize
sampler
set
setSamplerDescriptor
src
support
theta
type
w
with

ASSERT
AT
ATen
AtLeast
Behavior
C
CHECK
Contiguous
Count
D
DimVector
FlipFn
Input
It
MemoryFormat
Note
Nothing
Output
Preserve
Python
Rotation
Self
SmallVector
TORCH
Tensor
TensorIterator
TensorIteratorConfig
TensorList
TensorTransformations
The
This
To
We
`shifts`
above
abs
add
advances
align
all
an
and
anything
are
arg
as
aten
atleast
avoid
b
be
because
begin
below
bitset
build
but
bytes
case
cast
cat
cbegin
cend
char
check
checks
clone
coalescing
common
const
contiguous
copy
corrects
cpp
cpu
create
cube
d
declare
default
define
defined
device
difference
different
dim
dimension
dimensions
dims
direction
dispatch
div
do
does
dummy
element
empty
error
expected
false
fast
first
flattened
flip
fliplr
flipped
flipud
from
got
h
handle
has
how
in
input
ints
invert
iter
iterate
k
know
lambda
left
like
list
lower
may
mem
modulo
most
move
must
n
narrow
native
ndim
need
negative
no
not
numbers
one
opposite
out
output
overlap
pointer
points
prevent
ptr
pytorch
quantized
range
reinterpret
repeated
required
reshape
restrided
result
roll
rolled
rot
rotation
same
scalar
set
shape
shifts
signed
size
slice
src
start
static
stride
strided
strides
stub
switch
tail
tell
tensor
tensorIterator
that
there
think
this
total
trait
transform
transformation
transpose
type
understand
unsafe
unsqueeze
us
vertex
view
void
vs
want
we
which
with
work
wraps
zero

?
ASSERT
ATen
ActivationND
Batch
Bias
CHECK
ContextLinear
Decouple
FP
Filter
For
INTERNAL
Input
IntArrayRef
IntrusivePtr
Layout
Linear
LinearOpContext
Max
Min
NCHW
NHWC
Operator
Option
Output
Reason
Scalar
Supports
TODO
TORCH
Tensor
The
Weight
XNNPACK
XNNPackLinearOpContext
allocate
and
are
aten
available
back
batch
bias
cbegin
cend
channels
clamp
combination
compatibility
connected
const
context
contig
contiguous
cpp
cpu
create
defined
device
either
empty
error
failed
flags
format
fully
grad
h
handling
improve
individually
input
internal
invalid
ip
kFloat
kMax
kMin
kernel
linear
max
memory
messages
min
mobile
move
names
namespace
native
nc
ndimension
needed
not
nullptr
op
operator
operators
options
or
output
pack
padded
padding
parameters
pixel
pre
provided
pthreadpool
ptr
pytorch
requires
scalar
setup
size
squeeze
src
status
stride
success
suggest
supported
tail
tensor
their
threadpool
type
u
unsqueeze
unsupported
usable
use
using
weight
with
xnn
xnnpack

ATen
LegacyNNDefinitions
MaybeOwned
Note
Option
See
Tensor
aten
bias
borrow
const
conv
cpp
d
depthwise
dilation
forward
from
hacky
kernel
maybe
native
opt
optional
out
output
owned
padding
pytorch
removal
size
src
stride
tensor
thnn
weight
wrapper

ALL
APPLE
ASSERT
AT
ATen
Addition
CHECK
COO
CPU
CSR
CUDA
Can
Check
D
DISPATCH
Do
Expected
FLOATING
FYI
Functions
GRAIN
INDEX
INTERNAL
Index
MACH
MKL
MSC
NotImplementedError
Only
SIZE
Scalar
See
SparseCsrTensor
SparseCsrTensorMath
TODO
TORCH
TYPES
Tensor
This
VER
Windows
accept
accessor
add
addition
addmm
all
alpha
an
and
are
argument
as
aten
axpy
backward
be
beta
broadcasting
but
canCast
cast
certain
check
col
com
commonDtype
contiguous
convert
copy
cpp
cpu
cpublas
crow
csr
cuda
currently
dense
device
different
dim
do
does
due
element
elif
empty
end
endif
equals
equivalent
error
expand
expected
false
format
forward
from
functionally
functions
github
got
h
happens
has
hasMKL
have
high
https
icol
ifdef
implemented
in
index
indices
input
internal
irow
issues
ixj
ixk
j
just
jxk
k
kCPU
kStrided
layout
lazy
linking
make
mat
matrices
matrix
memory
mkl
mm
mul
multiplication
must
native
nnz
non
not
offset
op
operation
options
or
other
out
output
parallel
promoteTypes
ptr
pytorch
r
redispatch
reported
resize
result
resultBuffer
routines
s
same
scalar
size
sparse
square
squares
src
start
static
storage
stride
strides
suggest
support
supported
technically
tends
tensor
tensors
that
too
type
unnecessary
usable
use
utiliy
val
value
values
valuesBuffer
vec
was
what
while
wise
with
worker
yet
zero
zeros

ATen
LUT
LUTNormMicrokernelTester
NORM
SCALAR
U
aten
cc
cpp
cpu
eq
inplace
large
lut
n
native
norm
pytorch
qnnpack
quantized
scalar
small
src
test
u
ukernel
usize

A
ASSERT
ATen
BIT
Binary
Box
Bytes
CHECK
COMPUTE
CPU
CREATE
Cache
CompileGlslToSpv
Compiler
DEBUG
DELETER
DESCRIPTOR
Default
Deleter
Descriptor
DescriptorSetLayout
Factory
For
Furthermore
GPU
GetCompilationStatus
GetErrorMessage
Gpu
HANDLE
Handle
HasCache
HasDeleter
HasDescriptor
HasHandle
HasSignature
HasWorkGroup
INFO
INTERNAL
If
Invalid
LAYOUT
LayoutDescriptor
MODULE
NULL
Null
ONLY
Object
Padding
PartialEq
RUNTIME
SET
SHADER
SHADERC
STAGE
STRUCTURE
Self
SetGenerateDebugInfo
SetNanClamp
SetOptimizationLevel
SetSourceLanguage
SetTargetEnvironment
SetWarningsAsErrors
Shader
ShaderDescriptor
ShaderDescriptorShader
ShaderDescriptorShaderBinary
ShaderDescriptorShaderSource
ShaderDescriptorType
ShaderFactory
ShaderFactoryCompiler
ShaderFactoryHandle
ShaderFactoryHasher
ShaderLayout
ShaderLayoutCache
ShaderLayoutDescriptor
ShaderLayoutFactory
ShaderLayoutFactoryHandle
ShaderLayoutFactoryHasher
ShaderLayoutObject
ShaderModule
ShaderWorkGroup
ShadercCompileOptions
ShadercCompiler
Signature
SmallVector
Source
SpvCompilationResult
TORCH
TYPE
The
This
Type
VK
VULKAN
VkDescriptorSetLayout
VkDescriptorSetLayoutBinding
VkDescriptorSetLayoutCreateInfo
VkDescriptorType
VkDevice
VkShaderModule
VkShaderModuleCreateInfo
Vulkan
WorkGroup
\
accessible
actual
aforementioned
after
also
and
api
appear
are
arguments
as
assert
assign
assignment
aten
back
be
been
binary
binding
bindings
blueprint
bundle
bundled
bytes
cache
caches
calls
cast
cbegin
cend
cfg
combine
comp
compilation
compile
compiler
compute
configurable
const
constructor
consumption
contain
context
cost
cpp
create
creation
declaring
default
define
defined
defines
definition
descriptor
descriptorCount
descriptorType
destruct
device
dispatch
enable
endif
enum
env
eq
equal
error
extra
facilities
factory
false
form
from
fully
function
glsl
gpu
graphics
h
handle
hash
have
host
ifdef
implementation
in
info
input
instantiated
intended
interface
invoke
its
language
layout
layouts
level
locations
memory
minimize
module
monolithic
move
multiple
namely
native
new
not
nullptr
number
object
objects
operator
optimization
options
or
other
out
output
outside
pImmutableSamplers
parameter
parameters
part
per
performance
pimpl
pipeline
pipelines
pointers
portion
program
prototype
ptr
purge
push
pytorch
reason
reconstructions
redundant
regular
required
requires
reside
resources
result
retrieve
reuse
runs
said
sees
set
sets
shader
shaderc
shaders
signature
simple
size
sizeof
small
source
spirv
src
stageFlags
state
static
status
strlen
success
system
terminated
that
them
themselves
through
together
ty
type
typically
u
union
unique
unused
used
usize
usually
uvec
version
vk
vkCreateDescriptorSetLayout
vkCreateShaderModule
vulkan
was
what
which
while
world
would
zero

?
ASSERT
ATen
AlignedAllocator
AvgPoolMicrokernelTester
AvgPoolMpUKernelFunction
AvgPoolUpUKernelFunction
Call
Compute
GE
LE
NEAR
Prepare
PyTorchQ
Verify
acc
assert
aten
avgpool
begin
bias
bind
channel
compute
const
cpp
cpu
default
device
distribution
end
fill
generate
h
indirectX
isnormal
iteration
iterations
j
k
kc
kernel
kh
kr
ks
kw
max
micro
microkernel
min
mpAcc
mr
mt
n
native
optimized
packed
packedKs
packedN
packedn
parameters
params
pixel
point
pytorch
qnnp
qnnpack
qr
quantization
quantizationParams
quantize
quantized
random
randomDevice
ref
reference
results
rng
s
scalar
scalarQuantizationParams
scale
shuffle
size
sizeof
src
stride
test
tester
this
u
uniform
union
usize
void
xA
xScale
xStride
xZeroPoint
y
yAcc
yFP
yMax
yMin
yRef
yScale
yStride
yZeroPoint
zero

A
ARM
ATen
BLOCK
COL
Each
For
K
M
Note
Original
PACKED
Packed
Remainder
SIZE
Since
Size
So
This
Thus
When
accumulated
activations
adjacent
alls
also
and
appropriate
arbitrary
are
as
aten
be
because
block
blocks
c
can
care
const
contain
contiguous
copying
cpp
cpu
does
filled
format
ft
gemm
given
group
has
helps
in
into
intrinsics
just
k
kernel
kx
loading
locality
m
memory
mr
multiple
multiplication
native
none
not
ok
optimized
optimizing
out
packA
packa
packed
parts
placed
pytorch
qnnpack
quantized
random
remaining
rest
result
rows
slow
some
sparse
src
sse
stored
stride
such
super
taken
that
them
this
those
tranpose
transposed
u
ukernel
use
valid
values
vim
we
weights
wil
will
with
writing
written
zero
zeros

?warp
ATen
Allocator
Because
Box
C
CHECK
CLASS
CPU
CUDA
CUDADeviceAllocator
CUDAHooks
CUDAHooksArgs
CUDAHooksInterface
CUDAHooksRegistry
CUDART
Cannot
Class
Consider
Create
Device
DeviceIndex
Does
EVEN
Generator
HELP
How
IF
INCLUDE
ISO
In
Initialize
Is
JIT
MOBILE
MUST
NB
NVRTC
One
Option
Pinned
PyTorch
REGISTER
Registry
See
String
THCState
TODO
TORCH
The
There
This
We
Wl
You
\
`at`
access
actual
actually
after
against
allocated
allocator
allow
also
an
and
another
any
are
argument
arguments
as
aten
avoid
backend
batchnorm
batchnormMinEpsilonCuDNN
be
because
been
before
benign
binary
both
but
buy
c
cache
caches
call
called
can
careful
cases
cfg
change
check
class
class?
clear
clsname
code?
common
compilation
compiled
config
const
constructed
contain
contains
context
convolution
copy
could
cpp
cu
cuDNN
cuFFT
cuda
culprit
current
decide
declare
default
define
defined
definitions
delete
depend
dependencies
dependency
depthwise
destruct
destruction
destructors
detail
detailed
devce
device
dilated
directly
disabled
dispatched
dll
dnn
do
doesn
don
due
dummy
dynamic
dynamically
endif
epsilon
error
ever
example
export
false
fft
filter
first
fix
fixed
flag
forgets
from
function
functionality
fused
generator
getCUDAHooks
getVariableHooks
global
gpu
h
handles
has
hasPrimaryContext
hascuda
hascudart
hasmagma
have
here
hooks
ident
implement
implementation
implies
in
index
initcuda
initialize
interact
interface
into
invocation
its
kernel
kernels
lack
lazy
ldd
leak
leaking
least
let
libATen
libraries
libraries?
library
like
limitations
link
linker
linkers
linking
live
loaded
lock
look
losing
macro
many
max
may
memory
might
min
mio
much
must
need
needed
needing
never
new
no
non
not
nullopt
nullptr
nvrtc
object
objects
occurred
omnibus
once
one
only
or
os
ostensible
out
pen
permanently
pinned
plan
pointer
precipitated
primary
principle
prior
probably
program
provided
ptr
purposely
putting
pytorch
query
re
real
really
reason
reference
register
registry
relax
release
require
requires
restriction
rules
running
s
safe
see
separate
set
shared
should
show
simpler
since
situations
size
size@cuda@at@@YAHXZ
so
solution
some
splits
src
state
static
store
str
stub
supports
suppress
symbols
synchronize
tests
that
their
there
this
though
thread
thus
time
times
too
trait
transitively
try
trying
under
unloaded
unwrap
us
use
used
using
variable
variadic
version
versioncudart
very
virtual
void
vptr
want
was
we
were
which
while
will
windows
with
without
word
you
your

ATen
Auto
CompositeRandomAccessor
CompositeRandomAccessorCPU
KeyAccessor
References
ReferencesHolder
TupleInfoCPU
Types
ValueAccessor
Values
args
aten
cpp
cuda
decltype
h
holder
lazy
native
pytorch
references
rh
src
static
swap
thrust
tie
type
typename

ASSERT
ATen
CHECK
CheckedFrom
INTERNAL
ParamUtils
TORCH
Tensor
TensorArg
and
arg
aten
backward
be
checkSameSize
coalesce
conversion
cpp
device
dim
dimensions
empty
equal
function
grad
h
half
input
less
like
make
maybe
must
name
native
negative
non
not
output
pair
preprocessing
pytorch
softmax
sparse
src
str
supported
than
tuple
with
wrap

?
@note
AND
ASSERT
AT
ATen
Although
C
CACHE
CHECK
CPU
CUDA
CUDAHooks
CUFFT
Calculate
Check
Complex
Convert
Create
CuFFT
CuFFTConfig
CuFFTDataLayout
CuFFTDimVector
CuFFTHandle
CuFFTParams
CuFFTParamsLRUCache
CuFFTPlanCache
CuFFTSizeType
CuFFTTransformType
D
DEFAULT
Default
Determine
Does
Double
Drop
Embedded
Enum
F
FFT
FFTs
Float
For
HCC
HIP
HIPFFT
Half
Hit
INTERNAL
If
In
IntArrayRef
It
Kv
LT
LinkedList
MAX
Map
Miss
NOT
NOTE
NULL
NUM
Not
Only
Otherwise
Out
PLAN
PLATFORM
Padding
ParamsEqual
ParamsHash
Please
R
RawMutex
Real
Return
Returns
SIZE
SM
ScalarType
See
Self
Since
Size
Sizeype
SmallVector
SpectralOps
Strides
Struct
THC
TORCH
The
These
This
Users
VERSION
We
Z
Zero
accidentally
actual
additional
advantage
all
allocation
allocator
allowed
already
always
and
arbitrary
are
around
as
assert
assign
assignment
assume
assumes
assuming
aten
autoAllocate
avoid
back
base
batch
be
because
before
begin
big
bits
bools
bug
build
but
cache
cached
call
can
cannot
capability
case
cast
cbegin
cend
cfg
changes
check
chosen
class
clear
clobering
clone
cloned
complex
compute
computing
config
configure
const
constraints
construct
constructor
containing
contains
contiguous
contract
copy
counterparts
cpp
created
cu
cuFFT
cuda
cudaDataType
cufft
cufftCreate
cufftDestroy
cufftSetAutoAllocation
cufftXtMakePlanMany
cur
default
delete
derive
detail
dev
device
dim
dimension
dimensions
disable
dist
do
does
doesn
don
double
drop
due
dummy
easily
element
embed
embedded
embedding
emplace
end
endif
enought
enum
erase
even
excluding
exec
execute
executing
failing
fails
false
fft
fftDestroy
find
first
flag
forward
freeing
from
front
full
function
functions
getCurrentDeviceProperties
given
got
h
half
handle
handles
happens
has
hashes
hashing
here
hipFFT
hipfft
hipfftMakePlanMany
hipfftType
hit
hooked
hooks
how
idist
ifdef
ifndef
ignores
in
include
including
index
inembed
information
input
insert
integers
into
issues
istride
iter
iterator
itype
just
kDimVectorStaticSize
kDouble
kFloat
keep
key
kkv
kv
larger
last
layout
lazy
least
len
less
let
limit
limited
list
loaded
lookup
lru
maintain
major
make
map
mapping
max
memset
minor
mod
move
multiply
must
mutex
native
ndim
need
needed
needs
negative
negativity
never
new
non
not
nullptr
number
odist
once
one
onembed
onesided
only
op
or
ostride
other
otype
our
out
output
owns
pair
parameters
params
part
perf
piecewise
plan
planning
pop
positive
pow
powers
precision
prop
ptr
purpose
put
puts
pytorch
range
rank
raw
real
reference
remove
repr
representation
represented
representing
represents
requires
resize
resulting
returned
returns
rocFFT
s
safe
second
see
semantics
separated
set
sets
setting
shape
should
shouldn
sided
signal
signals
simple
size
sizeof
slice
so
splice
src
static
stides
still
stride
strides
structures
such
super
support
supported
supports
switch
take
tampered
tell
tensor
tests
th
than
that
then
these
this
thread
throughout
track
transform
transforms
trivial
try
tuple
ty
type
typename
types
unique
unit
units
unordered
us
usage
use
used
using
usize
value
various
version
via
way
we
well
whether
whose
will
with
work
workspace
wrapper
ws
zeroed

?
API
ASSERT
ATen
C
CHECK
CPU
Conv
DIV
DParams
Device
DeviceType
DispatchKey
DispatchKeySet
Expected
FN
Float
HW
IMPL
INTERNAL
KH
KW
LIBRARY
Layout
Mean
MemoryFormat
OC
OH
OW
Option
Scalar
ScalarType
Sizes
Strided
TORCH
Tensor
TensorList
TensorOptions
UP
VULKAN
Vulkan
VulkanAten
VulkanImpl
VulkanImplInterface
VulkanImplRegistrar
VulkanOpaqueTensorImpl
VulkanTensor
VulkanTensorImpl
adaptive
add
addmm
alpha
and
argument
aten
available
b
back
base
batch
be
begin
beta
bias
cast
cat
ceil
cfg
channels
check
clamp
compute
concatenated
const
contiguous
continue
conv
convolution
convolutions
copy
count
cpp
cpu
d
dH
dW
defined
depthwise
device
dilation
dilationH
dilationW
dim
dimension
dimensional
dimensions
dims
divisor
downcast
either
empty
end
equal
erase
except
expected
expects
format
from
got
groups
h
handle
hardtanh
has
hasBias
have
height
host
iC
iH
iN
iW
ic
ih
implemented
in
include
incompatible
index
infer
infinity
input
inputSize
inputSizes
insert
ints
iw
kCPU
kFloat
kH
kVulkan
kW
keepdim
kernel
layout
lazy
limits
m
make
mat
matching
max
mean
memory
min
mm
mode
move
mul
must
n
native
nearest
new
nogroup
norm
normalize
not
nullopt
oH
oW
oh
omitted
only
opaque
optional
options
or
other
output
outputSize
outputSizes
override
overrideable
ow
pad
padH
padW
padding
params
pin
pool
pooling
prepack
prepacked
ptr
push
pytorch
relu
reshape
result
s
safe
scalar
scale
scales
select
set
shape
single
size
slice
sliced
src
start
static
step
stride
strided
strides
suggest
supported
tensor
tensors
transpose
transposed
tuple
tv
type
unordered
unsafe
unsafeGetTensorImpl
unsqueeze
upsample
vTensors
value
vbias
vec
view
vinput
voutput
vt
vtensor
vulkan
vweight
w
weight
weightSizes
weights
width
with
wsizes
xt
y
yt

AT
ATen
DISPATCH
FN
IMPL
LIBRARY
NAME
QINT
QuantizedCPU
SELECTIVE
Scalar
TORCH
TYPES
Tensor
affine
aten
cpp
cpu
define
device
dispatch
empty
functions
implementation
kernel
lazy
m
native
options
point
pytorch
qthreshold
quantized
qx
qy
scalar
scale
src
static
stub
threshold
type
underlying
value
yaml
zero

ATen
PyTorchQnnpConvQuantizationParams
add
adds
aten
c
channel
channels
const
cpp
cpu
cvtepi
cvtps
cvtsi
do
dwconv
epi
epu
even
extract
ft
hi
increment
input
kernel
lo
load
loadl
loadu
m
max
min
mm
mul
mulhi
mullo
native
none
odd
output
packs
packus
params
per
point
points
predecrement
ps
pytorch
qnnpack
quantization
quantized
requantization
scales
setzero
shift
si
src
srl
srli
sse
storel
stride
sub
u
uintptr
ukernel
unpackhi
unpacklo
up
usize
va
vacc
vi
vim
vk
vkernel
vmultiplier
void
vout
voutput
vprod
vxi
vxk
vzero
w
weights
while
width
zero

@note
@return
A
AFTER
ALWAYS
ANY
ANYTHING
API
ASSERT
ATen
Accessing
Add
After
AnnotatedKernel
AnnotatedKernelIterator
Args
Argument
Array
Assertions
Autograd
Box
By
C
CHECK
Calling
Calls
CaptureKernelCall
Check
Copy
Could
CppSignature
DEBUG
DISABLE
DISPATCHER
DSOs
Deregistered
DispatchKey
DispatchKeySet
Dispatcher
DispatcherOperatorDefIterator
Duplicate
Each
F
FRAGMENT
FUNCTION
For
FuncType
FunctionSchema
GradMode
Handle
IMPL
INLINE
INTERNAL
IValue
If
Immediately
Implement
Implementation
Implemented
Invoke
Invoking
Irritatingly
It
KernelFunction
Keys
LIBRARY
LeftRight
Like
LinkedList
Listeners
Looks
MOBILE
Making
Map
Most
Mutex
NB
NON
NOT
NOTE
Note
NumDispatchKeys
OK
ONLY
OP
OnOperatorDeregistered
OnOperatorRegistered
Once
Only
OpRegistrationListener
OperatorDef
OperatorEntry
OperatorHandle
OperatorName
Option
Original
PER
PROFILING
PYTORCH
Performing
Plumbing
Postcondition
Previous
RAII
RVO
RecordFunction
RecordScope
Register
RegistrationHandleRAII
RegistrationListenerList
Releases
Return
ReturnType
Returns
SCHEMA
See
Self
Setting
Since
SkaFlatHashMap
So
Stack
Storing
String
TODO
TORCH
Tensor
Test
That
The
These
This
Through
Top
TorchJitStack
Tried
TypedOperatorHandle
UNLESS
UNLIKELY
Use
Using
Variant
WITH
We
When
Wraps
`realSingleton`
able
abort
about
above
abstracts
access
acquire
acquired
across
actual
actually
add
addListener
additional
adjust
adjusted
after
afterward
again
alive
all
allOpNames
allows
almost
already
also
always
an
and
annotate
any
apply
are
arg
args
argument
arguments
as
assert
assertSignatureIsCorrect
associate
associated
aten
avoid
back
backend
backendFallbackKernels
be
because
before
begin
being
better
bit
block
boolean
both
box
boxArgs
boxed
bug
but
bytes
cache
calculatulation
call
callBoxed
callOnOperatorDeregistered
callOnOperatorRegistered
callWithDispatchKeySlowPath
callbacks
called
caller
calling
calls
can
cannot
capture
captureKernelCall
captured
case
cast
certain
certainly
char
check
checkInvariants
checking
checks
class
cleanup
combined
come
compatibility
compiler
completely
computed
concrete
concurrent
consider
const
constructor
convention
convert
copies
copy
core
coresponding
corresponding
could
count
counts
cpp
created
critical
currentDispatchKeySet
currently
cxa
dangling
dead
dealing
debug
declaration
def
default
define
defined
definitions
delete
deregister
deregisterDef
deregisterFallback
deregisterImpl
deregisterKernel
deregisterLibrary
deregisterName
deregisterSchema
deregistered
destruct
destructed
destructors
detail
did
different
differently
direct
directly
dispatch
dispatchKey
dispatchKeyExtractor
dispatchKeySet
dispatcher
dispatching
disposing
distributed
do
does
doesn
don
done
due
dump
dumpComputedTable
dumpState
duplicated
during
dynamic
easily
effectively
element
elision
emplace
enabled
end
endif
enforce
entry
erase
error
even
events
ever
every
everything
exactly
exception
exceptions
executed
executing
execution
existing
exists
expect
explicit
explicitly
fact
fallback
fallbacks
false
fast
final
find
findOp
findOrRegisterName
findSchema
first
flat
forget
forward
forwarding
found
frequency
friend
from
frontend
function
functions
gcc
generate
generated
generating
getDispatchKeySetBoxed
getDispatchKeySetUnboxed
getOutputs
gets
given
global
guarantees
guard
h
had
handful
handle
happen
has
hasSchema
hash
have
header
help
high
highest
highestPriorityTypeId
hold
holds
identical
ifndef
immediately
implement
implementation
implementations
impls
in
include
incorrectly
increase
increment
incur
indicate
inferred
information
initialization
inputted
instance
instances
instruction
instructions
intact
intended
interface
into
invariant
invariants
invocations
isActive
isAliasDispatchKey
isIncludedInAlias
isObserved
isValid
iter
iterator
iterators
just
keep
keeping
kernel
kernels
key
keys
kinds
ks
last
latest
lazy
leading
least
left
less
level
libraries
library
libstdc
lifetime
list
listener
listeners
local
lock
look
lookup
lot
lvalue
make
manages
map
mask
may
means
member
members
method
missing
mobile
move
moved
multiple
must
mutex
name
names
namespace
necessary
need
needn
needsInputs
needsOutputs
never
new
next
no
node
noexcept
non
none
not
note
notified
ns
nullopt
nullptr
number
object
observers
occur
occurs
offset
old
onOperatorDeregistered
onOperatorRegistered
once
one
only
op
operations
operator
operator?
operatorDef
operatorIterator
operatorLookupTable
operators
ops
opsWithDanglingImpls
optimization
optional
or
order
other
otherwise
out
output
outputs
outstanding
over
overhead
overload
parameters
pass
passed
path
peek
per
perf
performed
permitted
please
plumbs
point
pointer
pointers
positive
possible
pre
prematurely
prepareForDeregistration
present
presuambly
prev
previous
previously
prior
priority
program
public
purposes
push
put
pytorch
raise
raises
raising
range
rates
rather
re
read
real
realSingleton
really
reason
reasons
record
redispatch
redispatchBoxed
reduce
refer
reference
references
reflects
register
registerDef
registerImpl
registerKernel
registerSchema
registered
registering
registers
registration
registrations
release
relying
remove
removeListener
removed
request
responsible
result
results
retrieve
returned
returning
returns
runRecordFunction
running
runs
s
same
sampled
sampling
saves
say
saying
schema
schemas
second
semantic
semantics
seq
sequence
sequenceNumberForRunningRecordFunction
set
setOutputs
sets
should
shouldRunRecordFunction
shouldn
signature
since
single
singleton
site
ska
skip
smuggle
so
sound
specify
src
stack
state
static
steady
steals
still
store
stores
straightforward
stubs
style
such
suggests
table
tables
take
takes
templated
temporarily
termination
testing
than
that
them
then
there
they
this
though
through
throw
thrown
time
times
too
track
trait
treats
trigger
try
trying
turns
typed
typename
u
unboxed
undefined
unique
unloading
unobxed
unregistered
until
unused
up
update
updateFallback
use
used
useful
user
users
using
usize
valid
value
values
version
via
void
want
warning
was
way
we
were
whenever
whether
which
while
why
will
with
without
won
workaround
working
world
would
wouldn
write
writes
yes
you
your

?
@note
ASSERT
AT
ATen
Also
CHECK
CPU
CUDA
Can
Cannot
Check
Complex
Contiguous
Convert
Create
CuFFTPlanCache
D
Device
DimVector
Dimensions
Dispatch
Do
ERROR
FFT
FIXME
False
FftNormMode
Find
Fixes
Fixup
For
Fourier
Got
Has
Hermitian
IFFT
INTERNAL
If
In
IntArrayRef
Integers
Invalid
Inverse
Layout
Matching
MaybeOwned
MemoryFormat
NOTE
No
None
Not
Note
NumPy
ONCE
Only
Option
Otherwise
Pre
Promote
PyTorch
REPR
Raises
Real
Reorder
SS
ScalarType
See
ShapeAndDims
Short
Slightly
Small
SpectralOps
Stream
StringView
TODO
TORCH
Take
Tensor
TensorIterator
TensorIteratorConfig
TensorOptions
These
This
Transform
Translate
True
Unhandled
Unsupported
Use
WARN
We
When
Wraps
\
\n
`by
`dim`
`none`
`s`
`win
abs
according
actual
add
adjacent
after
all
alldims
allow
amount
an
analogous
analysis
and
applies
apply
arange
are
args
arguments
as
aten
available
away
axis
b
back
backward
batch
be
because
begin
behavior
borrow
build
but
byte
c
cache
call
can
cannot
canonicalize
case
cast
center
centered
channel
char
checks
clear
clone
coalesce
coalesced
col
compatible
complex
complexOpt
conj
conjugate
const
constant
continue
copies
copy
corresponding
cpp
cuFFTClearPlanCache
cuFFTGetPlanCacheMaxSize
cuFFTGetPlanCacheSize
cuFFTSetPlanCacheMaxSize
cuda
cufft
cur
currently
d
declare
default
defaulting
defaults
define
defined
deprecated
desc
details
device
dilation
dim
dimension
dimensional
dimensions
dims
direction
directly
dispatch
div
do
does
doesn
domain
don
dtypes
duplicate
either
element
elements
empty
end
entire
enum
envelop
equal
erase
error
example
expected
expects
explicit
false
faster
fft
fftfreq
fftn
fftshift
fill
find
floating
following
format
forward
frame
frames
frequency
from
front
function
functional
functions
further
future
getCUDAHooks
given
got
h
hacky
half
handling
has
have
hfft
hooks
hop
idx
ifft
ifftn
ifftshift
ihfft
im
imaginary
implemented
in
incompatible
index
init
input
integral
into
inverse
iota
irfft
irfftn
isComplexType
isFloatingType
istft
item
iter
kComplexDouble
kComplexFloat
kDouble
kFloat
kernel
last
layout
least
left
len
length
lengthOpt
length`
let
librosa
locality
loops
lowest
match
max
maximize
may
maybe
meanings
memory
methods
might
min
mirror
mirrored
mirroring
mode
modeled
modes
more
move
mul
must
n
n`
name
narrow
native
nd
ndim
necessary
need
negative
new
nn
no
non
none
norm
normalization
normalized
not
nullopt
number
numpy
offset
old
one
ones
onesided
onesidedOpt
only
onsided
opt
optional
options
or
order
ortho
ostringstream
out
output
outputs
overlap
owned
pad
padding
parameter
parts
perform
permutation
permute
permuted
physical
pin
pinned
plan
point
points
pow
precision
process
promote
promoted
ptr
push
python
pytorch
r
raise
rd
real
really
recover
release
removal
remove
repeat
require
requires
requiring
reserve
resize
ret
returns
rfft
rfftfreq
rfftn
right
roll
root
s
same
scalar
separately
set
shape
shift
side
signal
signals
since
size
slice
slicing
soon
sort
specific
specified
squeeze
src
ss
start
starting
static
stft
still
storage
str
stride
strided
strides
stub
style
such
support
switch
symmetry
temp
temporary
tensor
tensors
than
that
them
then
they
those
time
tmp
toDouble
toString
torch
transform
transformed
transition
translates
transpose
treated
trim
ty
type
typeMetaToScalarType
types
undef
unique
unsqueeze
unwrap
unwrapped
use
uses
valid
value
valued
values
vec
via
view
we
which
will
win
window
windows
with
wrap
wrapper
wraps
write
writing
y
you
zero
zeros

?
ATen
Convolution
For
GEMM
NULL
PRIu
PYTORCH
Probably
PyTorchQnnpOperator
PyTorchQnnpStatus
QNNP
QNNPACK
QUANTIZATION
RUNTIME
TODO
The
This
ThreadPool
UNREACHABLE
XZP
allocate
also
and
any
anywhere
aten
b
batch
be
because
before
bias
bottom
break
buffer
bytes
c
calloc
case
change
channel
channels
char
compute
connected
const
conv
convolution
covers
cpp
cpu
cr
create
d
default
delete
deprecate
dilation
dimension
dimensions
directly
doesn
dont
dw
dwconv
effective
endif
enum
equal
error
failed
false
finite
flags
format
fully
gemm
goto
greater
group
groups
height
in
indirection
inefficiency
info
init
initialized
input
invalid
isnormal
k
kc
kernel
kernels
kr
kthreshold
later
left
log
malloc
maps
max
memory
memset
min
mr
must
n
native
needed
needs
nhwc
non
none
not
now
nr
offset
operator
or
out
output
pack
packed
packing
padded
padding
parameter
params
per
performed
pixel
point
pointer
points
positive
properly
pytorch
qnnp
qnnpack
quantization
quantized
quint
realloc
requantization
ressurrect
right
round
row
scale
scales
setup
shared
should
since
size
sizeof
sr
src
status
step
stride
structure
subsampling
success
sum
supporting
switch
swizzle
than
this
threadpool
tile
tiled
top
type
u
uintptr
ukernel
uninitialized
unsupported
up
use
usize
void
w
we
weight
weights
width
with
won
xzp
zero
zu
zux

ATen
address
aten
c
const
cpp
cpu
do
ft
increment
input
n
native
neon
none
o
output
pytorch
qnnp
qnnpack
quantized
src
u
uint
uintptr
usize
val
vim
vld
void
vst
vw
vx
vxyzw
vy
vz
w
while
y
z
zip

ATen
CHECK
CONTIGUOUS
CPU
CUDA
Cross
FORMAT
LEGACY
MEMORY
Option
TORCH
Tensor
and
as
aten
break
const
cpp
cross
cuda
d
declare
define
device
devices
dim
dimension
dimensions
dispatch
does
empty
got
h
has
have
in
inconsistent
input
kCPU
kCUDA
lazy
like
match
maybe
must
native
no
not
only
other
out
pytorch
res
resize
same
size
src
static
stub
supports
tensors
type
using
value
void
wrap

?
ALGO
AT
ATen
BLANK
Backend
CHECK
CONTIGUOUS
CPU
CTC
CTCLossDescriptor
CUDA
CUDNN
CheckedFrom
CuDNN
DATA
DETERMINISTIC
ENABLED
ERROR
FLOAT
FORMAT
LEGACY
LOSS
LT
LossCTC
MEMORY
NAN
NON
NORMALIZATION
Note
PROPAGATE
PyTorch
SOFTMAX
See
TORCH
Tensor
TensorArg
TensorDescriptor
VERSION
accesses
algo
all
and
any
assert
aten
b
backward
batch
be
begin
blank
but
c
cfg
changed
check
checkBackend
checkContiguous
checkDim
checkScalarType
checked
compiled
conditions
costs
cpp
ctc
ctx
cuDNN
cudnn
cudnnCTCLoss
cudnnCTCLossAlgo
cudnnGetCTCLossWorkspaceSize
desc
deterministic
device
didn
dim
dispatch
documented
don
empty
end
false
getCudnnHandle
globalContext
grad
gradient
handle
have
illegal
in
infinity
input
kByte
kCUDA
kFloat
kInt
know
label
length
lengths
like
log
logprob
loss
make
match
max
memory
must
native
needs
not
older
only
options
other
philosophy
preprocessor
probs
ptr
pytorch
same
scalar
see
semantics
setEx
should
size
so
src
support
targets
that
they
this
tuple
type
use
used
userEnabledCuDNN
usize
void
we
with
workspace
yet
zero

A
AArch
ARM
ARMv
ATen
Assumes
Both
Conversion
Convert
Doing
FP
Floating
However
In
Large
Leverage
NEON
Product
PyTorchQnnpConvQuantizationParams
Rounding
Signed
So
The
This
VLD
We
aarch
absolute
add
adding
addition
after
aligned
always
an
and
any
as
assume
aten
available
be
because
before
better
both
builtin
c
can
cause
channel
clamp
clamped
clamping
consider
const
conv
conversion
cpp
cpu
cycle
do
don
dup
elements
endif
even
exactly
floating
ft
generally
gets
has
have
having
high
higher
ifdef
in
index
input
instead
instruction
integer
into
involve
just
k
kc
kernel
ks
lane
large
last
latency
less
lieu
limited
low
magic
make
max
min
mr
multiple
multiply
must
n
native
nearest
necessary
need
neon
none
not
nr
number
offers
only
operation
operations
order
output
overflow
padded
params
per
performed
point
points
predecrement
probably
pytorch
qmax
qmin
qnnpack
quantization
quantized
range
representation
represented
requantization
restrict
result
results
round
rounded
rounding
roundings
s
saturated
saturates
scale
scaled
scales
shift
sizeof
smaller
so
specifically
src
stage
statistically
stride
sub
substracing
than
that
then
they
this
throughput
thus
ties
towards
trick
u
uint
uintptr
ukernel
unbiased
use
using
usize
va
vacc
vaddq
value
values
vb
vcombine
vcvtnq
vcvtq
vdupq
vextq
vfmagic
vfmax
vfmin
vget
vim
vimagic
vld
vmaxq
vminq
vmlal
vmov
vmulq
void
vout
voutput
vqaddq
vqmovn
vqmovun
vreinterpret
vreinterpretq
vs
vshl
vst
vsubl
vsubq
vxa
vxb
w
we
well
which
while
will
with
works
would
zero

ATen
NULL
PRIu
PyTorchQnnpOperator
PyTorchQnnpStatus
QNNPACK
Scale
Sigmoid
Size
allocate
and
aten
batch
be
because
below
bytes
c
calloc
channels
const
cpp
cpu
create
delete
enum
error
expf
failed
finite
flags
format
ft
goto
initialized
input
invalid
isnormal
log
lookup
lrintf
lut
malloc
max
memory
min
must
native
nc
non
none
not
number
only
op
operator
out
output
parameter
params
pixel
point
positive
properly
pytorch
qnnp
qnnpack
quantized
quint
range
scale
scaled
setup
sigmoid
size
sizeof
src
status
stride
structure
success
supported
table
type
u
ukernel
uninitialized
unsupported
vim
with
zero
zu

?
A
AT
ATen
Allocator
Array
B
BFloat
Byte
CHECK
CUDA
CUDAType
Char
ClassNLLCriterion
CudaBFloat
CudaClassNLLCriterion
CudaDoubleClassNLLCriterion
CudaDoubleGatedLinear
CudaDoubleLogSigmoid
CudaDoubleMultiLabelMarginCriterion
CudaDoubleMultiMarginCriterion
CudaDoubleSpatialClassNLLCriterion
CudaDoubleSpatialConvolutionMM
CudaDoubleSpatialDepthwiseConvolution
CudaGatedLinear
CudaHalfClassNLLCriterion
CudaHalfGatedLinear
CudaHalfLogSigmoid
CudaHalfMultiLabelMarginCriterion
CudaHalfMultiMarginCriterion
CudaHalfSpatialClassNLLCriterion
CudaHalfSpatialConvolutionMM
CudaHalfSpatialDepthwiseConvolution
CudaLogSigmoid
CudaMultiLabelMarginCriterion
CudaMultiMarginCriterion
CudaSpatialClassNLLCriterion
CudaSpatialConvolutionMM
CudaSpatialDepthwiseConvolution
DeviceGuard
DeviceType
DispatchKey
Double
ERROR
Float
Half
In
Int
LegacyTHFunctionsCUDA
Long
MaybeOwned
MultiLabelMarginCriterion
NULL
Note
ONCE
Option
OptionalDeviceGuard
PyTorch
QR
Scalar
ScalarType
See
Short
SpatialClassNLLCriterion
SpatialConvolutionMM
SpatialDepthwiseConvolution
Storage
THCudaByteTensor
THCudaCharTensor
THCudaDoubleTensor
THCudaHalfTensor
THCudaIntTensor
THCudaLongTensor
THCudaShortTensor
THCudaTensor
THNN
TORCH
Tensor
TensorImpl
TensorList
TensorOptions
Tensors
The
To
UndefinedTensorImpl
WARN
\n
about
accGradParameters
allocator
although
and
arguments
as
aten
backward
be
bias
borrow
break
buffer
byte
c
case
check
checked
columns
consider
const
conv
copy
copyIgnoringOverlaps
cpp
cross
crossKernel
cuda
d
decomposition
default
dense
deprecated
depthwise
device
dilation
dim
dispatch
does
empty
expected
false
favor
field
forward
from
future
gels
getCUDADeviceAllocator
getTHCState
globalContext
glu
grad
guard
hacky
has
ignore
ignoring
in
in\n
index
infer
information
input
intlist
intrusive
kStrided
kernel
last
layout
linalg
list
log
loss
lstsq
m
make
margin
mask
maybe
multi
multilabel
n
named
nll
non
not
nullptr
omitted
ones
opt
optional
options
other
out
output
overlaps
owned
padding
problem
ptr
pytorch
qr
reclaim
reduction
release
removal
removed
replaced
res
residuals
result
returned
returns
reversed
s
scalar
scalarTypeToTypeMeta
should
sigmoid
singleton
size
solution
src
stored
stride
supported
switch
tensor
th
thnn
tl
toDouble
torch
total
tuple
type
unpacking
unwrap
updateGradInput
updateOutput
use
using
value
weight
whenever
will
with
with\n
wrapper

API
CU
CUDA
FILE
GENERIC
Scalar
THC
THCState
THCTensor
THCTensorMath
TORCH
aten
cpp
define
endif
fill
generic
h
ifndef
lazy
ptrdiff
pytorch
src
state
static
value
void
zero

A
ADD
ANY
ATen
Args
Autograd
But
DispatchKey
DispatchKeySet
Dispatcher
E
EXPECT
Exception
Expected
FAILURE
FALSE
Functor
GetCPUAllocator
HasSubstr
IValue
Inputs
Into
IntoIterator
Make
NB
OperatorHandle
Result
Size
StorageImpl
THAT
THROW
TODO
Tensor
TensorImpl
This
TypeMeta
We
\
actual
add
added
all
allocate
allocator
are
args
aten
autograd
backend
boxing
but
byte
bytes
c
caffe
call
callBoxed
callOp
callWithDispatchKey
case
catch
const
constructed
constructor
containing
contains
core
cpp
currently
default
detail
didn
dispatch
dispatchKey
doesnt
dummy
dummyTensor
equals
exception
expect
expectMessageContains
expected
extract
false
find
findSchema
fine
forward
functor
grad
h
has
have
helpers
here
ideal
in
intrusive
itemsize
kernel
key
keys
ks
legacyExtractDispatchKey
let
list
make
makeStack
message
name
nelements
not
only
op
operator
or
precomputed
pytorch
really
redispatch
remove
requires
resizable
s
set
sets
simulate
singleton
singletons
size
so
sound
src
stack
storage
tensor
test
testing
this
throw
throws
try
type
typed
u
unboxed
unsafeGetTensorImpl
unwrap
use
usize
value
we
what
with

ATen
Binary
BinaryOps
CHECK
Expected
MKLDNN
Ops
Scalar
TORCH
Tensor
add
algorithm
alpha
as
aten
be
binary
broadcasting
c
cfg
compute
const
cpp
currently
device
dim
dimension
dnnl
does
eltwise
empty
emptyBinaryOp
false
feature
forward
from
grad
ideep
in
infer
inference
input
item
itensor
kind
layout
linear
memory
mkldnn
move
mul
native
ndimension
new
not
op
opt
optTypeMetaToScalarType
options
other
out
output
pinned
promoteTypes
prop
pytorch
requires
result
same
scales
should
size
src
sum
support
tensor
training
typeMetaToScalarType
with
y
z
zero

?
AT
ATen
CHECK
DNN
Device
ENABLED
Layout
MKL
MKLDNN
MemoryFormat
NOTE
Option
ScalarType
TODO
TORCH
Tensor
TensorFactories
argument
aten
avoid
begin
build
but
cfg
conversion
cpp
device
dims
disabled
dst
empty
end
extra
false
format
from
has
ideep
in
incompatible
itensor
layout
memory
mkldnn
move
native
needs
new
not
optional
pin
pytorch
src
support
tensor
type
value
with

ATen
INTERNAL
PYTORCH
QNNP
aten
const
cpp
cpu
declare
export
function
h
ident
macro
n
name
native
neon
pytorch
qnnpack
quantized
rmax
rules
src
sse
u
ukernel
usize

API
CU
CUDA
ClassNLLCriterion
FILE
GENERIC
GatedLinear
LogSigmoid
MultiLabelMarginCriterion
MultiMarginCriterion
OPTIONAL
SpatialClassNLLCriterion
SpatialConvolutionMM
SpatialDepthwiseConvolution
THC
THCIndexTensor
THCState
THCTensor
THCUNN
THNN
TORCH
accGradParameters
accreal
aten
bias
buffer
columns
cpp
dH
dW
define
dilationH
dilationW
dim
endif
ft
generic
gradBias
gradInput
gradOutput
gradWeight
h
ifndef
ignore
index
input
kH
kW
lazy
margin
none
ones
output
padH
padW
pytorch
reduction
scale
src
state
static
total
updateGradInput
updateOutput
vim
void
weight
weights

ARCH
ATen
CUDA
CUDACC
Complex
Float
HIP
HIPCC
Half
Integer
NumericUtils
This
and
approximation
assert
aten
bandwidth
be
bf
cast
complex
convert
cpp
defined
do
double
endif
exp
expf
false
fast
floating
function
h
half
imag
integer
integral
isn
isnan
less
log
logf
must
or
peak
performant
point
precise
pytorch
real
same
src
static
tan
tanf
test
then
this
type
types
use
used
uselessly
val
value
will
with

ATen
DataType
MAJOR
MINOR
MIOPEN
PATCH
Tensor
Types
VERSION
aten
cpp
error
getMiopenDataType
h
kBFloat
kFloat
kHalf
miopen
miopenBFloat
miopenFloat
miopenHalf
msg
not
pytorch
runtime
scalar
src
supported
tensor
throw
toString
type
version

ATen
Do
END
Ideally
Keep
Mean
NB
None
Possibly
Reduction
Sum
These
aten
be
behavior
but
class
constants
control
core
cpp
doesn
enum
functions
h
in
jit
loss
losses
mean
nn
not
py
pytorch
reduce
reduction
scoped
src
support
sync
that
this
torch
weighted
with
would

ATen
C
DeviceType
GUARD
IMPL
Meta
MetaGuardImpl
NoOpDeviceGuardImpl
REGISTER
aten
c
cpp
detail
lazy
pytorch
src
static

?
ASSERT
AT
ATen
CHECK
DISPATCH
FLOATING
GRAIN
INTERNAL
Option
SIZE
Scalar
ScaleType
TORCH
TYPES
Tensor
UpSampleMoreKernel
`grad
align
and
area
as
aten
backward
begin
bilinear
but
c
channels
compute
const
contiguous
copy
corners
cpp
cpu
d
depth
dimension
dispatch
end
expected
got
grad
h
height
id
ih
index
indexr
input
input`
internal
iw
kernel
lambda
linear
loop
native
nbatch
ndim
od
oh
one
output
ow
parallel
pixel
ptr
pytorch
register
scalar
scale
scales
size
slice
source
src
treat
trilinear
type
upsample
value
vec
w
width

?
?n
A
ALL
ALLOCATOR
AND
AT
ATen
Acquire
Acquires
AutoDispatchBelowAutograd
B
BFloat
BTW
BinaryFn
Blackman
Bool
But
CAST
CHECK
COMPLEX
CPU
CPUGeneratorImpl
CUDA
Cannot
Case
ComplexDouble
ComplexFloat
Conjugate
Copy
DEFINE
DISPATCH
Device
Different
DimnameList
DispatchKey
Double
Ensure
Example
Expected
Explicit
Float
GRAIN
Generator
Half
Hence
IR
If
In
Input
Instead
It
Layout
Long
MAPPED
MemoryFormat
NOTE
NYI
NoTracerDispatchMode
None
Note
OP
OPTIONAL
Option
Performs
Preferably
Preserve
RAM
Rectangle
Refer
SHARED
SIZE
Scalar
ScalarType
See
Size
Some
Specified
StorageImpl
Strided
StringView
TH
THMapAllocator
TODO
TORCH
TYPES
Temporary
Tensor
TensorFactories
TensorImpl
TensorIterator
TensorIteratorConfig
TensorOptions
The
These
This
To
Trapezoid
Triangle
Type
Unlikely
Unsupported
We
When
Window
`m`
`n`
abs
accepted
access
accidental
act
add
adjusted
affine
all
allocator
alpha
an
and
angle
any
approaches
arange
are
args
argument
arithmetic
as
assumes
aten
avoid
axis
b
backend
bartlett
base
be
because
begin
behavior
below
beta
binary
blackman
blocking
both
bottom
bound
bounded
break
build
but
byte
bytes
c
calculate
call
can
cannot
case
cases
cast
casts
channel
channels
check
checks
clear
clone
codegen
col
column
columns
com
combinations
complex
compute
conj
considered
const
constructed
contiguous
coordinates
copy
correct
cos
could
cpp
cpu
create
created
cuda
cumprod
currently
d
datatype
declare
default
defaulting
define
defined
delete
dense
detailed
details
device
diff
difference
differences
dim
dimensional
disable
disappear
dispatch
doesn
double
down
dtypes
either
elements
empty
en
end
entirely
equal
equals
every
exactly
exist
expects
explicit
explicitly
export
eye
false
fast
features
few
file
filename
fill
first
flags
flip
floating
following
forall
format
found
from
ft
full
function
functions
future
gen
generator
generators
getDefaultCPUGenerator
github
got
greater
guarantee
guard
h
hacky
half
hamming
hann
happen
happily
has
have
having
here
high
https
hurt
ident
imag
implemented
in
incompatible
increasing
index
indices
infer
inference
inferred
initial
inplace
input
integer
integral
interleaved
internal
into
intrusive
isBoolean
isComplex
isComplexType
isFloatingType
isIntegral
itemsize
iter
iteration
jumps
just
kBool
kCPU
kDouble
kFloat
kLong
kPerChannelAffine
kPerTensorAffine
kSparse
kStrided
kaiser
keeps
key
large
last
layout
lazy
lead
length
less
like
linspace
little
ll
lock
logspace
long
loop
low
m
macro
make
makeDataPtr
making
match
matrix
max
maximum
mean
memory
merge
method
min
mode
more
move
mul
must
mutex
my
n
name
named
namedinference
names
narrow
native
nd
necessary
need
needed
negative
never
new
next
no
non
none
nonnegative
normal
not
not?
now
nullopt
nullptr
number
object
offset
one
ones
only
op
operations
operators
opt
optTypeMetaToScalarType
option
optional
options
or
org
out
output
overflows
overhead
overlapping
overloads
overrides
parallel
pass
passed
passing
per
perf
performance
periodic
permutation
pi
pin
pinned
places
please
point
points
polar
precision
process
promote
promotes
propagate
provide
provided
ptr
pull
pytorch
qscheme
quantized
quantized?
r
rand
randint
randn
random
randperm
range
real
rectangle
redundant
removal
remove
representation
requires
res
resizable
resize
resized
respectively
result
results
returned
revert
robustness
row
rows
rules
s
safely
same
sav
scalar
scale
scales
second
select
sequential
sequentially
set
setter
shared
shift
short
should
signature
similarly
simpler
since
size
skip
slice
so
something
sparse
special
specialized
specify
src
start
static
step
steps
still
storage
stride
strided
strides
stub
sufficient
suggest
support
supported
switch
sz
taken
tensor
tensors
term
than
that
then
there
these
this
three
toComplexType
toString
too
top
torch
trace
tracer
track
transpose
trapezoid
triangle
tril
triu
type
typeMetaToScalarType
types
typing
u
unary
uniform
unsafeGetTensorImpl
unsigned
unsqueeze
upper
use
using
usize
value
values
vander
vandermonde
very
via
vim
void
we
which
while
wiki
wikipedia
will
window
with
would
wrapper
z
zero
zeros
~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ARCH
ARM
ATen
CPUINFO
DWConvMicrokernelTester
NEON
REQUIRES
SSE
TEST
aarch
any
arch
arm
aten
cc
cfg
channel
channels
cpp
cpu
cr
div
dwconv
eq
ft
gt
input
inputStride
inputZeroPoint
kernel
kernelHeight
kernelWidth
kernelZeroPoint
mod
mp
multi
native
neon
none
only
output
outputStride
per
point
pytorch
qmax
qmin
qnnpack
quantized
single
src
sse
stride
subsampling
super
test
u
ukernel
up
use
vim
width
with
zero

ASSERT
AT
ATen
AnyClassType
AnyEnumType
AnyListType
AnyTupleType
AnyType
Arc
BoolType
BufWriter
By
CapsuleType
ClassType
ComplexType
ConstTypePtr
ContainedTypes
CreateWithContained
DeviceObjType
DictType
Dynamically
ERROR
EnableSharedFromThis
EnumType
FloatType
For
FunctionSchema
FunctionType
FutureType
GeneratorType
HasFreeVariables
How
If
IntType
InterfaceBar`
InterfaceType
IsModule
IsSubtypeOfExt
Kind
LayoutType
List
ListType
NoneType
NumberType
Option
Optional
OptionalType
Overload
PartialEq
PyObjectType
Python
QSchemeType
QuantizerType
RRefType
ReprStr
RequiresGrad
Returns
ScalarTypeType
Self
StorageType
Str
StreamObjType
String
StringType
Takes
TensorType
This
TupleType
Type
TypeInterface
TypeKind
TypePrinter
TypePtr
Use
VarType
`
`Foo
`annotation
`int
`printer`
additional
and
annotation
appear
appears
are
as
aten
base
be
but
c
can
case
cast
castRaw
clear
const
construct
constructor
contain
contained
containedTypes
contains
core
cpp
create
createWithContained
ct
current
custom
customize
debuggers
declarations
default
define
defined
describe
describes
details
did
different
element
elements
empty
enum
equal
equals
exactly
expect
explictly
export
ext
fall
falls
false
forall
free
friendly
from
grad
h
has
help
how
human
ident
implementation
in
includes
indicated
inferred
information
instance
instead
invalid
io
isSubtypeOfExt
its
jit
kind
like
list
macro
messages
method
might
module
more
move
need
new
non
not
null
nullopt
nullptr
object
obvious
only
or
out
output
overload
override
pass
per
pointer
printed
printer
pytorch
r
rather
raw
readable
ref
relation
renamed
replacing
repr
requires
returned
returning
returns
rhs
rules
shared
should
size
sometimes
src
static
str
str`
stream
subclass
subtype
subtyping
than
that
then
this
through
trait
tuple
type
types
u
user
users
using
value
variable
variables
version
vs
we
were
which
why
will
with
you

?
AND
AT
ATen
AVX
BFloat
CHECK
CHUNK
Calculate
Case
Currently
DISPATCH
ERROR
Each
FLOATING
For
GRAIN
Get
Glibc
In
It
LOG
LogSoftMax
Note
On
SIZE
SOFTMAX
SSE
Scalar
ScalarType
Score
See
SoftMax
SoftMaxKernel
Step
Sum
TORCH
TYPES
Tail
Tensor
The
There
This
Unify
VecHostSoftMaxBackwardLastDim
VecHostSoftMaxLastDim
VecSoftMax
Vectorization
Vectorized
`max
`tmp
across
all
an
and
apply
approximation
are
arr
as
aside
aten
avoid
backward
base
be
because
before
begin
below
bug
bugs
call
calls
case
cases
chosen
chunk
cmath
com
compiled
computations
compute
const
constexpr
counting
cpp
cpu
d
difference
digits
dim
dispatch
double
element
elements
end
enough
exactly
example
exp
fall
false
from
general
github
glibc
grad
grain
grainsize
host
https
idx
ii
implemented
improvements
in
inner
input
input`
inside
internal
into
issuecomment
issues
j
keep
kernel
kind
large
lastdim
launchpad
loadu
log
logic
loop
m
map
max
maximum
min
must
native
ndimension
necessary
net
not
number
numbers
numerical
only
operations
or
order
outer
output
parallel
parallization
part
per
perf
plus
problem
ptr
pytorch
reduce
register
result
rough
roughly
s
same
scalar
score
should
simple
size
sizeof
small
softmax
some
source
src
static
step
store
stride
sum
sum`
support
tail
task
that
there
this
thread
through
tmp
total
transitions
type
ubuntu
using
vec
vectorized
version
very
we
which
while
why
will
with
works
would
y

?
AND
AT
ATen
AvgPoolKernel
C
CHECK
ChannelsLast
Contiguous
DISPATCH
FLOATING
H
II
III
Long
MemoryFormat
Option
Pass
ScalarType
Supports
TORCH
TYPES
Tensor
Unsupported
Vectorized
W
and
as
aten
average
backward
batch
begin
break
c
case
cast
channels
compute
contiguous
continue
copy
count
cpp
cpu
d
dH
dW
default
delta
dh
dim
dimension
dims
dispatch
divide
divisor
dw
end
factor
false
format
gin
gout
grad
has
height
ih
image
in
include
index
init
input
iw
kH
kW
kernel
kh
kw
lane
last
len
loadu
local
max
mean
memory
min
move
n
native
nbatch
ndim
ndimension
next
oh
one
only
out
output
override
ow
pad
padH
padW
padh
padw
parallel
pointers
pool
pooling
ptr
pytorch
register
scalar
size
src
static
step
store
suggest
sum
supports
switch
tensors
treat
type
using
value
vec
width
with
zero

@note
A
ASSERT
AT
ATen
Allocator
CHECK
CPU
CUDA
CUDACachingAllocator
CUDAContext
CUDAHooks
CudaDeviceProp
DEPRECATED
Device
DeviceIndex
Handles
If
It
Related
That
There
This
We
Windows
access
according
against
allocator
an
and
apply
are
associated
aten
available
be
both
builds
but
c
call
can
changes
choice
class
com
common
compiled
compose
consistent
context
count
cpp
cuda
cudaDeviceCanAccessPeer
cudaDeviceProp
cudaGetDeviceProperties
current
defines
deque
device
devices
dispatch
distinct
driver
error
etc
expected
expose
file
files
flag
flags
force
from
function
functionality
functions
getCurrentDeviceProperties
getDeviceProperties
github
gpu
gpus
h
hand
https
in
included
index
info
init
initCUDAContextVectors
initDeviceProperty
instead
intended
interface
issues
lazy
linking
links
manage
manner
means
modify
modules
more
need
new
no
not
once
one
only
or
other
otherwise
overlap
own
peer
please
preferred
problem
prop
properties
property
pull
pytorch
raise
rather
report
resize
runtime
s
should
simple
simply
single
size
some
specify
src
state
static
than
that
their
there
this
torch
use
used
vectors
warp
warpSize
we
which
whose
will
with
you

@note
AT
ATen
Both
C
CHECK
CUDA
CUDAEvents
CUDAGuard
CUDAStream
Constructors
CudaEvent
CudaIpcEventHandle
Default
Device
DeviceIndex
Drop
ERROR
Event
GPU
HCC
HIP
However
Later
Less
No
Note
Option
Ord
Ordering
PLATFORM
PartialOrd
Self
Some
TORCH
The
`flags`
acquired
actual
allow
and
any
are
around
as
assign
associated
aten
avoid
be
before
below
block
but
calculating
called
can
catch
cmp
completed
const
constructed
context
copyable
cpp
create
createEvent
created
creating
cuIpcGetEventHandle
cuIpcOpenEventHandle
cuda
cudaError
cudaErrorNotReady
cudaEventCreateWithFlags
cudaEventDestroy
cudaEventDisableTiming
cudaEventElapsedTime
cudaEventQuery
cudaEventRecord
cudaEventSynchronize
cudaIpcEventHandle
cudaIpcGetEventHandle
cudaIpcOpenEventHandle
cudaStreamWaitEvent
cudaSuccess
current
default
derive
destruction
device
devices
does
done
drop
either
elapsed
endif
err
event
events
ever
explicitly
false
first
flags
from
getCurrentCUDAStream
guard
h
handle
has
helper
ifndef
in
index
initially
ipc
isCreated
kCUDA
lazily
left
match
movable
move
moveHelper
ms
must
new
no
not
object
once
operator
or
other
partial
pytorch
query
raise
reconstructed
record
recorded
recording
resources
right
s
safely
same
sets
should
specified
src
stream
streams
supported
swap
synchronize
than
that
this
throw
time
try
u
unless
use
value
was
will
with
wrappers
yet

?
ATen
B
C
INT
Size
assert
aten
biased
bits
c
clamped
const
cpp
cpu
fmagic
fmax
fmin
fp
ft
imagic
input
lmax
lmin
long
lrintf
magic
n
native
none
output
point
pytorch
qmax
qmin
qnnp
qnnpack
quantized
requantization
requantize
rounded
scalar
scale
scaled
src
u
vim
w
y
z
zero

ATen
C
ClampingParams
PyTorchQnnpAddQuantizationParams
PyTorchQnnpAvgPoolQuantizationParams
PyTorchQnnpConvDynamicQuantizationParams
PyTorchQnnpConvQuantizationParams
PyTorchQnnpFormat
PyTorchQnnpOperator
PyTorchQnnpOperatorUnion
PyTorchQnnpQ
PyTorchQnnpU
PyTorchQnnpUKernelType
PytorchQnnpOperator
RequantizationParams
Size
SparseMatrix
Sparsity
UINT
add
adjustment
aten
average
avgpool
batch
bias
block
bottom
buffer
channel
channels
clamp
clamping
col
const
conv
convolution
cpp
cpu
dilation
dq
dwconv
dynamic
element
enum
format
gemm
global
group
groups
h
height
indices
indirection
input
kernel
last
left
log
lookup
lut
matrix
max
min
native
none
operator
output
packed
padding
params
per
pixel
point
pointer
pooling
prepackA
prepacked
pytorch
qnnp
qnnpack
quantization
quantized
quint
requantization
right
row
scale
shuffle
size
softargmax
sparse
src
stride
sum
support
table
top
type
u
ukernel
union
valid
values
void
weights
width
xFF
xzp
zero

?
ATen
Leaky
NULL
PRIu
PyTorchQnnpOperator
PyTorchQnnpStatus
QNNPACK
ReLU
Size
allocate
and
aten
batch
be
because
below
bytes
c
calloc
channels
const
cpp
cpu
create
delete
enum
error
exceed
failed
finite
flags
format
ft
goto
in
initialized
input
invalid
isnormal
leaky
less
log
long
lookup
lrintf
lut
malloc
max
memory
min
must
native
nc
negative
non
none
not
number
op
operator
out
output
parameter
params
pixel
point
positive
properly
pytorch
qnnp
qnnpack
quantized
quint
range
ratio
relu
scale
scaled
setup
size
sizeof
slope
src
status
stride
structure
success
table
type
u
ukernel
uninitialized
unsupported
vim
with
y
zero
zu

A
ATen
SequenceNumber
and
aten
autograd
backward
cpp
enumeration
forward
framework
h
increment
lazy
link
local
nr
number
observers
ops
pass
peek
pytorch
sequence
simple
src
static
thread
u
used

ARM
ATen
NEON
REQUIRES
RMAX
RMaxMicrokernelTester
SSE
TEST
U
aarch
any
arch
arm
aten
cc
cfg
cpp
cpu
div
eq
gt
lt
mod
n
native
neon
pytorch
qnnpack
quantized
rmax
src
sse
super
test
u
ukernel
use
usize

ASSERT
ATen
Compute
Create
NE
NEAR
TanH
TanHOperatorTester
Verify
and
assert
aten
batch
batchSize
begin
bind
c
channels
const
cpp
cpu
create
default
delete
destroy
device
distribution
end
fill
generate
h
initialize
input
inputScale
inputStride
inputZeroPoint
isnormal
iteration
iterations
max
min
mt
native
nc
nullptr
operator
output
outputRef
outputScale
outputStride
outputZeroPoint
point
pool
pytorch
qmax
qmin
qnnp
qnnpack
quantized
random
randomDevice
ref
reference
results
rng
scale
scaledTanHX
setup
size
src
status
stride
success
tanh
tanhOp
tanhX
test
tester
testq
this
thread
u
uniform
usize
xA
y
zero

ADD
ALIGN
ATen
Add
Args
B
BEGIN
CMP
CODE
CSEL
Compute
DIRECTIVES
ELF
END
EOR
EXT
FADD
FMUL
FUNCTION
First
GNU
HS
IGNORE
It
LD
LDP
LDR
LO
LS
LSL
Load
MOV
Mul
NE
R
RET
S
SCVTF
SMLAL
ST
STP
SUB
SUBS
Second
Shift
TOS
This
USUBL
aarch
activation
align
aten
b
base
be
because
block
blocks
byte
bytes
c
ch
channel
const
contain
conv
cpp
cpu
d
dq
dynamic
element
endif
first
ft
gemm
h
id
ids
ifdef
ifndef
index
indx
k
lazy
loop
mr
multiplier
n
native
needs
neon
next
none
nonzero
note
nr
nth
number
offset
out
output
packed
packedA
params
passed
per
point
pointer
points
processed
processes
progbits
ptr
pytorch
qnnp
qnnpack
quantization
quantized
restrict
row
s
second
section
should
sp
sparse
src
stack
static
stride
temp
that
u
ukernel
union
usize
v
value
values
via
vim
void
w
which
zero

ASSERT
ATEN
ATen
CPU
CppFunction
DISPATCH
DispatchKey
DispatchKeySet
Does
Doesn
ExcludeDispatchKeyGuard
GenericMode
GenericWrapper
GenericWrapperTensorImpl
Global
Handle
IMPL
In
IncludeDispatchKeyGuard
JIT
JitStack
LIBRARY
MAKE
Mode
ONLY
OperatorHandle
Rewrap
STATIC
Self
TESTING
TODO
TORCH
Tensor
TensorImpl
This
Unwrap
Wrapper
actually
all
an
and
anything
args
arguments
as
aten
b
backend
base
batch
be
because
both
but
c
call
callBoxed
case
cases
cast
cfg
core
could
count
counter
cpp
cpu
detail
device
dispatch
do
doesn
ease
example
explicitly
fallback
fallthrough
false
file
generic
gives
gm
guard
has
ids
implementation
interesting
isTensor
jit
kDouble
key
lazy
list
m
make
makeFallthrough
makeFromBoxedFunction
mod
mode
more
move
mul
new
no
norm
not
ones
op
outputs
override
passes
point
pop
propagate
push
pytorch
rep
rets
returns
schema
set
simple
simply
size
so
src
stack
starting
static
style
super
tensor
test
testing
things
this
through
toTensor
torch
trigger
type
underlying
unsafeToTensorImpl
use
used
usize
we
with
wrapper
yes
zeros

ANY
API
ASSERT
AT
ATen
All
AllowLegacyTypes
Ambiguity
Arc
Args
As
Autograd
AutogradCPU
AutogradOther
BOXING
Benchmarks
Box
BoxedKernelFunction
C
CHECK
CIRCUITED
CPU
Call
CompositeImplicitAutograd
Create
DO
DispatchKey
DispatchKeySet
DispatchTable
Dispatcher
Example
FAST
Fast
For
FuncPtr
FuncType
HAVE
However
INTERNAL
If
Instead
InternalBoxedKernelFunction
Its
JitStack
KernelFunction
KernelFunctor
Kernels
Keys
LOOK
Lambda
MyFunctor
NB
NOT
None
Note
OperatorHandle
OperatorKernel
Or
Please
Plumbing
Python
Rather
Return
SHORT
STACK
See
Self
Stack
String
TLS
TODO
TORCH
Tensor
The
This
Through
To
TraceTypeEverything
Tracer
Tracing
VariableTypeEverything
What
When
YOU
You
\n
\nCanonical
\n\n
`
`InferenceMode
`tensor
actually
add
all
allow
allows
also
always
ambiguous
an
and
any
anything
arbitrarily
are
args
argument
arguments
as
aten
attempt
autograd
autogradother
automatically
available
b
backend
backends
based
be
because
been
before
behavior
better
bit
both
box
boxed
boxing
bug
but
bypass
c
calculate
call
callBoxed
called
calling
calls
can
case
cased
cases
catchall
choice
circuited
class
codegen
codepath
comment
common
compile
compiler
composite
computation
compute
computed
const
convention
core
could
cpp
create
created
current
currently
decay
decide
decltype
dedicated
defined
details
directly
disambiguate
dispatch
dispatcher
distinguished
do
doc
does
doesn
doing
drop
dump
dumpState
during
enable
ensure
entry
equals
error
every
everything
examples
executed
exist
existent
exists
expect
expensive
explanation
fake
fallback
falling
fallthrough
false
file
final
find
fine
first
forward
from
func
function
functor
future
gets
give
global
good
guaranteed
guard
guts
h
handle
handled
has
have
helpful
highest
how
implementation
implementations
implements
in
inference
inside
instead
intend
internal
into
invariants
involves
isn
its
jit
just
kernel
kernel?
kernels
key
keys
keyset
know
knowing
lambda
lazy
left
let
line
lives
local
lowering
make
makeFromBoxedFunction
makeFromUnboxedFunction
makeFromUnboxedFunctor
makeFromUnboxedLambda
makeFromUnboxedRuntimeFunction
makes
mapped
maps
masked
match
mechanism
message
mitigate
mod
mode
model
more
move
n
name
named
names
namespace
near
necessary
need
new
next
no
non
none
normally
not
notably
notes
occur
old
one
only
op
open
operation
operator
opposed
opt
optionally
or
order
oss
ostringstream
other
out
over
overhead
override
pass
passed
passing
path
peek
performance
pick
please
plumbs
pointer
precedence
prefer
present
priority
proceed
processed
public
pytorch
raises
read
reading
recalculating
redispatch
redispatches
redispatching
reg?
register
registered
registers
registration
rely
removed
rename
repeated
reporting
request
requires
reserved
result
rule
runtime
s
save
saying
see
semantics
set
setting
short
should
shown
signature
similar
single
skipping
so
some
special
specialized
specially
specific
src
stack
state
state\n~~~~~~~~~~~\n
stateless
static
still
storage
stored
stores
str
summary
super
support
supported
table
takes
tensor
tensors
testing
than
that
their
then
there
they
this
thread
through
time
torch
touching
tracker
training
trick
triggered
try
type
types
typically
unboxed
unboxing
unimplemented
unique
universally
unnamed
unreachable
updated
upon
us
use
used
useful
user
using
usually
valid
value
ve
via
void
want
was
way
we
whether
which
why
will
with
withDispatchKeys
work
works
would
wrapper
write
y
yet
yields
you
yourself
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ADD
APPLE
ATen
At
BEGIN
BEQ
BHS
BLO
BNE
But
CMP
Decrement
ELF
END
F
FUNCTION
For
GNU
If
Increment
LDM
LDR
LSL
Load
POINT
POP
PUSH
Pre
Push
S
STR
SUB
SUBS
Since
Store
TST
This
Thus
U
VADD
VCVT
VDUP
VEXT
VLD
VLDM
VLDR
VMAX
VMIN
VMLAL
VMUL
VPOP
VPUSH
VQMOVN
VQMOVUN
VSHL
VST
VSTR
VSUB
VSUBL
We
ZERO
aarch
add
address
again
align
and
another
arch
arm
armv
assembly
aten
b
back
base
be
below
bytes
c
can
channel
channels
const
conv
copy
cpp
cpu
d
decrement
depth
do
dont
dwconv
element
endif
enough
fpu
from
ft
gets
h
have
hi
ifdef
ifndef
in
include
increment
index
indirection
input
kernels
lazy
lo
load
lr
maintain
more
native
neon
non
none
note
o
occur
orig
original
output
overwritten
params
pc
per
pixel
pixels
point
pointer
pointers
pop
pops
pre
process
progbits
push
pushes
pytorch
qnnp
qnnpack
quantization
quantized
r
registers
reloaded
remainder
requant
requantization
restrict
runtime
scale
scales
section
since
single
some
sp
src
stack
static
store
stride
syntax
that
u
ukernel
unified
union
up
updated
used
usize
vfmagic
vfmax
vfmin
vim
vimagic
vinput
vkernel
void
w
we
weights
width
will
wise
working
zero

ATen
PyTorchQnnpConvQuantizationParams
Since
Still
We
aarch
aligned
always
are
assume
assuming
aten
builtin
c
channel
clamped
const
cpp
cpu
dup
end
endif
ft
gemm
high
ifdef
in
index
input
k
kernel
lane
loading
low
lower
max
min
mr
multiple
n
native
neon
none
nr
only
output
params
point
points
predecrement
pytorch
qnnpack
quantization
quantized
replicate
requantization
s
scale
scales
shift
src
stride
sub
this
u
uint
uintptr
ukernel
up
upper
used
usize
v
va
vacc
vaddq
values
vb
vcombine
vcvtnq
vcvtq
vdupq
vext
vextq
vfmagic
vfmax
vfmin
vget
vim
vimagic
vld
vmax
vmaxq
vmin
vminq
vmlal
vmov
vmulq
void
vout
voutput
vqaddq
vqmovn
vqmovun
vreinterpret
vreinterpretq
vset
vshl
vst
vsubl
vsubq
vxa
vxb
w
we
zero

ATen
B
Both
C
Clang
Conversion
Convert
FP
However
INT
In
Large
Product
Rounding
Select
Since
Size
This
We
absolute
add
adding
addition
after
already
an
and
any
as
assert
aten
be
before
biased
bit
bits
builtin
but
c
can
cause
clamp
clamped
clamping
concat
const
conversion
convertvector
cpp
cpu
cvt
documentation
does
even
exactly
expect
extension
floating
fp
ft
gcc
generally
gets
guarantee
have
in
input
instruction
integer
into
intrinsic
intrinsics
involve
just
large
less
lieu
limited
load
low
magic
max
min
multiply
must
n
native
nearest
needed
none
not
number
only
operation
operations
output
packed
performed
point
provide
psimd
pytorch
qmax
qmin
qnnp
qnnpack
quantized
range
representation
represented
requantization
requantize
result
round
rounded
rounding
roundings
s
saturation
scale
scaled
smaller
so
splat
src
store
subrange
substracing
such
than
that
then
this
thus
ties
trick
u
use
using
value
values
vectors
vfmagic
vfmax
vfmin
vim
vimagic
vscale
w
we
well
will
with
works
would
xy
xyzw
y
z
zero
zw

ATen
PyTorchQnnpAvgPoolQuantizationParams
aarch
address
aligned
assert
assume
aten
bias
builtin
c
const
cpp
cpu
defined
do
dup
endif
ft
gavgpool
hi
high
increment
input
lane
lo
low
m
max
min
n
native
neon
none
output
params
point
pytorch
qnnpack
quantization
quantized
s
scale
src
stride
u
uint
uintptr
ukernel
up
usize
vacc
vaddl
vaddq
vaddw
vbias
vcombine
vcvtnq
vcvtq
vdupq
vext
vfmagic
vfmax
vfmin
vget
vi
vim
vimagic
vld
vmax
vmaxq
vmin
vminq
vmov
vmulq
vout
voutput
vqaddq
vqmovn
vqmovun
vreinterpret
vreinterpretq
vscale
vshift
vshl
vst
vsubq
vsum
while
zero

?
A
ACTION
AN
AND
ANY
ARCH
ARISING
AS
AT
ATen
AUTHORS
AccScalar
Acquire
Apache
Approximate
Assumes
Authors
Avoid
BE
BSD
BTRS
BUT
BaseSampler
Bernoulli
BernoulliStub
Beta
Betas
Boost
CHECK
CLAIM
CONNECTION
CONSECUTIVE
CONTIGUOUS
CONTRACT
COPYRIGHT
CPU
CPUGeneratorImpl
CUDA
Cauchy
CauchyStub
Cephes
Clause
Compute
Copyright
DAMAGES
DBL
DEALINGS
DIG
DISPATCH
Dirichlet
Distributions
Double
E
EVENT
EXPRESS
Exp
Exponential
ExponentialStub
FITNESS
FLOAT
FLOATING
FLT
FOR
FORMAT
FROM
Fast
For
Gamma
Generate
Generator
Geometric
GeometricStub
HIPCC
HOLDERS
Half
Here
Hoermann
Hormann
However
IMPLIED
IN
INCLUDING
INFINITY
INT
IS
In
It
KIND
Kern
LEGACY
LIABILITY
LIABLE
LICENSE
LIMITED
Library
License
LogNormal
LogNormalStub
Long
MANT
MAX
MEMORY
MERCHANTABILITY
MIT
Marsaglia
Math
NAN
NO
NONINFRINGEMENT
NOT
No
Normal
NormalSampler
NormalStub
Normalize
Note
Numpy
OF
OR
OTHER
OTHERWISE
OUT
Option
Other
PARTICULAR
PROVIDED
PSI
PURPOSE
Permission
Poisson
Push
RNG
ROCM
Random
RandomFromToMeta
RandomFromToStub
RandomStub
Reference
Region
Reject
Rejection
Rice
Robert
SHALL
SOFTWARE
Sampler
Sanity
Scalar
ScalarType
See
Self
Since
Software
THE
TO
TODO
TORCH
TORT
TYPES
Taylor
Tensor
TensorFlow
TensorIteratorBase
TensorIteratorConfig
That
The
Then
This
To
Transformed
Tsang
U
Uniform
UniformMeta
UniformSampler
UniformStub
Use
V
Version
WARRANTIES
WARRANTY
WHETHER
WITH
WITHOUT
We
`inf`
`nan`
`sample
`self`
about
above
abs
acceptance
accscalar
adapted
add
additional
affect
algorithm
all
alpha
also
an
analytic
and
answers
any
apache
apply
approx
approximation
are
argmax
arguments
as
associated
asymptotic
aten
available
avoid
axbx
b
back
base
be
bernoulli
beta
betas
binomial
bits
bivariate
both
boundary
bounding
box
break
btrs
build
but
c
calculated
calculates
can
cannot
capture
cast
casted
casting
categories
cauchy
cc
cdf
ceil
cfg
charge
check
checking
checks
close
coef
coefficients
com
compat
computation
condition
conditions
consecutive
const
constexpr
contains
continue
converges
copies
copy
copyright
correction
count
counterpart
cpp
cpu
cu
cuda
d
dalpha
dbetas
deal
decltype
dedupe
default
define
defined
den
denom
derived
deviates
device
digamma
dim
dirichlet
dispatch
dist
distribute
distribution
distributions
div
do
documentation
does
doesn
doi
double
drawn
dtypes
easy
either
element
elif
empty
endif
enlam
ensure
eps
equivalent
error
exceed
exp
expand
expansion
expected
expects
exponential
fabs
factor
false
files
floating
floor
following
formula
found
free
from
full
function
functions
furnished
gamma
gamma`
gen
generate
generator
generators
geom
geometric
getDefaultCPUGenerator
github
got
gpu
grad
gradient
gradients
granted
guard
gumbel
h
half
happen
have
hcc
here
hereby
higher
hip
http
https
ignore
implement
implementation
implemented
implements
in
included
including
index
input
integer
invalid
invalpha
inversion
invoke
isFloatingType
isnan
issuecomment
issues
item
iter
j
just
k
kLong
kTailValues
keepdim
kern@gmail
kernel
kp
lambda
large
largest
lazy
len
lgamma
licensed
licenses
like
limit
limitation
lock
log
loglam
logp
logprob
low
lower
m
make
max
may
mean
median
merge
meta
method
mid
min
missing
modify
more
multinomial
must
mutex
n
nan?
native
negative
new
nexttoward
no
non
normal
not
note
notice
numCategories
number
numer
numerical
nvcc
nvstd
obtaining
one
only
op
options
or
org
os
our
out
output
outside
pairs
paper
parameter
path
pdf
performed
permission
permit
person
persons
pi
point
poisson
poisson`
polevl
poly
portions
pow
precision
prefactor
prevent
prob
probabilities
probability
prod
protection
publish
pytorch
qprob
r
random
range
rate
ratio
rational
real
recursion
regime
reject
rejection
released
reparameterized
replacement
representable
resize
restriction
result
ret
rights
risk
robert
s
saddle
same
sample
sampler
samples
sampling
scalar
scale
scaled
section
sell
sensical
serial
series
shall
should
side
sigma
simplify
since
singularity
size
slam
small
so
softmax
software
some
spq
sq
sqrt
src
stability
standard
static
stddev
stirling
stub
subject
sublicense
substantial
sum
summand
supported
supports
tail
tan
templates
tensor
term
terms
that
theory
there
this
tight
times
topk
total
transformed
type
u
ua
under
underflow
uniform
until
upperbound
us
use
uses
using
usize
uv
v
val
valid
vals
value
very
void
vr
was
we
well
which
while
whom
will
with
without
work
write
wrt
www
xx
y
yay
z
zero
zeros
~

ATen
AVX
DATA
DEFINE
DO
Do
HEADER
IN
Make
MapOp
NOT
Note
Op
ReduceOp
STATIC
Scalar
See
THIS
TODO
Vectorized
acc
all
arr
aten
but
compile
const
cpp
cpu
d
efficient
fun
functional
h
initializers
input
into
loaded
loadu
map
more
next
not
output
outputs
pair
pytorch
red
reduce
reduces
set
similar
size
src
store
this
using
vec
with

?
AFTER
API
APIType
APIs
ASSERT
ATen
AliasAnalysisKind
An
ArgTypeTestKernel
ArrayRef
Autograd
AutogradCPU
AutogradCUDA
AutogradOther
AutogradPrivateUse
AutogradXLA
BackendSelect
C
CPU
CPUTensorId
CUDA
CUDATensorId
Call
Cannot
CatchAll
CompositeExplicitAutograd
CompositeImplicitAutograd
Could
CppFunction
DeeplyNestedType
DeviceType
Dict
Direct
Discovered
DispatchKey
DispatchKeySet
Dispatcher
Does
DummyKernel
DummyKernelWithIntParam
EXPECT
Ensure
Error
Explicitly
FALSE
FROM
FULL
FUNCTION
Fallthrough
Function
IMPL
IValue
In
Input
InputType
Internal
Just
KernelFunction
LIBRARY
List
MAKE
Mismatch
MockKernel
More
NB
Note
Only
OpRegistrationListener
OpRegistrationListenerForDelayedListenerTest
OperatorHandle
OperatorKernel
OperatorRegistrationTest
Optionally
Output
OutputType
PURE
Please
PrivateUse
RegisterOperators
Rewrite
SCHEMA
Scalar
Self
Should
Similar
Some
SparseCPU
Stack
String
Suspect
TEST
TODO
TORCH
TRUE
Tensor
Tensor?
Test
TestArgTypes
TestLegacyAPI
TestModernAPI
TestModernAndLegacyAPI
The
These
This
Thus
TorchJitparseSchema
Torchdispatch
Torchschema
Tracer
Tracing
Tried
When
XLA
\n
\t
`Input`
`Output`
`inputExpectation`
`input`
`outputExpectation`
`output`
access
accessing
actualOutput
add
addRegistrationListener
additional
after
alias
aliasAnalysis
all
allowed
alpha
already
also
always
an
and
are
arg
argument
arguments
around
as
assert
aten
autograd
autogradcpu
available
back
backend
backends
base
based
basics
be
because
been
before
behavior
below
bool?
both
boxed
but
c
call
callOp
callOpUnboxed
callOpUnboxedWithPrecomputedDispatchKeySet
called
calling
calls
can
cannot
case
cases
catch
catchAll
catchAllKernel
catchall
caught
caused
check
com
composite
confusing
const
containing
contains
convention
conventions
conversion
convert
core
correct
correctly
could
cpp
cppreference
cpu
cuda
currently
danglingImpls
decltype
deeply
def
default
delayed
deprecated
deregistered
deregistering
deregisters
detailed
dict
different
direct
directory
discouraged
dispatch
dispatched
dispatcher
disptach
do
does
doesn
don
done
double
dummy
dummyTensor
either
emplace
empty
en
error
etc
even
expect
expectListEquals
expectThrows
expectation
expecting
explicit
explicitly
extenders
extractDispatchKey
fails
fallback
falls
fallthrough
false
file
files
filled
findDanglingImpls
findOp
findSchema
findSchemaOrThrow
fine
first
float?
found
from
fulfills
function
functions
general
generic
gets
given
givenOpWithCatchallKernel
givenOpWithDispatchedKernel
gone
googletest
grad
handle
has
have
hello
here
higher
https
ids
implicit
implicitly
impls
import
in
indeed
infer
inferred
initial
inner
input
inputExpectation
insert
inside
instead
int?
intentionally
internal
invoke
isDefaultAliasAnalysisKind
isNone
its
just
k
kAutograd
kCPU
kCUDA
keep
kernel
kernels
key
keys
kind
ks
lambda
lambdas
language
last
lazy
level
library
list
listener
logic
long
lower
m
macro
macros
make
makeDeeplyNestedObject
makeFallthrough
makeFromBoxedFunction
makeFromUnboxedFunction
map
mapped
maps
matches
math
means
message
met
mismatching
mixed
move
multiple
my
name
namespace
nested
new
no
non
nonautograd
none
not
nothing
now
nullopt
object
off
ok
one
only
op
operator
operators
ops
optional
options
or
order
other
out
output
outputExpectation
outputs
overload
overrides
part
passed
pick
plumbing
pointer
possible
precedence
precompute
primitive
priority
privateuse
ptr
push
pytorch
reaches
redispatch
redispatches
redispatching
reenable
register
registerFallback
registered
registering
registers
registrar
registration
registrations
registry
regular
require
requires
result
retest
returned
returning
returns
running
s
same
schema
schemas
scope
second
select
set
should
show
shown
signature
signatures
since
single
singleton
size
so
some
sparsecpu
specified
specify
specifying
src
stack
stackBasedKernel
stackbased
static
str
str?
succeeds
support
supported
synonyms
take
takes
taking
tensor
tensors
test
testArgTypes
tests
than
that
their
then
thenFails
there
this
through
throws
toBool
toDouble
toGenericDict
toInt
toListRef
toString
toStringRef
toTensor
toTypedDict
top
torch
tracing
tree
tuple
twice
type
typed
types
unboxed
unique
unordered
until
up
updatedDispatchKeySet
updates
use
used
users
using
usually
v
value
values
void
w
ways
we
weird
whenRegisteringCatchallKernel
whenRegisteringCatchallKernelInSameOpCall
whenRegisteringDispatchedKernel
whenRegisteringDispatchedKernelInSameOpCall
which
while
will
with
without
work
works
wrong
xla
y
you

A
ALGO
API
ASSERT
AT
ATen
Algo
AlgorithmSearch
AlgorithmSearchMiOpenConvBwdDataAlgorithm
AlgorithmSearchMiOpenConvBwdWeightsAlgorithm
AlgorithmSearchMiopenConvFwdAlgorithm
Aristotle
Aten
Auto
Avoid
Backward
BenchmarkCache
Benchmarking
Bias
C
CHECK
CPU
CheckedFrom
Checking
Constant
Conv
Convenience
Convolution
ConvolutionArgs
ConvolutionDescriptor
ConvolutionParams
Convolutions
DC
DEFAULT
DataType
Depthwise
Drop
ERROR
FilterDescriptor
For
HIP
HIPCachingAllocator
HashMap
Hashing
In
Input
It
MIOPEN
MIOpen
MaybeOwned
MiOpenConvAlgoPerf
MiOpenConvBwdDataAlgorithm
MiOpenConvBwdWeightsAlgorithm
MiOpenConvFwdAlgorithm
MiOpenConvolutionBwdDataAlgoGEMM
MiOpenConvolutionBwdWeightsAlgoGEMM
MiOpenConvolutionFwdAlgoGEMM
MiOpenHandle
Mutex
NB
NOTE
NULL
Not
Note
OOM
Option
POD
ParamsEqual
ParamsHash
Perf
Remove
See
Self
THCudaFree
THCudaMalloc
TODO
TORCH
Tensor
TensorArg
TensorDescriptor
TensorGeometryArg
The
There
This
Too
Transposed
Used
Weight
Workspace
add
addition
algo
algorithm
algos
although
ambiguity
and
another
are
arg
args
arguments
around
as
assert
assumed
aten
b
back
backward
backwards
bdesc
be
because
begin
begins
being
benchmark
benchmarked
best
bias
borrow
build
but
bwd
bwdDataAlg
bwdFilterAlg
c
cache
can
cast
catch
cdesc
cfg
channels
char
check
checkAllSameGPU
checkAllSameType
checkDimRange
checkSameDim
checkSize
checking
checks
choose
chooseAlgorithm
clear
compiled
compute
computed
condition
const
contenst
contig
contiguous
conv
convolution
convolutions
copy
correctly
count
cpp
cudnn
dataType
default
defined
depthwise
desc
descriptors
design
deterministic
device
dilation
dim
directly
distinguish
do
doesn
don
drop
easily
empty
emptyCache
end
enough
entirely
entry
error
errors
everything
exception
exclusive
expand
expected
expecting
exposed
false
fastest
feature
few
file
filter
find
findAlgorithm
forward
from
further
fwd
fwdAlg
getMiopenDataType
getMiopenHandle
getWorkspaceSize
globalContext
got
gpus
grad
greater
group
groups
guard
hacky
handle
handles
hashes
hashing
have
hip
hipGetDevice
hipGetLastError
id
idesc
idx
in
input
insert
invoke
invokes
iterator
its
just
lazy
lazyInitCUDA
let
lock
machinery
many
map
mask
max
may
maybe
memcmp
memory
memset
miopen
miopenConvBwdDataAlgorithm
miopenConvBwdWeightsAlgorithm
miopenConvFwdAlgorithm
miopenConvolution
miopenConvolutionBackwardBias
miopenConvolutionBackwardData
miopenConvolutionBackwardDataGetWorkSpaceSize
miopenConvolutionBackwardWeights
miopenConvolutionBackwardWeightsGetWorkSpaceSize
miopenConvolutionForward
miopenConvolutionForwardBias
miopenConvolutionForwardGetWorkSpaceSize
miopenConvolutionMode
miopenDataType
miopenDepthwise
miopenFindConvolutionBackwardDataAlgorithm
miopenFindConvolutionBackwardWeightsAlgorithm
miopenFindConvolutionForwardAlgorithm
mod
mode
multiple
must
mutex
n
name
narrow
native
needed
negative
never
new
not
odesc
omitted
one
opt
optional
options
ostream
out
output
owned
pad
padding
parameter
parameters
params
part
passing
perf
perfResults
philosophy
place
pod
pointers
preprocessor
prevent
ptr
purposely
pytorch
raw
re
read
reasons
record
reinterpret
removal
resize
result
results
reuse
rocm
search
second
see
seems
set
setConvolutionParams
shape
should
since
size
sized
sizeof
so
src
ss
static
str
stride
stringstream
super
supplied
support
swaps
switch
sz
takes
tensor
than
this
thread
too
transpose
transposed
try
tuple
type
u
unambiguous
uniformity
us
use
used
using
usize
value
values
via
void
vs
wdesc
we
weight
weights
while
with
wonder
workspace
would
wrapper
ws
wsscache
wssizes
you
zero

?
ASSERT
ATen
AlignedAllocator
BCSRMatrix
Compute
Debug
END
GemmBlockSparseMicrokernelTester
GemmDqSparsePAckedAUKernelFunction
GemmDqSparseUKernelFunction
GemmSparsePackAUKernelFunction
K
LE
LT
M
Matrix
Mr
NE
Nr
PyTorchQ
START
Temp
This
While
\n
\n\n
aPtr
aStride
aZeroPoint
acc
accordingly
allocated
and
arguments
assert
aten
b
bZeroPoint
bcsr
be
begin
bernoulli
bias
biasn
bind
bit
block
blocks
c
cStride
cbegin
cend
col
colBlockSize
const
conv
cout
cpp
cpu
default
dequantization
device
dist
distribution
do
does
dynamic
elem
element
end
endl
ensure
fill
fillBlockSparseWeights
fire
ft
gemm
generate
generateBlockCSRMatrix
h
has
indices
iteration
iterations
k
kIndex
kb
kernel
kernels
kr
ks
later
loop
m
mIndex
mat
matrix
max
microkernel
min
mr
mt
multiplier
n
nIndex
name
native
nb
ne
none
not
nr
optimized
output
packa
packed
packing
padded
params
point
points
print
ptr
pytorch
qgemm
qmax
qmin
qnnp
qnnpack
quantization
quantizationParams
quantized
random
randomDevice
real
ref
reference
removed
results
reuse
rng
row
rowBlockSize
s
scales
size
sizeof
sparse
sparsity
src
stride
test
tester
that
this
thus
u
uniform
unique
usize
utils
value
values
vim
we
weights
what
while
will
zero

?
AT
ATen
CHECK
D
ERROR
Expected
Given
TORCH
Tensor
allow
and
as
aten
batch
be
blocks
but
calculated
check
col
cpp
dilation
dim
dimension
dimensions
dims
div
divisible
expected
got
grad
greater
h
height
im
input
kernel
length
match
mode
n
native
ndim
ndimension
negative
non
number
only
or
other
output
pad
padding
plane
positive
possibly
product
pytorch
rtn
s
shape
should
size
sliding
small
spatial
src
stride
tensor
than
too
valid
which
width
with
zero

ATen
Affine
Compare
Copied
Dequantize
Each
EqualTo
For
IntrusivePtr
IntrusivePtrTarget
Note
QScheme
QTensorImpl
Qscheme
Quantization
Quantize
Quantized
Quantizer
QuantizerBase
QuantizerInterface
QuantizerPtr
ScalarType
Self
Tensor
We
When
`other`
`this`
about
account
add
against
all
also
an
and
are
as
aten
base
be
bump
can
class
common
concrete
core
corresponding
cpp
creating
csrc
dequantize
different
enum
equal
equality
example
from
h
have
hold
holds
immutable
incref
information
inside
instance
into
intrusive
ir
jit
leaf
ll
make
mapping
might
most
multiple
necessary
need
new
one
operation
or
other
ownership
parameters
perform
please
point
pointer
ptr
pytorch
qscheme
quantization
quantize
quantized
quantizers
raw
reclaim
refcount
requires
s
same
scalar
scale
scheme
schemes
scope
share
should
since
so
src
store
storing
support
sure
that
they
this
torch
trait
type
types
unique
use
we
will
you
zero

ATen
Apply
ArgNames
Args
BENCHMARK
Benchmark
BenchmarkState
C
CharacteristicArguments
MAIN
NO
PYTORCH
QNNPACK
SetBytesProcessed
SetItemsProcessed
Sigmoid
SkipWithError
arguments
aten
b
batchSize
begin
bench
bind
bytesPerIteration
c
cast
cc
cfg
channels
characteristic
const
cpp
cpu
create
delete
device
distribution
end
failed
fill
flags
generate
initialize
input
itemsPerIteration
iterations
lazy
max
min
mt
n
native
nc
not
nullptr
operator
output
point
pool
pytorch
qnnp
qnnpack
quantized
random
randomDevice
range
ref
rng
scale
setup
sigmoid
sigmoidOperator
sizeof
src
state
static
status
stride
success
thread
u
uniform
usize
xA
zero

?
AT
ATen
D
DISPATCH
FILL
FLOATING
IP
IW
KW
MaxPooling
NB
NC
OP
OW
PoolingParams
SJ
Scalar
TYPES
Tensor
Value
aten
begin
const
constexpr
contiguous
cpp
cpu
d
dispatch
end
fill
has
ij
in
index
infinity
input
ip
isnan
kernel
kj
limits
lowest
max
n
native
oe
oj
op
output
padding
parallel
pool
ptr
pytorch
register
scalar
src
start
stub
type
update
used
val
valid

ATen
EXPECT
FALSE
TRUE
Tensor
add
and
aten
begin
contiguous
cpp
dense
expand
expanded
insert
lazy
memory
non
ones
overlap
overlapping
pytorch
rand
scalar
size
src
static
tensor
test
transpose
vec

ADD
ALIGN
ATen
Add
Adjust
And
Args
B
BEGIN
CMP
CODE
CSEL
Callee
Channel
Compute
DIRECTIVES
ELF
END
EXT
FCVTNS
FMOV
FMUL
FUNCTION
Fold
GNU
HS
IGNORE
LD
LDP
LDR
LO
LS
LSL
Load
MOV
NE
POINT
R
RET
S
SCVTF
SMLAL
SQADD
SQXTN
SQXTUN
ST
STP
SUB
SUBS
Store
TOS
UMAX
UMIN
USHL
USUBL
ZERO
aarch
align
and
architecture
arm
assembly
aten
b
base
bias
bit
bits
byte
bytes
c
call
ch
channel
channels
com
const
conv
cpp
cpu
d
developer
docs
endif
ft
gemm
h
https
ifdef
ifndef
ihi
include
index
indx
k
lazy
lower
lsl
max
min
mr
mul
native
need
neon
none
note
nr
offset
only
out
output
params
passed
per
point
pointer
points
post
procedure
progbits
pytorch
qnnp
qnnpack
quantization
quantized
registers
requant
requantization
restrict
results
runtime
s
save
scale
section
shift
sp
src
stack
standard
static
stride
u
ukernel
union
usize
v
va
vacc
vb
via
vim
vmax
vmin
void
vzero
w
with
zero

?
ATen
Args
Backward
Bool
BoolTensor
CHECK
CPU
CUDA
Delta
Fake
FakeQuantize
Float
Forward
Let
Mask
MemoryFormat
Note
Op
PerTensorAffine
Preserve
Quantized
Returns
ScalarType
TODO
TORCH
Tensor
TensorIteratorConfig
The
This
Use
Xfq
Xq
Y
\
\Delta
\\
\begin
\end
\frac
\max
\min
\text
`X`
`dY`
`fake
`mask`
`quant
`zero
add
additional
affine
affine`
and
are
as
aten
backend
backward
be
below
bit
build
but
byte
cachemask
calculated
can
cases
cast
clamped
computed
const
cpp
dScale
dX
dY
dZeroPoint
d\Delta
define
device
dispatch
double
dx
dy
dz
element
empty
end
equal
equivalent
existing
factor
fake
false
forward
fp
fq
from
further
future
grad
gradient
gradients
has
in
include
input
into
inv
item
iter
kBool
kernels
learnable
less
like
look
lower
make
mask
math
max
max`
maximum
memory
min
min`
minimum
multiplication
must
native
nearbyint
need
needed
no
not
numerically
only
op
optional
options
or
output
over
overhead
packing
pass
path
per
point
point`
pre
pytorch
qmax
qmin
quant
quantization
quantize
quantized
quantizes
range
register
res
returned
same
saving
scalar
scale
scheme
should
since
size
src
static
stub
sum
sums
tensor
than
total
tuple
type
unsqueeze
use
uses
val
value
vec
vectors
version
we
what
will
with
z
zero

?
ARG
ARGS
BLOCKS
COND
CUDA
Can
DIM
FORMAT
GET
GPUs
NULL
NUM
Need
Please
Round
SIZE
STATE
Some
THArgCheck
THAssertMsg
THCDescBuff
THCIndexTensor
THCTensor
THCUNN
THCudaLongTensor
THError
THREADS
Use
VA
above
and
are
arg
as
assert
aten
be
block
blocks
but
cast
check
checkGPU
common
const
constexpr
cpp
cuda
d
device
different
dim
dimension
division
do
element
elements
equals
export
got
gpu
gradient
h
has
have
ident
indices
input
isSameSizeAs
ld
located
macro
many
match
max
move
n
nDimensionLegacyNoScalars
nElement
not
number
one
or
per
positive
ptrdiff
pytorch
requires
resize
resizeAs
rules
s
same
schedule
shape
shapes
single
size
sizeDesc
sizeLegacyNoScalars
sm
src
state
static
str
tensors
thcunn
them
threads
too
up
weight
which
while

@colesbury
@soumith
@zasdfgbnm
ATen
CHECK
CI
CUDA
CreateMIOpenHandle
DESTROY
DestroyMIOpenHandle
DeviceThreadHandlePool
Further
HANDLE
HIP
Handle
It
MIOPEN
MIOpenPoolType
MiOpenHandle
NO
PoolWindow
PoolWindows
See
Sometimes
This
Thread
Windows
above
already
and
are
as
aten
atexit
avoid
back
be
because
caused
com
context
cpp
create
cuda
decided
destroy
destroyed
destruction
device
disabled
dumb
endif
fbcode
getCurrentHIPStream
gets
github
globally
h
handle
handles
hangs
happens
hip
hipGetDevice
https
ifdef
in
initialization
initialized
issues
its
lazily
local
make
mentioned
mio
miopen
miopenCreate
miopenDestroy
miopenSetStream
myPoolWindow
newPoolWindow
not
note
now
or
ordering
pen
pool
ptr
ptrs
pull
pytorch
releasing
reserve
reserved
same
seeing
setting
shared
something
src
static
terminates
that
this
thread
time
type
unique
we
will
workaround
would

ATen
INTERNAL
PYTORCH
QNNP
aten
clamp
clamping
const
cpp
cpu
declare
export
function
h
ident
macro
n
name
native
neon
params
pytorch
qnnp
qnnpack
quantized
rules
src
sse
u
ukernel
union
usize
void
y

ANY
ASSERT
ATen
Args
Atest
Attempt
BOOL
BinaryOpsKernel
BoolMask
CASE
Device
EXPECT
FLOAT
Float
FloatMask
For
INT
INTBOOL
INTBOOLFLOAT
INTFLOAT
If
Infers
IntMask
MASK
Scalar
ScalarType
TEST
THROW
TODO
TRUE
Tensor
TensorOptions
Test
accessor
add
all
allclose
and
args
aten
atest
b
backward
base
be
binary
bit
bits
blob
boolean
but
const
correct
cpp
default
device
dimensional
do
empty
eq
equal
example
exception
exp
expected
ezyang
floats
fmod
foo
from
func
function
ge
gt
hasCUDA
holds
in
integer
isgone
item
j
kBool
kByte
kCUDA
kFloat
kInt
kernel
le
logical
lt
manual
max
maybe
min
mod
more
nd
ne
not
ones
only
op
operator
operators
ops
option
or
out
output
over
pow
precise
ptr
pytorch
rand
rd
reset
resize
running
seed
set
should
sigmoid
size
specify
src
st
strides
tensor
tensors
test
tested
trace
type
u
unit
up
v
view
void
will
with
wrong
xor
y
~a

?
AT
ATen
BetaBackward
C
CHECK
ComputeInternalGradients
D
DISPATCH
FLOATING
G
GammaBackward
GroupNormBackwardKernel
GroupNormBackwardKernelImpl
GroupNormBackwardKernelImplInternal
GroupNormInputBackward
GroupNormKernel
GroupNormKernelImpl
GroupNormKernelImplInternal
HxW
K
RowwiseMoments
Scalar
TORCH
TYPES
Tensor
Vectorized
Y
accumulate
arr
aten
backward
beta
bias
c
cast
cbegin
cend
compute
const
constexpr
cpp
cpu
d
dX
dY
db
dbeta
defined
dgamma
dispatch
ds
dx
dy
empty
end
eps
gamma
gradients
group
hxw
inner
input
internal
j
k
kernel
loadu
max
mean
memset
native
norm
null
nullptr
options
parallel
ptr
pytorch
register
rstd
s
scalar
scale
size
sizeof
sqrt
src
start
static
store
tie
type
utils
v
val
vec

ADD
ATen
AddOperatorTester
OP
aScale
aStride
aZeroPoint
add
aten
b
bScale
bStride
bZeroPoint
batch
batchSize
cc
channels
cpp
cpu
iterations
native
point
pytorch
qmax
qmin
qnnpack
quantized
scale
small
src
stride
strided
test
testQ
u
unit
usize
with
y
yScale
yStride
yZeroPoint
zero

ATen
Arc
CompilationUnit
ComplexType
ConstTypePtr
Contained
CustomTensor
Dict
DictType
EXPECT
Even
First
FloatType
IValue
IntType
Interface
Interfaces
JIT
List
ListType
MyNamedTuple
NE
Named
OneForward
Put
R
Rewrite
Rewritten
String
Tensor
TensorType
TorchJitSource
TorchJitSourceImporter
TorchTensor
Torchrand
Trivially
Tuple
TupleType
Tuples
TypePrinter
TypePtr
Unrelated
WowSoDifferent
across
affected
and
annotation
another
anywhere
aten
b
bar
basic
be
c
cast
class
classType
classes
compare
compilation
const
constantTable
contained
cpp
create
createNamed
cu
custom
d
def
defined
dictType
different
differentField
differentName
equal
equality
even
field
foo
forward
have
import
importType
inequality
inside
intType
interface
interfaceSrc
interfaceType
iv
listType
loadType
make
match
must
my
name
named
namedTupleType
namedtuples
names
not
nullopt
one
only
optional
outerTupleType
pass
printer
provided
ptr
pytorch
qual
rewritten
same
share
shared
shoudl
should
si
so
source
src
still
str
structurally
tensorType
test
them
these
they
torch
tuple
tupleType
tuples
type
typed
types
units
version
very
work
wow
y

ALL
AND
API
AT
ATen
BLOCK
Both
C
CHECK
COMPLEX
CPU
Cannot
Casting
Copies
Copy
Copying
DISPATCH
Device
DeviceType
Devices
ERROR
FBGEMM
FBGeMM
FP
Float
FloatToFloat
Format
Found
GRAIN
Half
IMPLEMENTED
MIN
Memory
NC
NOT
NR
NoNamesGuard
OK
ONCE
Other
Quantized
R
Re
SIZE
SZ
Scalar
TODO
TORCH
TYPES
Tensor
TensorIterator
TensorIteratorConfig
Tensors
This
ToFloat
VULKAN
WARN
XLA
above
add
all
allowed
also
and
are
aten
be
begin
blocking
both
bp
broadcast
buf
build
but
c
calls
can
case
cast
check
checking
columns
complex
compute
const
contiguous
conversion
copies
copy
cpp
cpu
declare
define
defined
dense
dequantize
destination
device
dim
directly
discards
dispatch
dispatched
dst
during
either
empty
enable
end
endif
error
exists
extra
false
fbgemm
following
from
generalized
guard
h
handled
has
here
ifdef
ignored
imaginary
implementation
implemented
in
includes
inplace
input
internal
into
iter
just
kBFloat
kBool
kByte
kCPU
kCUDA
kFloat
kHIP
kHalf
kMetal
kVulkan
kernel
kernels
lazy
matrix
max
may
maybe
memcpy
meta
metal
min
missing
most
namedinference
names
native
nc
need
needed
no
non
nonempty
not
nr
only
ops
options
or
out
outnames
output
outputs
overlapping
overriding
parallel
part
path
place
please
propagate
proper
ptr
pytorch
qscheme
quantized
quantizer
r
rc
real
reinterpret
resize
rows
rp
rpo
s
same
scalar
see
set
setup
should
simd
similar
size
sizeof
source
sp
sparse
special
spo
src
static
stride
strides
stub
support
supported
tensor
tensors
that
there
this
tmp
toString
transpose
transposed
trickier
type
types
undefined
use
using
valid
values
void
vulkan
we
will
with
works
xla

ATen
AlignedAllocator
Apply
ArgNames
Args
BENCHMARK
Benchmark
BenchmarkState
Bottleneck
CAPTURE
Conv
D
Fire
G
K
M
MAIN
MobileNet
MobileNetV
NO
PYTORCH
Post
Pre
PyTorchSGemmUKernelFunction
QNNPACK
ResNet
SetItemsProcessed
ShuffleNet
ShuffleNetV
SkipWithError
SqueezeNet
SqueezeNetV
VGG
aarch
any
arch
arm
aten
b
begin
bench
benchmark
bind
c
cache
cc
cfg
clamping
clampingParams
const
cpp
cpu
cpuinfo
d
device
distribution
divide
divideRoundUp
end
failed
fill
fp
generate
group
groups
in
infinity
initialization
initialize
iterations
k
kc
kcStride
kr
lazy
limits
m
mb
mc
min
mobile
mobilenet
mr
mt
n
nanf
native
nb
nc
ncStride
neon
net
netv
not
np
nr
pack
params
pooling
psimd
pytorch
qnnp
qnnpack
quantized
random
randomDevice
range
real
ref
res
reserve
resnet
rng
round
roundUp
scale
sgemm
sgemmBenchmark
shuffle
shufflenet
size
sizeof
squeeze
squeezenet
src
state
static
u
ukernel
uniform
up
usize
v
vgg
w
with

API
ATen
Add
Another
BackendSelect
Build
C
CPU
CatchAll
Choose
DTORCH
Disabled
DispatchKey
FORCE
If
Internally
MOBILE
Note
OPERATOR
OperatorNameView
PyTorch
QuantizedCPU
REGISTRATION
Returns
SCHEMA
STRINGIZE
See
Selective
StringView
Strip
TORCH
The
This
Use
Vulkan
WARNING
WHITELIST
When
about
add
added
all
allowlist
allowlisting
an
and
anyway
arbitrary
are
as
assert
aten
based
bc
be
big
binary
blow
break
bug
build
but
calls
certain
check
coded
com
compare
compile
constexpr
contain
contains
core
correct
could
cpp
cur
d
defined
dependencies
depends
details
detectable
dispatch
doesn
done
due
enabled
endif
fail
false
find
fmt
fmtlib
function
functionality
functions
gcc
github
given
goes
h
hard
have
header
hopefully
https
ifdef
iff
implements
implicitly
in
include
included
instead
issues
item
k
key
keys
later
linker
list
macro
majorly
make
max
may
mechanism
mobile
more
name
names
need
next
no
not
now
npos
obvious
only
op
operator
operators
ops
overload
overloads
parameter
parse
pass
programming
prune
pytorch
questions
really
records
registered
registering
registration
removing
returns
root
s
schema
selective
set
should
size
so
something
src
stackoverflow
sub
substr
sufficiently
sure
that
then
there
these
this
throw
time
turn
up
usage
used
using
usize
valid
value
view
way
ways
we
weren
will
with
without
work
wrong
you
your

?
AND
AT
ATen
CHECK
COMPLEX
DISPATCH
FLOATING
LerpKernel
Scalar
TORCH
TYPES
Tensor
TensorIterator
TensorIteratorConfig
Value
Valueype
`end`
`weights`
add
aten
binary
borrowing
build
but
cpp
cpu
dispatch
end
expected
got
input
iter
kernel
lerp
native
op
output
pytorch
register
ret
scalar
src
tensor
type
typename
using
val
weight
weights
zabs

?
API
ATen
Access
Activation
BUFFER
Block
Buffer
C
CHECK
COMBINED
Command
Compute
D
DESCRIPTOR
FN
IMAGE
IMPL
IntArrayRef
It
KERNEL
LIBRARY
LIKELY
Layout
Mean
Not
OK
Object
Option
Pool
Read
SAMPLER
STORAGE
ScalarType
SmallVector
Stage
TORCH
TYPE
Tensor
UNIFORM
VK
VULKAN
Vulkan
Write
access
adapter
an
and
api
appropriate
arg
async
aten
back
barriers
batch
block
buffer
but
bypasses
cfg
channels
command
const
context
convert
cpp
currently
d
dim
dimensional
dims
dispatch
downcast
expected
expects
extents
false
final
ft
gpu
group
handle
has
height
iextents
image
implemented
implied
input
insert
inserts
keep
keepdim
lazy
lifetime
local
m
managed
mean
native
necessary
none
normalize
not
object
only
ops
options
output
pool
push
pytorch
queue
range
reduction
resource
safe
set
size
src
static
stream
submit
supports
synchronization
tensors
track
triggers
u
uniform
unordered
utils
uvec
v
vTensor
vim
vulkan
wide
width
work

ATen
PyTorchQnnpAvgPoolQuantizationParams
Size
add
adds
assert
aten
bias
c
const
cpp
cpu
cvtepi
cvtps
cvtsi
epi
epu
extract
ft
gavgpool
hi
input
insert
lo
load
loadl
loadu
m
max
min
mm
mul
n
native
none
output
packs
packus
params
point
ps
pytorch
qnnpack
quantization
quantized
scale
setzero
si
slli
src
srli
sse
stride
u
ukernel
unpackhi
unpacklo
up
vacc
vbias
vim
vinput
vout
vscale
vscaled
vxinput
vzero
while
xm
zero

CONCAT
DataPtr
Free
Here
If
NAME
Note
Once
Real
Storage
StorageImpl
TH
THError
THPPointer
THStorage
THStorageFunctions
This
Trying
Weak
When
access
again
allocate
allocator
and
any
are
as
aten
atomically
be
byte
bytes
called
can
capacity
case
copy
count
cpp
dead
deallocated
decref
defined
does
exists
export
finalizers
free
freeing
from
getTHDefaultAllocator
greater
h
have
hpp
ident
incref
increment
independent
intrusive
libc
live
long
macro
make
memcpy
monotonic
more
move
must
nbytes
necessary
never
new
non
not
nullptr
number
object
old
one
or
plus
pointer
pointers
ptr
ptrdiff
pytorch
raw
realloc
refcount
refcounting
references
release
report
resizable
resize
retain
rules
s
scheme
set
since
size
src
stays
storage
strong
th
than
that
there
transition
type
use
via
way
weak
weakcount
you
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

?
AT
ATen
AdaptiveAvgPoolKernel
C
CHECK
ChannelsLast
Contiguous
DISPATCH
FLOATING
For
H
II
III
MemoryFormat
Note
Pass
Scalar
Supports
TORCH
TYPES
Tensor
Unsupported
Vectorized
W
adapative
adaptive
and
as
aten
average
backward
batch
begin
block
break
c
cache
case
channels
compute
consider
contiguous
copy
cpp
cpu
d
default
delta
dim
dimension
dispatch
end
false
fit
format
gin
gout
grad
height
ih
in
index
init
input
iw
kernel
kh
kw
lane
last
loadu
local
memory
move
n
native
nbatch
ndim
ndimension
next
oh
one
only
oridinary
otherwise
out
output
ow
parallel
pool
ptr
pytorch
register
scalar
scenario
should
size
src
start
step
store
suggest
sum
switch
treat
type
usage
using
vec
width
zero

?
AND
AT
ATen
Acquire
CHECK
CPUGeneratorImpl
DISPATCH
Do
FLOATING
Generator
Get
HALF
LIBCPP
Make
MultinomialKernel
NB
NaN
Note
Option
Scalar
See
TORCH
TYPES
Tensor
VERSION
and
apply
assume
aten
be
binary
bode
bucket
cast
categories
check
compat
const
cpp
cpu
cum
cumulative
datatypes
default
defined
dim
dispatch
dist
distr
distribution
doesn
double
empty
encountering
endif
entry
falls
from
gen
generator
generators
getDefaultCPUGenerator
guard
half
idx
ie
in
incremented
infinity
invalid
isfinite
j
kernel
last
left
libc
lock
lua
manually
mass
mid
multinomial
mutex
n
native
normalize
normalized
one
options
or
original
perform
pointer
prob
probabilities
probability
ptr
pytorch
random
real
register
replacement
result
right
row
sample
scalar
search
size
slot
so
src
static
store
stride
stub
sum
sums
sure
tensor
that
type
uniform
using
val
we
well
which
while
will
with
wrapper
zeros

ATen
Design
PYTORCH
QNNPACK
QUANTIZATION
RUNTIME
Run
aten
cpp
cpu
endif
ft
h
native
neon
none
point
pytorch
qnnpack
quantization
quantized
requantization
runtime
src
sub
time
u
uint
va
vim
vmovl
vsubl
vzp
zero

ATen
AVX
Default
Defined
Emulate
Ensure
F
FF
FFF
FFFF
FFFFF
FFFFFF
FFFFFFF
MSC
Negation
Note
Op
See
Self
Sizeype
This
VER
Vectorized
VectorizedInt
Vectorizedi
We
abs
add
advanced
align
aligned
also
always
an
and
arange
arrays
as
aten
avx
b
base
be
because
behavior
binary
bit
blend
blendv
but
c
can
case
cast
cfg
change
clamp
class
cmpeq
cmpgt
com
combining
compatible
compile
compiled
compiler
conj
const
constexpr
content
convert
could
count
cout
cpp
cpu
cvtepi
cvtepu
d
defined
definition
delete
derive
details
develop
developer
divides
do
documentation
does
don
double
dst
dummy
dump
efficiently
element
elementwise
emulate
emulated
en
enable
endif
endl
epi
eq
extensions
extract
extracting
feature
frac
gcc
ge
github
gt
guide
h
half
has
have
here
html
https
idx
ifdef
ifndef
ignore
ignores
imag
implemented
in
initialize
input
instruction
instructions
intel
intentionally
into
intrinsic
intrinsics
inverse
invert
issues
larger
lazy
le
like
ll
load
loadl
loadu
loop
lowest
lt
m
make
mask
max
maximum
memcpy
memory
min
minimum
mm
more
mullo
multiplies
multiply
multiplying
n
native
ne
need
needs
neg
new
no
not
one
ones
op
operation
operations
operator
operator~
or
other
output
pd
performing
point
pointwise
pragma
ps
ptr
public
pytorch
real
reference
reinterpret
results
same
set
setr
si
size
sizeof
so
software
src
static
step
store
storeu
sub
support
switch
technically
then
this
tmp
top
type
typename
u
ubsan
undefined
uninitialized
unroll
us
using
usize
utilize
v
val
value
values
vec
void
we
well
while
with
would
www
xF
xFF
xFFF
xFFFF
xFFFFF
xFFFFFF
xFFFFFFF
xor
zero

?
API
AT
ATen
ERROR
IsVulkanAvailable
Self
Tensor
VULKAN
Vulkan
VulkanCopy
VulkanImplInterface
VulkanImplRegistrar
VulkanInterface
api
aten
atomic
available
backend
build
const
copy
cpp
endif
extern
false
h
ifdef
lazy
linked
load
native
new
not
pytorch
registry
src
static
store
trait
vulkan
was

?
AND
AT
ATen
BFloat
Blas
By
CHECK
COMPLEX
CUBLAS
Check
Contiguous
D
DEVICE
DISPATCH
FLOATING
FUNC
Found
Half
IMPL
INT
IntArrayRef
MAX
MODE
Make
MaybeOwned
MemoryFormat
NoNamesGuard
POINTER
PointerModeGuard
Scalar
ScalarType
TODO
TORCH
TYPES
Tensor
TensorArg
That
We
`vec
`vec`
accordingly
actually
addmm
addmv
all
alpha
an
and
args
as
aten
avoid
baddbmm
batch
batches
be
below
beta
betaval
bgemm
blas
bmm
borrowed
both
bound
build
but
bypass
call
calling
case
cases
cast
check
checkAllSameGPU
clone
cmat
common
complex
compute
const
contents
contiguity
contiguous
copy
could
cpp
cublas
cuda
d
definition
dense
dependencies
device
dim
dimension
dispatcher
dot
double
elements
empty
expand
expected
false
fast
fixing
found
func
gemm
gemv
getCurrentCUDABlasHandle
got
guard
handle
have
here
ignored
in
inconsistent
incx
incy
infs
instead
internal
k
kCPU
keep
later
layout
lazy
ld
lda
ldb
ldc
leading
like
m
mat
match
matrix
max
may
memory
mm
mul
must
n
namedinference
names
nans
native
needing
non
nonempty
not
nullopt
number
only
options
other
out
outnames
output
overlapping
owned
parameters
pathological
perf
pin
pointerModeGuard
preflights
prepare
propagate
ptr
pytorch
r
requires
resize
respectively
result
same
scalar
shortcut
should
size
some
squeeze
src
static
stride
stride`
strides
supports
sure
sync
tensor
tensors
that
this
though
toComplexDouble
transpose
try
type
update
val
values
vdot
vec
vectors
will
with
zero
zeroed

ALWAYS
AND
ATen
Acceptable
Both
C
CMP
DEFINE
DP
Evaluate
Express
Extended
INLINE
MEMBER
NAN
Note
ONE
OP
OQ
Pi
See
Sizeype
Sleef
TERNARY
The
Vectorized
VsxMask
abs
absolute
acos
acosf
add
alias
align
all
an
and
angle
anonymous
another
arange
are
arithmetic
as
asin
asinf
assuming
atan
atanf
aten
attribute
avx
b
base
be
bit
bits
blend
blendChoice
blendv
boundary
branches
c
calc
can
case
cast
ceil
cephes
check
checks
class
cmp
cmpeq
cmpge
cmpgt
cmple
cmplt
cmpne
comparision
computed
conj
const
constexpr
conversion
copy
copysign
copysignf
correct
cos
coscof
cosh
count
cout
cpp
cpu
delete
directly
div
dp
dump
elements
enable
endif
endl
eq
erf
erfc
erfcf
erff
erfinv
even
exp
expm
exponent
extract
first
flag
floor
fmod
fmodf
frac
from
ft
fuction
fx
ge
generated
gt
h
half
header
helpers
here
hi
hypot
hypotf
idx
igamma
igammac
imag
imm
implementation
in
include
inf
integer
internal
intrinsics
inv
isinf
isnan
j
lazy
ld
le
lgamma
lgammaf
ln
lo
loadu
log
loge
logf
logic
lt
lvsl
m
madd
magic
map
mapbi
mask
masks
mathfun
max
maximum
may
memcpy
min
minimum
minus
mm
modifications
modular
movemask
mul
n
namespace
nan
nand
nd
ne
neg
negln
nextafter
nextafterf
none
nor
odd
offset
once
one
operation
operator
or
other
others
out
part
pass
pf
pi
poly
polynom
polynoms
possible
pow
pragma
precision
private
ps
ptr
public
pytorch
real
reciprocal
recp
reinterpret
result
ret
returned
returns
round
rsqrt
same
scalar
scale
second
see
sel
select
selection
set
should
sign
signed
simulation
sin
sincof
sinh
size
sizeof
sl
sleef
some
sources
splats
sqrt
src
st
static
step
store
sub
swap
switch
take
tan
tanf
tanh
temp
then
there
this
tmp
translated
treat
trunc
type
typename
u
union
update
upper
use
used
using
usize
v
vabs
value
values
vbool
vbpermq
vcheck
vec
vecb
vf
vfloat
vi
vim
vint
vm
vmask
void
vsx
vu
vuint
vv
we
will
with
without
xor
xtract
y
z
zero
~

ANY
ASSERT
ATen
Basic
CPU
DeprecatedTypeProperties
ExpandError
MismatchedSizes
OldFallback
THROW
TRUE
Tensor
TestEmptyTensor
TestExplicitDimBasic
TestExplicitDimWithMismatchedSizes
TestExplicitDimWithScalar
TestIn
TestOut
WithScalar
aClone
aScalar
aTensorScalar
add
addcmul
addmm
arg
args
aten
b
bScalar
basic
behavior
broadcast
c
can
clone
cpp
dim
empty
equal
error
expand
expanded
explicit
fallback
function
have
in
inplace
kFloat
manual
mismatched
old
ones
out
place
pytorch
randn
scalar
seed
specification
src
tensor
test
with
would
yields

AT
ATen
CHECK
Change
Currently
ENABLED
MKLDNN
MemoryFormat
Mkldnn
Option
TORCH
Tensor
TensorShape
are
aten
axes
begin
cfg
clone
compiled
compute
const
copy
cpp
device
dim
direct
does
dst
end
false
format
from
h
has
ideep
in
infer
inferred
instead
iota
itensor
memory
mkldnn
move
native
ndims
new
not
operations
opt
optTypeMetaToScalarType
option
optional
options
place
pytorch
reshape
size
src
support
supported
swap
tensor
transpose
unsupported
use
value
view
with
y
yet

ADD
APPLE
ATen
BEGIN
BEQ
BHS
BLO
BNE
But
CMP
Decrement
ELF
END
F
FUNCTION
For
GNU
If
Increment
LDM
LDR
LSL
Load
POINT
POP
PUSH
Pre
S
STR
SUB
SUBS
Since
TST
This
U
VADD
VCVT
VDUP
VEXT
VLD
VLDM
VLDR
VMAX
VMIN
VMLAL
VMUL
VPOP
VPUSH
VQMOVN
VQMOVUN
VSHL
VST
VSTR
VSUB
VSUBL
ZERO
aarch
add
address
again
align
and
another
arch
arm
armv
assembly
aten
b
back
below
bytes
c
channels
const
conv
cpp
cpu
d
decrement
depth
dwconv
element
endif
fpu
ft
gets
h
have
ifdef
ifndef
in
include
increment
index
input
kernel
kernels
lazy
load
lr
native
neon
non
none
note
o
occur
output
overwritten
params
pc
pixel
point
pop
pre
process
progbits
push
pytorch
qnnp
qnnpack
quantization
quantized
r
remainder
requantization
restrict
runtime
scale
section
since
single
sp
src
stack
static
stride
syntax
that
u
ukernel
unified
union
up
used
usize
vfmagic
vfmax
vfmin
vim
vimagic
vinput
vkernel
void
w
we
weights
width
will
wise
zero

ASSERT
ATen
ArrayRef
Atom
CHECK
Cached
Can
DELIMITER
Delimiter
For
Helper
INTERNAL
Ideally
Invalid
Is
PartialEq
QualifiedName
Represents
Self
String
TORCH
The
Unnecessary
`
`atoms
`name`
`other`?
`this`
accessors
actual
allowed
append
aten
atom
atoms
back
bar
bare
base
baz
be
begin
below
bigger
cache
cacheAccessors
can
cannot
const
copy
core
cpp
d
delimiter
derived
dots
dotted
emplace
empty
end
eq
example
false
finalAtom
find
foo
form
from
fully
h
hash
in
insert
into
its
join
just
lazy
leading
like
list
move
must
n
name
names
namespace
new
no
noexcept
not
npos
operator
or
other
otherAtoms
out
pos
prefix
prefixView
push
pytorch
qualified
qualifiedName
qualifier
reserve
s
size
slice
something
split
src
startSearchFrom
static
substr
this
thisAtoms
u
use
usize
v
view
we
while

ATen
CLAMP
ClampOperatorTester
OP
and
aten
batch
batchSize
cc
channels
clamp
cpp
cpu
equal
input
inputStride
iterations
max
native
output
outputStride
pytorch
qmax
qmin
qnnpack
quantized
small
src
stride
test
testU
u
uint
unit
usize
with
zero

AND
AT
ATen
AVX
BFloat
But
CAPABILITY
COMPLEX
CPU
Complex
DISPATCH
Dispatch
Double
ExpScalar
FLOATING
Float
Half
INTEGRAL
PowKernel
Scalar
ScalarType
ScalarTypeToCPPType
So
TYPES
TensorIteratorBase
The
This
Vectorized
aforementioned
allow
allows
also
and
approach
are
as
aten
barring
base
be
being
both
but
calls
can
cannot
cast
common
complex
computation
const
cpp
cpu
currently
decltype
dispatch
distinction
doesn
double
either
equal
even
exist
exp
exponent
fast
handled
in
isComplexType
isFloatingType
iter
kBFloat
kHalf
kernel
kernels
mixed
multiple
native
not
optimized
or
order
output
parameter
path
pow
powi
precision
prevent
provide
pytorch
reciprocal
register
resolve
rsqrt
scalar
similar
since
small
source
specialization
specializations
sqrt
src
standard
static
stub
support
takes
tensor
this
types
use
used
using
vec

ALL
AND
AT
ATen
COMPLEX
Cast
DISPATCH
GRAIN
RangeFactoriesKernel
SIZE
Scalar
TYPES
TensorIterator
Vectorized
`end`
`start`
`step
acc
accscalar
all
and
arange
aten
be
begin
can
cast
conditional
const
cpp
cpu
dispatch
double
end
false
halfway
idx
integral
internal
iter
kBFloat
kernel
larger
linspace
native
parallel
pytorch
range
register
res
scalar
serial
should
since
size
src
start
static
step
steps
stub
t`
than
type
types
using
value
vec

ATen
Alt
Array
BINARY
Broadcast
Conversions
Currently
DEFINE
Default
EMULATE
FloatVecReturnType
IntVecReturnType
MASK
MEMBER
OP
Self
SizeType
The
This
UNSIGNED
ValueType
Vbool
VecInternalMaskType
VecInternalType
Vectorized
VectorizedQuint
Vint
Vuint
When
acceptable
add
align
always
an
and
are
argument
arithmetic
as
aten
b
bandwidth
be
bound
c
carried
cases
cast
classes
cmpeq
cmpge
cmpgt
cmple
cmplt
cmpne
const
constexpr
constructor
converters
count
cout
cpp
cpu
defines
dequantize
derive
doing
dump
efficient
elementwise
endl
expected
file
floating
follows
from
full
function
h
in
inp
inverse
iterations
kernels
lazy
ld
let
loadu
loop
madd
mask
max
maximum
memcpy
min
minimum
mod
mul
multiplier
new
offset
operations
operator
operators
or
other
out
over
packs
packsu
point
precision
premul
ptr
pytorch
qint
quantize
quantized
quint
reinterpret
relu
requantize
returned
rhs
rint
scale
signed
simply
six
size
sizeof
special
specified
splats
src
st
static
store
sub
subtract
super
that
these
tmp
type
types
u
underlying
union
unpackh
unpacking
unpackl
unsigned
unwrap
use
usize
usually
v
val
vals
value
values
vec
vecBi
vecBshi
vecb
vecf
veci
vecs
vecshi
vectorized
vectors
vf
vfloat
vi
vint
vmask
vmax
vmin
void
vsx
vuint
we
widening
will
with
writing
xFF
xor
zero
zp

@
A
APPLE
AT
ATen
BASE
CHECK
DISPATCH
Don
Drop
ERROR
FLOATING
GENERAL
ILP
INDEX
INT
LAYOUT
LP
MACH
MAJOR
MATRIX
MKL
MSC
MSVC
Macros
MatrixDescr
NON
OPERATION
Pytorch
ROW
SPARSE
Scalar
ScalarType
Self
SparseCsrLinearAlgebra
SparseCsrMKLInterface
SparseCsrTensor
SparseMatrix
TORCH
TRANSPOSE
TYPE
TYPES
Tensor
This
VER
WARN
Windows
ZERO
addmm
alpha
and
any
archive
articles
aten
beta
build
c
cfg
change
col
com
compile
compiled
compiler
const
convert
cpp
create
crow
csr
d
default
dense
desc
destroy
detect
disabled
drop
elif
endif
error
ever
failed
false
feature
fixes
future
github
h
how
http
https
iOS
ifdef
index
indices
issuecomment
kInt
kLong
linking
macos
macros
mkl
mm
mod
mv
nadeausoftware
native
ncols
needs
new
not
nrows
operating
org
predefined
proofing
ptr
pull
pytorch
res
retval
routines
s
scalar
since
size
some
source
sparse
src
stat
stopping
super
support
system
tip
type
use
using
values
warnings
we
web
will
with

AT
ATen
ComplexKernel
DISPATCH
FLOATING
Scalar
TYPES
TensorIterator
aten
b
complex
cos
cpp
cpu
dispatch
input
iter
kernel
native
polar
pytorch
register
sin
src
stub

ATen
Option
TODO
Tensor
TensorCompare
TensorMath
as
aten
cpp
cpu
descending
dim
false
forward
indicies
make
max
min
move
native
per
point
pytorch
quantized
repr
reshape
scale
sort
src
stable
tensor
tie
tuple
zero

ATen
C
CHECK
MOBILE
NoNamesGuard
Pooling
TORCH
Tensor
TensorArg
adaptive
and
argument
aten
ceil
check
checkDim
const
contain
count
cpp
d
defined
dilation
empty
endif
function
got
guard
include
indices
kernel
make
max
mkldnn
mode
name
namedinference
names
native
one
output
pad
padding
pool
propagate
pytorch
quantized
reset
should
size
squeeze
src
stride
tie
tuple
u
unsqueeze
use
with
xnnpack

?
A
ARM
AT
ATen
All
Apply
C
CHECK
CONTIGUOUS
CPU
CUDA
Calculate
Calculated
ChannelsLast
Check
Compute
Contiguous
Conv
ConvForward
ConvNd
ConvParams
Convolution
CuDNN
Currently
D
DIM
Deterministic
DimVector
Disable
Display
ENABLED
ERROR
Enabling
Expand
Expected
F
FORMAT
FP
Float
For
Formatter
Given
GradMode
IMPL
IMPLEMENTED
If
Inconsistent
Input
IntArrayRef
Invalid
Kernel
LEGACY
LIBRARY
MAX
MEMORY
MIOPEN
MIOpen
MKLDNN
MM
MOBILE
MaybeOwned
MemoryFormat
Modified
NCHW
NEON
NHWC
NNPACK
NOT
Not
Note
ONCE
Only
Operations
Option
Result
See
Self
SmallVector
StringView
THNN
TODO
TORCH
Tensor
Tensors
This
Transpose
Use
Using
View
WARN
We
XNNPACK
You
accidentally
accumulate
activate
actual
add
allow
allowTF
already
and
are
as
asymmetric
aten
available
avoid
b
back
backend
backends
backward
batch
be
because
been
begin
below
benchmark
benchmarkCuDNN
benchmarks
bfloat
bias
big
bit
borrow
broadcast
bs
but
bypass
c
calculate
call
can
cannot
case
cast
cat
ch
chan
channel
channels
check
checks
com
compiledWithCuDNN
compiledWithMIOpen
compute
computed
cond
considering
const
constant
constexpr
contiguous
conv
convT
convolution
convolutions
copy
correct
cpp
cpu
created
ctx
cuda
cudnn
currently
d
define
defined
delta
dense
depthwise
depthwith
deterministic
deterministicAlgorithms
deterministicCuDNN
device
difference
dilated
dilation
dim
dimension
dimensional
dimensions
dispatch
divisible
do
does
doesn
don
double
due
elements
empty
enabled
end
endif
enough
ensure
equal
error
even
expand
expanded
expected
false
fast
faster
fastest
figure
filters
first
fmt
format
forward
from
fully
function
gI
gO
gOt
gW
gWt
generic
getCUDAHooks
ggI
ggIt
ggO
ggW
ggb
ggi
ggw
github
globalContext
goes
going
got
grad
greater
group
grouped
groups
gw
h
hacky
handle
handled
handling
has
have
height
here
history
https
idx
implement
implementation
implementations
implemented
in
incorrect
indexing
inference
inferring
input
insert
instead
integers
intended
irange
itself
k
kBFloat
kDimVectorStaticSize
kFloat
kHalf
kernel
kernels
lack
large
last
latter
layout
least
leaving
len
lengths
like
likely
list
log
long
lr
mask
match
max
may
maybe
memory
min
miopen
mkldnn
mode
move
multi
multiple
multiplier
multiply
must
n
nInputPlane
nOutputPlane
narrow
narrowing
native
natively
nb
nd
ndimension
need
needed
needs
neg
negative
new
nnpack
no
nogroup
non
nonpos
not
number
o
odd
only
opt
optTypeMetaToScalarType
option
optional
options
or
ostringstream
other
out
output
outputs
outsize
over
override
overrideable
overwritten
owned
pad
padded
padding
param
parameters
params
path
per
perf
permuted
pinned
please
point
pooling
portion
positive
preferred
prepacked
pull
push
pytorch
r
range
rather
reach
reduced
relevant
removal
require
requires
reshape
resize
result
s
same
save
scalar
second
send
separately
separator
set
shape
should
single
size
slice
slow
some
source
spatial
specialized
specific
split
splitting
square
squeeze
src
ss
static
still
str
stride
strided
strides
stub
subtensor
subvariable
support
supported
supports
supportsDepthwiseConvolutionWithCuDNN
supportsDilatedConvolutionWithCuDNN
swap
symmetric
tensor
tensors
term
tf
than
there
they
this
thnn
threaded
threads
three
through
toString
transpose
transposed
triggering
tuneable
tuple
type
unsafe
unsqueeze
unsupported
up
use
used
userEnabledCuDNN
userEnabledMkldnn
usize
valid
value
var
vec
version
versionCuDNN
view
w
way
we
weight
which
whole
why
width
will
winograd
with
workload
wrapper
xnnpack
yet
zero

ADD
APPLE
ATen
Add
Adjust
After
And
Args
BEGIN
BEQ
BHS
BLO
BLS
BNE
BX
Byte
CMP
Channel
ELF
END
F
FUNCTION
GNU
LDM
LDR
LDRD
LSL
Load
MOVLO
MOVLS
MOVNE
POINT
POP
PUSH
S
SUB
SUBS
Store
TEQ
TOS
U
VADD
VCVT
VDUP
VEXT
VLD
VLDM
VMAX
VMIN
VMLAL
VMOV
VMUL
VPOP
VPUSH
VQMOVN
VQMOVUN
VSHL
VST
VSUB
VSUBL
ZERO
aarch
add
after
align
and
arch
arm
armv
aten
b
base
bias
bytes
c
ch
channel
const
conv
cpp
cpu
d
endif
fpu
from
ft
ifdef
ifndef
in
index
indx
ip
k
kc
ks
lazy
load
loading
lr
mr
native
neon
none
note
nr
offset
out
output
params
passed
per
point
pointer
points
progbits
pushing
pytorch
qnnp
qnnpack
quantization
quantized
r
reg
registers
requant
requantization
restrict
scale
section
shift
sp
src
stack
static
stride
syntax
u
ukernel
unified
union
usize
va
vacc
vb
vfmagic
vfmax
vfmin
via
vim
vimagic
void
w
zero

A
ASSERT
ATen
BIT
CACHE
CHECK
COMPUTE
CREATE
Cache
Compute
DEBUG
DELETER
Deleter
Descriptor
Factory
Flags
For
GPU
Gpu
HANDLE
Handle
Host
INFO
INTERNAL
Invalid
LAYOUT
LayoutDescriptor
NULL
None
ONLY
Object
OpenGL
PIPELINE
PartialEq
Pipeline
PipelineBarrier
PipelineBarrierStage
PipelineCache
PipelineDescriptor
PipelineFactory
PipelineFactoryHandle
PipelineFactoryHasher
PipelineLayout
PipelineLayoutDescriptor
PipelineLayoutFactory
PipelineLayoutFactoryHandle
PipelineLayoutFactoryHasher
PipelineObject
PipelineStageType
ResourceBufferBarrier
ResourceImageBarrier
SHADER
STAGE
STRUCTURE
Self
Shader
ShaderModule
ShaderWorkGroup
SmallVector
TORCH
TYPE
This
Transfer
VK
VkComputePipelineCreateInfo
VkDescriptorSetLayout
VkDevice
VkPipeline
VkPipelineCache
VkPipelineCacheCreateInfo
VkPipelineLayout
VkPipelineLayoutCreateInfo
VkPipelineShaderStageCreateInfo
VkPipelineStageFlags
VkSpecializationInfo
VkSpecializationMapEntry
Vulkan
WorkGroup
Y
Z
\
all
and
api
as
aten
bitflags
buffers
bundle
cache
caches
check
coherent
compute
configure
confiurable
const
constexpr
consumption
contains
contrast
cost
cpp
create
defines
departure
description
descriptor
destruct
deterimines
device
direct
driver
dst
empty
entires
entirety
eq
execution
extra
facilities
factory
from
gpu
group
h
handle
hash
having
images
in
individually
info
information
intended
interface
invoke
layout
layouts
lazy
local
machine
main
map
memory
minimize
minimizes
mod
module
monolithic
more
move
native
navie
new
nullptr
object
objects
offsetof
one
operator
other
overhead
pattern
pipeline
promotes
purge
pytorch
reconstructions
redundant
represents
required
resources
retrieve
reuse
s
sequence
set
sets
shader
shaders
sizeof
specialization
specific
src
stage
stages
state
states
static
super
these
type
typedef
u
usage
use
usize
vk
vkCreateComputePipelines
vkCreatePipelineCache
vkCreatePipelineLayout
vulkan
with
work

API
ATen
B
CPU
Conv
FN
IMPL
LIBRARY
None
Scalar?
SerializationTypeConv
TORCH
Tensor
Tensor?
VULKAN
Vulkan
VulkanRegisterOpContextClass
W
Y
aten
cfg
clamp
class
classes
const
context
conv
convolution
cpp
createConv
d
dClampPrePackOpContext
dOpContext
dPrePack
def
dilation
getstate
groups
intrusive
lazy
m
max
min
move
native
not
op
output
padding
pickle
prepack
ptr
pytorch
setstate
src
state
static
stride
torch
unpack
vulkan

?
ASSERT
AT
ATen
Above
BACKWARD
C
CHECK
CUFFT
Calculates
Collapse
Complex
Contiguous
Contrary
Create
CuFFT
CuFFTConfig
CuFFTParams
CuFFTParamsLRUCache
CuFFTTransformType
D
DimVector
Embedded
Execute
FFT
FFTs
FORWARD
FftNormMode
Finally
First
For
Generally
GetCuFFTTransformType
HCC
HIP
HIPFFT
IFFT
INTERNAL
INVERSE
In
Inplace
IntArrayRef
Intermediate
MAX
MemoryFormat
NDIM
NOTE
Note
Only
PLATFORM
Params
Perform
Permute
R
Resort
SmallVector
Sort
Specifically
SpectralOps
Strides
TODO
TORCH
Tensor
The
Then
This
Z
`inembed`
`istride`
above
acquiring
advanced
after
again
aligned
all
always
and
any
applies
apply
arbitrary
are
as
aten
avoid
b
back
base
batch
batched
be
begin
being
break
buffer
but
c
cache
caches
can
case
cast
cbegin
cend
check
checked
clear
clone
cloned
collapsing
com
come
complete
complex
compute
computed
config
conjugate
consider
const
constant
contiguous
copy
corresponding
could
cpp
cuFFT
cuda
cufft
cufftSetStream
cufftSetWorkArea
cufftXtExec
current
d
defer
denom
device
dim
dimension
dimensional
dimensions
dims
directly
div
do
docs
doesn
double
either
element
embed
embedded
embeds
emplace
empty
enclosing
end
endif
every
example
exec
execute
execution
expected
fact
false
fft
fill
first
floating
formula
forward
from
ft
general
getCUDAHooks
getCurrentCUDAStream
getNumGPUs
gh
got
greater
guard
half
halfsize
have
hipFFT
hipfftComplex
hipfftDoubleComplex
hipfftDoubleReal
hipfftExecC
hipfftExecD
hipfftExecR
hipfftExecZ
hipfftReal
html
http
ifdef
ignoring
in
index
indices
inembed
infer
innermost
input
integers
into
invalidation
inverse
inverting
iota
isn
istride
kByte
kCUDA
kDimVectorStaticSize
kDouble
kFloat
larger
last
lastdim
layout
lazy
let
locality
locally
lock
lookup
make
matter
max
maximizes
may
means
min
mode
move
movedim
mul
multiply
must
mutex
n
narrow
native
ndim
need
needed
none
norm
normalization
normalize
nullability
nullptr
number
nvidia
offset
one
onesided
operation
option
optional
options
or
order
original
other
out
output
over
overwrite
overwritten
particular
partition
permutation
permute
place
plan
planned
point
pre
prepare
ptr
pytorch
r
re
real
reference
refers
reinterpret
remaining
requested
requires
rescale
reshape
reshaping
resize
result
resulted
results
root
same
scalar
scale
set
shape
should
signal
simple
simplified
since
single
size
slice
slicing
so
sort
sorted
sqrt
src
static
storage
stream
stride
strided
strides
subset
subtensors
such
support
supports
swap
switch
symmetry
temp
tensor
tensors
than
that
their
these
time
toComplexType
toValueType
torch
transform
transformed
transforms
twosided
type
uintptr
uncached
unique
unit
up
used
using
usize
value
via
view
viewed
void
w
was
we
were
while
with
working
workspace
ws
y
z
zeros

?
@note
ALL
ASSERT
AT
ATen
Actual
Build
CHECK
CPU
Checks
Clone
Comp
Compute
Contiguous
Convert
D
DISPATCH
DimVector
Dimname
Ensure
FUNC
Flatten
Fn
For
GRAIN
HIGHER
IMPL
INTERNAL
INTERPOLATION
If
Implementing
Interpolate
It
Julien
LINEAR
LOWER
META
MIDPOINT
MODE
Make
MemoryFormat
Move
NEAREST
NOTE
NaN
NoNamesGuard
Note
November
One
Option
P
Program
Programs
QUANTILE
QuantileInterpolationMode
Quicksort
R
Re
SIZE
Scalar
Sedgewick
See
Sort
SortFn
Sorting
StringView
TH
TORCH
TYPES
Tensor
TensorAccessor
TensorIteratorConfig
The
TopkFn
Treat
Two
Use
Vector
View
We
above
accelerator
active
adapted
add
adjust
all
allocate
along
an
and
any
apply
are
argsort
around
arr
art
article
as
assert
aten
avoid
b
based
be
begin
being
below
both
break
broadcast
build
but
c
calculate
can
cast
ceil
char
check
checkDeviceType
checkSameType
checkScalarType
choice
clone
close
com
compatibility
complex
computations
compute
computed
const
contiguous
continue
converting
copy
count
cpp
cpu
cs
csie
cut
d
declare
define
descending
device
dim
dimension
dimname
dims
dispatch
do
double
edu
efficient
efficiently
either
element
elements
empty
end
enum
erase
exist
existing
false
fill
find
first
flatten
floor
following
forward
found
from
full
gather
ge
got
grain
gt
guard
h
has
have
here
higher
hoare
http
idx
ignore
implementation
in
inclusive
index
indices
indirectly
indp
inds
input
insert
internal
interpolation
iota
ip
isnan
item
iter
its
j
k
kCPU
kDouble
kFloat
kLong
keepdim
kth
kthvalue
large
largest
last
lazy
le
lerp
less
like
limits
linear
liner
logical
lomuto
loop
lower
macros
make
masked
match
max
maybe
median
midpoint
mode
modes
move
msort
must
n
namedinference
names
nan
nanmedian
nanp
nanquantile
native
nearest
needed
no
non
not
nth
ntu
number
numpy
one
only
op
optional
options
or
other
out
output
outputs
overflow
overlap
partition
partitioning
pasted
pdf
performed
piv
pivot
points
position
possible
pow
presented
propagate
provided
pseudocode
ptr
pytorch
quantile
quantiles
query
questions
quick
quicksort
quiet
range
rank
ranks
reduce
reduced
reduction
reinterpret
remove
repr
required
resize
result
round
running
s
same
scalar
schemes
search
sedgewick
select
selected
set
shape
size
sliceSize
slightly
so
sort
sorted
squash
squeeze
src
stable
stackexchange
state
static
store
stride
strides
stub
sum
swap
synchronizing
tensor
tensors
textbook
th
that
there
this
those
three
tl
tmp
toType
too
top
topKSize
topk
torch
transpose
tuple
tw
type
typically
u
unmodified
unsqueeze
using
valp
vals
value
values
vec
view
vs
want
we
weights
which
while
will
with
won
wrap
www
y
zero
~b

?
API
ATen
Access
BUFFER
Bias
Buffer
C
CHECK
COMBINED
Command
Compute
DESCRIPTOR
Destination
DeviceType
FN
Future
HasState
IEEE
IMAGE
IMPL
INT
Impl
IntArrayRef
IntrusivePtr
It
K
KERNEL
LIBRARY
Layout
Linear
LinearOpContext
LinearOpContextPacked
LinearOpContextState
LinearOpContextUnpacked
Mm
Not
OK
Object
Option
Parameter
Pass
Payload
Pool
Read
Reason
ResourcePool
SAMPLER
STORAGE
Scalar
Self
SmallVector
Source
Stage
State
TORCH
TYPE
Tensor
The
TorchJitCustomClassHolder
UNIFORM
VK
VTensor
VULKAN
Vulkan
Weight
Write
accepts
access
addmm
alpha
an
and
appropriate
are
arg
async
aten
available
b
barriers
base
beta
bias
biases
bit
block
both
buffer
but
bypasses
can
cfg
combination
command
complement
const
context
contiguous
convert
cpp
cpu
create
defined
device
dispatch
div
downcast
dst
either
extents
false
floating
ft
future
gpu
grad
h
handle
has
have
height
host
identical
image
implemented
implied
in
index
individually
input
inserts
integers
intrusive
invalid
its
kFloat
keep
kh
kw
lazy
lifetime
linear
m
make
managed
mat
memcpy
memset
mm
move
multiplier
native
nbytes
ndimension
necessary
new
no
none
not
numbers
object
only
ops
optional
options
or
originals
output
own
pack
packed
parameter
parameters
payload
persistent
plane
point
pool
prepack
provided
ptr
pytorch
queue
representations
requires
resource
s
safe
scalar
size
sizeof
so
src
static
stream
submit
supported
synchronization
sz
tensor
tensors
their
track
triggers
type
u
uniform
unpack
unpacked
unsupported
up
usable
use
using
uvec
v
vTensor
vec
vim
vulkan
w
wait
weight
weights
which
width
with

ALIGN
ATen
COUNT
ELF
GNUC
INLINE
INTERNAL
LIKELY
MACH
MINOR
MSC
OF
PRIVATE
PYTORCH
QNNP
RESTRICT
STATIC
UNLIKELY
UNREACHABLE
VER
\
align
aligned
alignment
always
assume
aten
attribute
builtin
clang
common
condition
cpp
cpu
declspec
define
defined
do
elif
endif
expect
h
hidden
ifndef
internal
lazy
native
prefetch
pytorch
qnnpack
quantized
restrict
sizeof
src
static
trap
unreachable
visibility
while

?
AT
ATen
DISPATCH
FLOATING
RenormKernel
Scalar
TYPES
TensorIteratorBase
Vectorized
aten
blendv
cast
common
const
cpp
cpu
dispatch
eps
factor
fct
iter
kernel
maxnorm
native
norm
one
pytorch
register
renorm
s
scale
src
static
stub
using
v
vec

?
ACM
AT
ATen
Algorithm
Arguments
B
Bratley
CHECK
Contiguous
DISPATCH
FLOATING
Fill
First
For
Fox
Implementing
Instead
MAXBIT
Main
Mar
Math
MemoryFormat
Multiply
Option
P
RECIPD
Remaining
Require
Scalar
ScalarType
Section
SobolEngineOps
Softw
TORCH
TYPES
Tensor
The
This
Trans
Use
We
`SobolEngine`
`data`
`dimension`
`ltm`
`quasi`
`sobolstate`
`strides`
above
accessed
accessor
again
all
an
and
arange
are
argument
as
aten
avoid
be
bit
bitsubseq
but
can
cdot
choosing
clone
col
column
comes
consisting
core
cpp
d
deal
device
diag
diagonal
diagonals
dim
dimension
dot
dots
draw
due
element
elements
empty
engine
every
explicitly
extra
fast
ff
fill
first
forward
from
function
generated
generator
given
has
implicit
in
indexed
inferred
initialize
initsobolstate
issues
its
j
k
kFloat
kLong
last
layout
length
list
loop
lower
lsmdp
ltm
m
made
main
matrices
matrix
maxbit
memory
mul
n
native
needs
newv
obtain
operation
opt
optTypeMetaToScalarType
options
over
pass
passed
perform
performance
pg
pinned
poly
pow
power
product
ptr
pytorch
quasi
quasirandom
random
randomized
randomizing
reasons
remaining
required
result
rightmost
row
rows
s
same
samples
scramble
scrambling
see
sequence
size
sobol
sobolstate
specific
specified
square
src
ss
state
stride
sum
tensor
tensors
th
through
top
triangular
tuple
type
used
using
v
value
variable
variables
vdj
void
we
well
why
wise
with
wquasi
zero

AT
ATen
ERROR
It
Option
Tensor
TensorBody
TensorList
The
VariableMethodStubs
are
aten
backward
bind
cpp
create
dispatch
dynamic
everything
fw
grad
gradient
graph
h
here
implemented
in
just
keep
leaf
level
manually
method
native
new
not
nr
opt
output
primal
pytorch
redirects
requires
retain
retains
set
src
stubs
used
version
we

ATen
INTERNAL
PYTORCH
QNNP
aten
const
cpp
cpu
declare
export
function
h
ident
lut
macro
n
name
native
norm
pytorch
qnnpack
quantized
rules
scalar
src
u
ukernel
usize
void
y

ATen
Tensor
This
Utils
as
aten
being
buggy
check
contiguous
cpp
cuDNN
cudnn
dimension
does
equal
function
h
has
have
ignore
in
likes
makes
not
pytorch
s
setting
src
stride
strides
tensor
tensors
that
which
zero

ACM
ATen
All
DataFrame
F
For
Function
Implementing
Joe
Kuo
LARGEST
MAXBIT
MAXDEG
MAXDIM
Mar
Math
NUMBER
RECIPD
Remark
S
Section
SobolEngineOpsUtils
Softw
Tensor
The
These
This
Trans
Y
\n
`SobolEngine`
`length`
`n`
`pos`
according
add
additional
agnostic
algorithm
an
and
append
arange
are
as
assumes
astype
aten
au
batched
be
below
bit
bits
bitsubseq
bmat
cdot
column
columns
concatenate
const
constant
contains
content
conversion
core
cpp
d
dataframe
definitions
degenerate
details
df
direction
directory
edu
ensure
enumerate
expand
explicitly
extern
file
fillna
from
functions
generated
generator
h
https
import
in
included
indexed
initsobolstate
inner
integer
inter
joe
kuo
lazy
length
line
lines
maths
matrix
minimum
modified
mul
n
native
nbits
new
newaxis
nloc
not
notice
np
nubmers
number
numpy
nums
open
operations
options
ordered
pandas
parse
pd
peform
perform
point
poly
pos
position
pow
power
present
product
properly
python
pytorch
quasirandom
r
range
read
readlines
replace
represent
representation
rightmost
rows
s
script
see
sequence
should
size
sobol
sobol’s
some
split
square
src
starting
static
subsequence
sum
tensor
that
this
thius
unsw
use
used
using
v
value
values
vs
vstack
web
were
with
without
working
z
zero
~fkuo
–

ASSERT
ATen
CHECK
DEFAULT
Default
Device
DeviceGuardImplInterface
DeviceIndex
DeviceType
Event
EventFlag
INTERNAL
Metal
MetalGuardImpl
NB
NOT
Self
Stream
TORCH
These
aten
backend
base
block
c
count
cpp
current
d
derive
destroy
device
do
doesn
event
events
exchange
false
flag
functions
guard
index
lazy
metal
native
new
no
op
pytorch
query
record
register
related
s
set
src
static
stream
support
ty
unchecked
void

ASSERT
ATen
Allocate
BIT
BYTE
Batched
BitRowwiseQuantizedSBFloatToFloatOrHalf
CPU
Calculate
CatchAll
Copy
D
De
Dimensioned
E
ELEM
Embedding
EmbeddingPackedParamsBase
Example
FBGEMM
FN
FP
For
Fused
FusedNBitRowwiseQuantizedSBHalfToFloatOrHalf
Half
IMPL
INTERNAL
IntrusivePtr
LIBRARY
NAME
NOTE
NUM
PER
PackedEmbeddingBagWeight
Python
QEmbeddingUnpackWeights
RATE
SELECTIVE
Sizeo
TODO
TORCH
Tensor
The
This
TorchBind
TorchClass
Unpack
We
account
accounting
added
affine
allocate
an
and
appropriate
are
assert
astype
aten
axis
bag
bags
based
batch
batched
be
bias
bit
blob
but
byte
bytes
cast
channel
class
col
columns
const
containing
copy
cpp
cpu
create
currently
custom
de
depending
device
dim
dimension
dimensions
due
elem
embedding
empty
end
endif
entries
equal
exactly
expected
extend
false
fbgemm
first
floating
format
fp
from
full
has
have
helper
idx
ifdef
in
input
internally
its
kCPU
kFloat
kQUInt
last
lazy
m
matrix
memory
might
multiplying
n
native
nature
nbit
not
np
number
numpy
offset
only
operator
ops
options
origin
original
output
over
packed
parallel
parameters
params
per
performed
point
points
prepack
ptr
pytorch
qembeddingbag
qtensor
qtensors
quantization
quantized
quantizes
quint
random
rate
register
reinterpret
rest
result
row
rows
s
sample
scale
scales
set
shape
since
size
sizeof
squeeze
src
start
static
storage
store
sub
suggest
support
tensor
tensors
then
this
thus
toType
torch
u
un
unpack
unpacked
used
using
usize
value
values
vec
w
weight
weights
width
will
with
zero
zp

ATen
CHECK
Integer
PointwiseOps
Scalar
TORCH
Tensor
TensorIterator
TensorIteratorConfig
Ternary
The
add
addcdiv
addcmul
all
and
as
aten
b
backend
backward
be
behavior
build
can
checkBackend
const
cpp
cpu
declare
define
device
dispatch
division
double
dtypes
empty
false
future
h
higher
historic
huber
implementation
implemented
in
includeBool
input
integer
isIntegralType
iter
just
latter
lazy
longer
mse
native
no
operations
options
order
out
output
perform
pointwise
pytorch
release
result
scalar
smooth
src
static
stub
supported
tensor
torch
trunc
type
using
value
void
will
with

ATen
C
Compute
FFFFF
INT
MM
Multiplier
PABSD
PACKSSDW
PACKUSWB
PADDD
PADDQ
PADDW
PAND
PCMPGTD
PMAXUB
PMINUB
PMULUDQ
PSHUFD
PSRAD
PSRLQ
PSUBD
PSUBQ
PXOR
SHUFFLE
SHUFPS
Shift
Size
UINT
abs
add
adds
and
assert
aten
bits
c
castps
castsi
char
clamped
cmpgt
const
cpp
cpu
cvtsi
epi
epu
even
fp
ft
immediate
in
input
instructions
loadu
m
mask
max
min
mm
mul
multiplier
n
native
neg
none
odd
output
packed
packs
packus
parameters
point
product
ps
pytorch
qmax
qmin
qnnp
qnnpack
quantized
range
register
remainder
requantization
requantize
rev
rounded
rounding
scale
scaled
set
setzero
shift
short
shuffle
si
sra
src
srli
ssse
storeu
sub
total
u
vim
vmultiplier
vq
vqmax
vqmin
vremainder
vshift
vthreshold
vzero
w
xor
xy
xyzw
y
z
zero
zw

@zasdfgbnm
ALL
AND
AT
ATen
BFloat
Bool
COMPLEX
ComplexHalf
CopyKernel
DISPATCH
Half
If
Note
QINT
Scalar
ScalarType
See
TODO
TYPES
TensorIterator
The
This
Vectorized
We
Ws
You
`libtorch
able
above
actually
all
and
apply
as
aten
be
because
below
bit
blocking
call
can
cast
com
command
compile
compiler
complex
conditionals
conj
copy
correctly
cpp
cpu
datatypes
dest
dispatch
don
done
force
function
generate
github
grep
hand
have
here
https
impossible
in
inlining
instantiate
instantiation
instantiations
instead
inter
into
isComplexType
isQIntType
issues
iter
jump
just
kernel
levels
libtorch
like
little
looking
loop
majority
making
might
native
need
no
non
not
only
opaque
operations
optimizations
optimizer
out
output
patterns
per
physical
probably
pytorch
readelf
register
save
separate
should
simplified
single
size
so
so`
src
statement
static
stub
sure
symbols
table
tensor
their
these
this
treating
type
us
using
vec
vectorization
verify
we
with
worth
would
writing

?
A
ASSERT
AT
ATen
Activation
Add
Addition
B
Both
CHECK
DISPATCH
Else
FN
Got
IMPL
INTERNAL
If
In
LIBRARY
Let
NAME
Note
Only
PYTORCH
QEngine
QINT
QNNPACK
QnnpackOperatorDeleter
QuantizedCPU
RELU
ReLUFused
Reason
SELECTIVE
Scalar
TORCH
TYPES
Tensor
This
To
When
Xq
`quantized
`torch
activationLimits
add
add`
addition
adjust
affine
after
all
and
are
as
assumed
aten
b
backward
based
batch
be
both
broadcast
c
calculated
can
cases
cast
cfg
check
compatibility
const
contig
contiguous
copy
cpp
cpu
create
createStatus
define
deprecated
device
dims
dispatch
double
elems
empty
endif
failed
false
first
flags
flatten
following
format
from
functions
globalContext
have
highest
ifdef
implement
in
initQNNPACK
input
into
item
iterate
jit
kCPU
kPerTensorAffine
kPerTensorSymmetric
kQUInt
kept
kernel
layout
lazy
like
lowest
m
make
max
memory
merged
min
most
must
native
nc
ndimension
nearbyint
not
now
nullopt
nullptr
only
op
operands
operator
or
other
out
output
over
overhead
parameters
per
pin
point
prime
pthreadpool
ptr
pytorch
qEngine
qa
qadd
qb
qc
qnnp
qnnpack
qscheme
quantization
quantized
quantizer
quint
qy
relu
removed
representable
requantize
round
rules
runStatus
s
same
scalar
scale
second
set
setup
setupStatus
shceme
should
simply
size
space
src
static
status
stride
stub
success
suggest
sum
suported
supported
tensor
tensors
that
there
they
threadpool
toDouble
toFloat
trace
trace`
type
u
underlying
uniq
unique
use
usize
value
variations
we
will
z
zero

ATen
INTERNAL
PYTORCH
QNNP
aten
c
clamping
const
cpp
cpu
declare
export
fp
function
h
ident
k
macro
mr
name
native
neon
nr
params
psimd
pytorch
qnnp
qnnpack
quantized
rules
sgemm
src
stride
ukernel
usize
void
w

ANY
APIs
ASSERT
ATen
Add
Alias
BROKEN
BlobSetTensor
CPU
Caffe
CreateBlob
FALSE
FreeMemory
GPU
If
NOT
NetDef
OPENGL
PT
Resize
RunNetOnce
ShareExternalPointer
Sum
TEST
THROW
TRUE
Tensor
TensorImpl
Test
These
This
ToPytorch
UnsupportedDevice
We
Workspace
XBlobGetMutableTensor
access
actually
add
allocated
always
another
are
around
aten
away
b
backend
buf
buffer
but
c
caffe
calling
can
caught
change
checking
contiguous
cpp
crazy
d
defined
direction
do
doesn
empty
ensure
every
everywhere
expensive
external
externally
fine
from
have
however
in
initialized
inplace
input
interop
item
its
kCPU
kFloat
kLong
kSparse
larger
layout
legacy
meantime
mutual
net
no
non
not
nullptr
ones
op
or
output
partially
pass
permute
pod
preserve
preserved
ptr
pytorch
read
regular
rely
resizable
resize
resizes
result
semantics
set
shared
should
simple
situation
sizeof
smaller
soon
sparse
src
still
storage
strided
stuff
sum
tensor
tensors
test
that
they
type
user
using
view
visible
we
workspace
write
zeros

?
@note
@private
A
APIs
ASSERT
AT
ATen
Accessors
An
AnyType
Apply
Arc
Args
Argument
ArrayRef
As
Because
Blob
Bool
BoolTensor
BoolType
BufWriter
C
CASE
CHECK
CU
Callers
Calling
Can
Cannot
Capsule
CapsuleType
Chechs
Checks
ClassType
ClassTypePtr
Classes
CompAliasedIValues
Complex
ComplexDouble
ComplexHolder
ComplexType
ConstantString
Ctx
Custom
CustomClassHolder
DEFINE
DSO
DataPtr
DataPtrs
Default
Detect
Device
DeviceDescriptor
DeviceIndex
DeviceObjType
DeviceType
Dict
DictType
Different
Dimname
Display
Double
Drop
ERROR
Enum
EnumHolder
Equality
Error
Expected
FORALL
FP
Fall
False
First
FlatHashMap
FloatType
For
Formatter
Future
FutureType
Generator
GeneratorType
GenericDict
GenericList
HashAliasedIValue
HashAliasedIValueMap
HashAliasedIValues
HashMap
HashSet
Hashing
INTERNAL
IValue
IValueComparator
IValueFormatter
IValuePayload
IValuePayloadTriviallyCopyablePayload
IValueTag
IValues
Identity
If
In
Initializing
Inserts
Instead
Int
IntType
Interpreter
IntrusivePtr
IntrusivePtrTarget
IntrusiveTargetDefaultNullType
InvalidTag
Invariant
It
Its
Just
Layout
Leaving
Like
List
ListType
MKLDNN
Make
Mappings
MemoryFormat
Mimic
NB
NORMAL
NOTE
NaN
Needs
No
None
NoneType
Not
Note
NullType
Numerical
Object
Once
Option
Optional
OptionalArray
Other
Outlined
PR
PartialEq
Payload
Payload`
Perhaps
Please
Prefer
Primitive
Properly
PyObject
PyObjectHolder
PyObjectType
PyTorch
Python
QScheme
Quantizer
QuantizerType
Quick
RRef
RRefInterface
RRefType
Reaching
Record
ReferenceWrapper
Result
Returns
RuntimeError
SNIFAEs
Scalar
ScalarType
See
Self
Sequence
Set
Shortcut
Since
Some
Special
Specifically
Storage
StorageType
Stream
StreamObjType
String
StringStream
StringType
StringView
Strings
StrongTypePtr
Symbol
TAGS
TODO
TORCH
TYPE
Tag
Tensor
TensorImpl
TensorType
Tensors
That
The
There
These
This
Thus
To
ToOptional
TorchCustomClassHolder
TorchIValue
TorchJitCompilationUnit
TorchJitFunction
TorchJitStack
TorchJitpop
TorchScript
TorchTensor
Tried
Trivially
TriviallyCopyablePayload
True
Tuple
TupleType
TupleTypePtr
Type
TypeIndex
TypePtr
UB
Undefined
UndefinedTensorImpl
Uninitialized
Union
Unitialized
Unknown
Unwrap
Use
Used
VALUE
Value
We
WeakIValue
When
While
Why
ZERO
\
\endrst
\n
\rst
`
`@private`
`BoolTensor`
`Device`
`IValue
`Tensor`
`UndefinedTensorImpl
`X`
`bool
`bool`
`customFormatter`
`double`
`hash
`hash`
`i
`id
`intrusive
`isX`
`is`
`my
`nullptr`
`repr
`rhs`
`tensor
`this`
`torch
above
abs
absolutely
abstraction
abundantly
accepts
access
accessors
across
actual
actually
addCallback
after
alias
aliased
aliasing
alive
all
allow
also
always
ambiguity
ambiguous
an
and
annotate
annotated
annotation
another
any
anything
apparently
are
arg
args
argument
arguments
arises
arranged
as
asIvalue
assign
assume
assumes
aten
atomic
attempt
attr
attribute
attributes
autograd
automatically
avoid
b
back
backwards
base
based
be
because
becomes
begin
behavior
below
big
bits
blob
block
bool?
boolean
both
bound
boundaries
boxed
boxing
break
built
but
byte
bytes
c
cache
call
called
caller
calls
callsites
can
cannot
capsule
carefully
case
cast
castRaw
caster
casting
cbegin
cend
changed
changes
check
checkObjectSortSchema
checks
choose
clang
clarity
class
classConverter
classes
classes?
clear
clearToNone
clients
clone
collect
com
come
comment
commented
common
comparable
comparator
compare
compared
comparing
comparison
compilation
compiler
complete
completed
complex
compute
consider
consistency
const
constValue
constant
constants
construct
constructible
constructor
constructors
contain
contained
container
containers
contains
continue
convert
converted
converter
copied
copy
copyable
core
correct
correctness
costly
could
count
cout
cpp
create
ctx
cu
currently
custom
customFormatter
customizing
d
dataPtrs
debugging
declared
decltype
decref
deepcopy
def
default
define
defined
defines
definition
depends
derive
destroy
destructor
detail
determine
determining
device
devices
dict
different
differently
difficult
digits
dimname
directly
disambiguate
disjunction
dispatches
do
document
documentation
does
doesn
doing
don
done
dont
double
doxygen
drop
dst
dstFuture
due
dump
duplicated
eager
early
easy
effectively
efficiency
efficient
either
elem
element
elementType
elementTypeCanBeInferredFromMembers
elements
elems
emplace
empty
emulate
enable
encode
encoded
encountered
end
endif
ends
entire
entry
enum
environment
eq
equal
equality
equals
equivalent
erased
err
error
ever
every
example
exception
exchange
exhaustive
exists
exit
expand
expect
expectRef
expected
explanatory
explicit
export
expression
extract
extractTensors
fact
fails
false
far
fast
fastEqualsForContainer
figure
file
find
findMethod
fine
finish
first
fit
fix
flat
fmap
fmt
follow
following
forall
format
formatSetOfDevices
formatter
former
found
fpclassify
fragile
from
fromQualString
fromSymbol
frozen
full
func
function
functions
fut
future
futures
generate
generated
generator
generic
getAttr
getAttributeSlot
getAttributes
getCustomClassTypeImpl
getCustomClassTypeMap
getElementType
getLessThanComparator
getMethod
getName
getSchema
getSlot
getSubValues
getValueType
gets
getstate
getting
github
got
gotten
greater
gt
guaranteed
h
handling
has
hasError
hasMethod
hash
hashed
have
hence
here
hides
hold
holder
holding
holds
hot
how
https
id
ident
identity
imag
implementation
implementations
implicitly
implies
in
including
incref
index
indirect
infer
inlined
input
insert
inspect
instead
intentionally
internal
internalToPointer
interpreter
introduced
intrusive
invoke
io
ip
irange
isAliasOf
isBool
isBoolean
isComplex
isComplexDouble
isDevice
isDouble
isFloatingPoint
isGenericDict
isInt
isIntegral
isList
isNone
isObject
isPtrType
isPyObject
isStorage
isStream
isString
isTensor
isTuple
isUndefinedTensor
issues
iterator
its
itself
iv
ivalue
ivalue`
ivalues
j
jit
jump
just
keep
key
keyType
kind
kwarg
latter
layout
lazy
least
less
lexicographically
lhs
like
limits
list
listed
literals
lock
long
look
looking
lookup
loudly
lt
lts
lvalue
m
macro
make
making
manually
map
mark
markCompleted
marked
match
max
may
maybe
mean
means
member
memo
memory
merge
meta
method
methods
mkldnn
module
more
move
moveFrom
moved
must
my
n
name
named
necessarily
necessary
need
needs
negation
neither
nested
never
new
newPayload
no
non
none
nonzero
nor
not
notes
null
nullopt
nullptr
numAttributes
numbers
obj
object
objects
official
omitting
one
only
op
operation
operations
operator
ops
optimization
optimize
optional
or
order
ordering
orig
oss
ostream
ostringstream
other
our
out
outputs
over
overlaps
override
owning
pack
page
pair
pairs
parsed
particular
pass
path
paths
payload
payloads
perf
performance
pickle
pickling
pl
places
pointer
pointers
poorly
pop
position
possible
prec
precision
prefer
primarily
primitive
print
printComplex
printDict
printList
printMaybeAnnotatedDict
printMaybeAnnotatedList
printQuotedString
printing
prints
private
produces
programming
prove
provided
provokes
ptr
ptrEqual
ptr`
ptrs
purpose
push
py
pybind
python
pytorch
qscheme
qualified
qualifiedClassName
qualifiedName
qualname
quantizer
quirks
raw
re
reach
real
really
reason
reasons
reclaim
record
recreate
recurse
recursion
recursively
ref
refcounted
reference
refs
register
regular
reinterpret
related
release
remaining
remove
report
reporting
repr
represent
representation
represented
reproduce
requirement
requires
res
reserve
reset
resize
result
retain
retrieve
returning
returns
revisit
rhs
rhsSubValues
rhsTensor
right
rref
rules
runtime
s
same
satisfied
satisfy
saved
saving
scalar
scalars
schema
second
see
semantics
separate
sequence
serialization
serialize
serializer
serve
set
setError
setErrorIfNeeded
setSlot
setprecision
sets
setstate
several
shallow
shared
should
show
sign
signature
signbit
signed
silences
silent
simple
simpleClassTypeArg
simpler
since
singleton
size
ska
slight
slot
slots
so
some
something
sort
special
src
srcFutures
srcs
stack
standalone
standard
start
state
statement
static
stay
stops
storage
store
str
stream
strict
strictly
strings
stringstream
sub
subValues
subclasses
subtype
subtypes
subvalue
subvalues
success
sufficient
support
supported
supposed
sure
surprisingly
swap
switch
symbol
system
table
tag
tagKind
tagged
tags
take
takes
tell
temp
templated
templates
temporary
tensor
tensors
test
than
that
their
them
themselves
then
there
these
they
thing
this
thisSubValues
thisTensor
through
throw
tmap
toBlob
toBool
toComplexDouble
toDevice
toDouble
toEnumHolder
toFuture
toGenericDict
toIValue
toInt
toList
toListRef
toLong
toNone
toObject
toObjectRef
toPyObject
toPyObjectHolder
toQualString
toRRef
toStorage
toStr
toStream
toStringRef
toTensor
toTuple
toX
together
too
torch
treat
trivially
try
tryToInferType
trying
tuple
tuples
ty
type
typePtr
typeid
typename
types
u
undef
undefined
underlying
understand
unhandled
unhashable
uninitialised
uninitialized
union
unit
unless
unordered
unqualified
unqualifiedClassName
unregistered
unsafe
unsafeGetStorageImpl
unsafeGetTensorImpl
unsafeReleaseGeneratorImpl
unsafeReleaseStorageImpl
unsafeRemoveSlot
unsafeToTensorImpl
unsupported
until
unwrap
up
use
used
useful
user
using
usize
usual
v
val
valgrind
value
valueType
values
ve
versa
via
vice
view
views
visible
visit
visitor
void
vs
want
warn
warnings
way
we
weak
what
whatever
whereas
whether
which
while
why
will
win
wise
with
within
without
won
work
works
would
wrapper
write
wrong
you
zero
~Tensor

ATen
PyTorchQnnpConvQuantizationParams
aarch
aligned
assume
aten
builtin
c
channel
clamped
const
cpp
cpu
dup
endif
ft
gemm
high
ifdef
index
input
k
kernel
lane
low
max
min
mr
n
native
neon
none
nr
output
params
point
points
predecrement
pytorch
qnnpack
quantization
quantized
requantization
s
scale
scales
shift
src
stride
sub
u
uint
uintptr
ukernel
usize
va
vacc
vaddq
vb
vcombine
vcvtnq
vcvtq
vdupq
vextq
vfmagic
vfmax
vfmin
vget
vim
vimagic
vld
vmaxq
vminq
vmlal
vmov
vmulq
void
vout
voutput
vqaddq
vqmovn
vqmovun
vreinterpret
vreinterpretq
vshl
vst
vsubl
vsubq
vxa
vxb
w
zero

@note
ADInplaceOrView
ATen
Autograd
AutogradCPU
AutogradCUDA
AutogradMLC
AutogradOther
AutogradXLA
AutogradXPU
But
Function
IMPL
If
It
LIBRARY
NB
Note
Register
Since
TODO
TORCH
This
TorchCppFunction
VariableFallbackKernel
also
always
and
aten
autograd
backends
be
but
call
called
com
core
correct
cpp
custom
deleted
described
dispatch
dispatcher
do
don
extension
fallback
fallthrough
file
from
functions
github
have
how
https
implementation
implements
in
instead
into
issues
just
kernel
kernels
key
keys
knows
lazy
m
makeFallthrough
maybe
mechanism
native
need
never
not
ones
operator
operators
ops
override
own
private
pytorch
register
registered
replaced
see
set
should
so
src
static
tensors
that
their
them
themselves
this
use
usually
variable
want
wants
whole
will
with
work
yaml
you

?
ALL
AND
AT
ATen
CHECK
COMPLEX
CPU
DISPATCH
GPU
GRAIN
Not
Note
ONCE
Option
RangeFactories
SIZE
Scalar
TORCH
TYPES
Tensor
TensorIterator
The
This
WARN
acc
account
accscalar
across
and
anyway
appear
arange
as
aten
autopromoted
base
be
because
begin
borrowing
bound
but
case
cast
ceil
complex
compute
computed
consistency
const
contiguous
copy
corner
cpp
cpu
d
declare
define
deprecated
device
devices
differing
dispatch
do
does
dont
double
effective
elements
end
error
false
fill
future
gpu
halfway
has
higher
in
inconsistent
internal
into
invalid
isComplexType
isfinite
issues
iter
kBFloat
larger
linspace
logspace
match
max
may
might
must
native
negative
non
nonzero
not
nullary
number
occur
once
only
op
operator
optional
or
out
output
overflow?
parallel
per
possible
pow
precision
problem
process
providing
ptr
pytorch
r
range
release
resize
resized
result
rounding
runtime
s
same
scalar
shape
sign
size
skip
src
start
starts
static
step
steps
stub
support
take
tensor
than
that
this
throw
type
unsupported
upper
use
using
value
vs
want
warning
we
which
will
with
xend
xstart
xstep

ATen
INTERNAL
PYTORCH
QNNP
aten
buffer
channels
clamping
const
cpp
cpu
declare
export
fp
function
h
ident
increment
input
macro
name
native
output
params
psimd
pytorch
qnnp
qnnpack
quantized
rules
sdwconv
smpdwconv
src
stride
supdwconv
u
ukernel
up
usize
void
weights
width

ATen
Apply
ApplyDynamic
ApplyDynamicRelu
ApplyRelu
Bias
LinearPackedParamsBase
LinearPackedParamsBaseInterface
LinearPackedSerializationType
Option
Self
SetBias
Tensor
TorchJitCustomClassHolder
Unpack
Weight
ao
apply
aten
base
bias
block
cpp
cpu
dynamic
error
features
h
implemented
in
input
native
new
not
out
output
packed
parameter
params
point
pytorch
quantized
relu
runtime
scale
set
size
sparse
src
this
throw
trait
type
unpack
zero

ATen
C
MM
PABSD
PACKSSDW
PACKUSWB
PADDQ
PADDW
PBLENDW
PMAXUB
PMINUB
PMULUDQ
PSHUFD
PSIGND
PSRLD
PSRLQ
SHUFFLE
Size
UINT
abs
absmul
add
adds
assert
aten
bits
blend
c
char
clamped
const
cpp
cpu
cvtsi
epi
epu
fp
ft
input
instructions
loadu
m
max
min
mm
mul
multiplier
n
native
none
output
packed
packs
packus
point
precise
pytorch
qmax
qmin
qnnp
qnnpack
quantized
requantization
requantize
rounding
scale
scaled
set
shift
short
shuffle
si
sign
src
srl
sse
storeu
total
u
vim
vmultiplier
vqmax
vqmin
vrounding
vshifthi
vshiftlo
vzero
w
xCC
xy
xyzw
y
z
zero
zw

?
A
ASSERT
AT
ATen
B
Box
C
CHECK
ChannelsLast
Col
Contiguous
Conv
ConvPackedParamsBase
ConvParamsSerializationType
Convert
ConvertToChannelsLast
CopyICFirst
CopyToChannelsLast
Currently
D
DHW
DIM
DISPATCH
DispatchKey
DispatchKeySet
Element
EmbeddingPackedParams
EmbeddingPackedParamsBase
EmbeddingParamsSerializationType
Expected
FBGEMM
FbgemmConvParam
FbgemmPackBMatrix
FbgemmPackWeightsForConv
FbgemmPackedGemmMatrixFP
G
Got
H
HW
IC
INT
INTERNAL
IValue
IntrusivePtr
IsChannelsLast
JIT
K
KH
KW
LinearPackedParams
LinearPackedParamsBase
M
MakeStridedQTensorCPU
MemoryFormat
NB
Note
OC
Option
PYTORCH
PackAWithIm
PackAWithQuantRowOffset
PackAWithRowOffset
PackBMatrix
PackWeight
PackedConvWeight
PackedEmbeddingBagWeight
PackedLinearWeight
PackedLinearWeightFp
PackedLinearWeightsQnnp
PackedParam
Param
QEngine
QINT
QNNPACK
QScheme
QTensorImpl
QuantizedCPU
QuantizerPtr
ReluFused
Remove
SPATIAL
Scalar
ScalarType
Self
SerializationType
SpatialDim
StorageImpl
THW
TODO
TORCH
TYPES
Tensor
TensorOptions
Tensors
The
TorchClass
TorchList
Torchclass
Type
Unknown
UnpackWeight
Unsupported
W
act
affine
allocate
allocator
and
any
apply
are
as
asymmetric
aten
axis
base
batch
be
because
begin
below
bias
bit
both
byte
bytes
call
can
capacity
cast
cat
cfg
channel
channels
check
chunk
col
column
columns
compressed
computations
connect
const
contained
contig
contiguous
conv
conversion
convert
copy
corresponding
cpp
cpu
created
currently
d
dPackedParamsBase
dTensor
dTensorToChannelsLast
def
deserialize
device
dilation
dilations
dim
double
doubles
dst
dynamic
embedding
embeddingbag
empty
endif
false
fbgemm
feature
fp
from
fully
functions
fuse
fused
getCPUAllocator
getstate
globalContext
groups
h
historical
hw
ic
icf
ifdef
image
in
include
indices
inference
inner
input
instance
integers
intrusive
irst
isQIntType
itemsize
j
k
kFloat
kQInt
kSpatialDim
kernel
kernels
last
layer
lazy
len
let
linear
logically
longs
make
map
mapping
matrix
mode
move
multiplier
multiply
native
needed
nelements
new
nonnegative
not
object
oc
offset
offsets
only
op
optional
options
or
out
output
packed
padding
paddings
pads
param
params
parse
parsing
per
permute
pickle
point
points
prepack
prepacking
prepared
pruned
ptr
pytorch
qEngine
qengine
qtensor
qtensorimpl
quantization
quantized
quantizer
range
rate
ready
reduce
register
relu
resizable
row
rows
sample
save
scalar
scale
scales
scheme
serialization
serialize
serialized
set
setstate
shape
size
src
state
static
step
storage
stride
strided
strides
structure
sum
support
supported
supports
takes
tensor
tensors
term
that
think
tie
times
toString
transform
transpose
transposed
tuple
type
typeMetaToScalarType
u
uint
unpack
unsqueeze
unwrap
use
used
using
usize
utils
v
values
version
versions
w
way
we
weight
weights
well
whereas
which
width
with
zero
zp

ATen
LIKELY
Multiply
PYTORCH
Pack
PyTorchQnnpAddQuantizationParams
QNNP
Shift
Subtract
aarch
accumulate
add
aligned
and
assume
aten
b
builtin
c
const
cpp
cpu
dup
endif
factors
ft
hi
high
ifdef
increment
lane
lo
low
mask
max
min
multiplier
n
native
neon
none
output
params
point
prefetch
products
pytorch
qnnpack
quantization
quantized
right
round
s
saturate
shift
src
u
uint
ukernel
usize
va
vacc
vadd
vb
vbic
vbicq
vceqq
vcombine
vext
vget
vim
vld
vmax
vmaxq
vmin
vminq
vmla
vmlaq
vmov
vmovl
vmovq
vmul
vmulq
vqadd
vqaddq
vqmovn
vqmovun
vreinterpret
vreinterpretq
vright
vrshl
vrshlq
vshl
vsra
vsraq
vst
vsubl
vxa
vxb
vy
vzero
y
zero

API
ATen
Utils
VULKAN
aten
cfg
cpp
dimension
h
n
native
normalize
ops
pytorch
src
vulkan

?
@note
ALL
AND
API
ASSERT
AT
ATen
As
Assume
At
Bool
Byte
CHECK
COMPLEX
CPU
CUDA
Can
Cannot
Computation
Contiguous
Convert
CumFn
DISPATCH
Denote
Dimname
DimnameList
Doing
Expected
F
FLOATING
False
Finally
For
From
GRAIN
Got
GradMode
Helper
However
INDEX
INFINITY
INTERNAL
If
In
IntArrayRef
It
Layout
Logically
Looking
MaybeOwned
MemoryFormat
Moreover
NOTE
NaN
NoNamesGuard
Normalization
Note
NumPy
O
Once
Operation
Option
Please
R
REDUCE
Reasoning
ReduceFn
ReduceFnFlag
ReduceNormFn
ReduceOps
ReduceStdVarFunction
Refer
Reference
Returns
SIZE
Scalar
ScalarType
See
Similar
So
Sparse
Strided
TODO
TORCH
TYPES
Tensor
TensorIterator
TensorIteratorConfig
TensorList
TensorOptions
The
This
Thus
To
Trivial
True
Unsupported
Used
We
When
With
You
`
`Nan`
`all`
`any`
`bool`
`indices`
`input
`input`
`make
`output`
`uint
abitrary
able
abs
acc
accessed
account
accounts
accscalar
actual
add
after
again
all
allow
along
also
always
amax
amin
an
and
any
append
appending
application
apply
are
argmax
argmin
argument
arguments
as
assert
assume
assumed
assuming
aten
atomic
attempting
avoid
ax
axis
b
back
backward
backwards
basic
be
because
before
begin
below
bit
bitset
borrowed
build
but
c
calculate
calculated
calculates
call
called
can
cannot
case
cases
cast
casting
casts
cat
center
chain
char
check
checked
checks
com
combine
common
compare
compatibility
compatible
complex
components
comptability
computation
compute
computing
conj
conj`
conjugate
conjugating
const
constexpr
convert
coordinates
copy
copying
correction
cost
could
cpp
cpu
create
cu
cuda
cum
cummax
cummaxmin
cummin
cumprod
cumsum
cumulative
curr
currently
custom
d
dF
declare
deduce
deduced
define
defined
depend
deriv
derivative
derive
device
devices
diag
diff
differences
differencing
different
dim
dimension
dimensional
dimensions
dimname
dimnames
dimreduce
dims
direction
directions
discussion
dispatch
dist
div
divide
do
does
doesn
don
done
double
dtypes
dx
dy
dynamic
easilly
easy
edge
efficient
either
elem
element
elements
emplace
empty
enable
enabled
end
equal
equiv
erase
every
everything
examples
except
existing
exists
exp
expand
expected
expects
explicitly
express
extra
failing
fallback
false
faster
favored
feature
file
fill
finally
find
first
fixed
flag
flip
floating
fname
following
follows
formula
formulas
forward
from
function
functions
gather
github
given
got
gpu
grad
gradient
greater
guard
gymnastics
h
handles
has
have
helper
here
higher
holomorphic
how
however
https
idx
ignore
imag
imaginary
implement
implementation
implemented
implements
implies
imputs
in
include
includeBool
index
indexed
indicator
indices
infinity
inner
input
inside
instead
integer
integers
integral
interchangeable
interested
internal
intersection
ints
invalid
iota
irange
isComplexType
isFloatingType
isIntegralType
isnan
issuecomment
issues
item
iter
itr
its
j
just
k
kBFloat
kBool
kCPU
kDouble
kFloat
kHalf
kLong
keepdim
keepdims
kernel
labels
last
later
layout
leads
least
leave
len
length
less
lieu
like
likely
limits
line
list
load
local
locate
locations
log
logcumsumexp
logic
logical
logsumexp
look
looking
make
makes
map
mask
masked
masks
match
matrix
max
maxes
may
maybe
md
mean
means
min
mode
more
much
mul
multiple
must
n
named
namedinference
names
nansum
narrow
native
ndimension
ndims
necessary
need
negative
new
new?assignees
no
non
nonzero
norm
not
noting
now
nullopt
number
object
observations
omit
omitted
one
ones
only
op
operation
operator
operators
opt
optional
options
opts
or
order
orders
other
otherwise
out
outer
output
ouw
over
overall
overload
owned
parallel
parameters
perf
performance
pkus
play
please
plus
point
poor
port
position
positions
pre
precision
prepend
prepending
preprocess
prerequisites
present
prevents
previous
probably
problems
processed
prod
prods
product
production
products
promote
promoted
propagate
propagates
properly
provided
ptr
pull
python
pytorch
quiet
range
real
reason
reduce
reduction
reduction`
referred
regressed
reinterpret
relu
replacing
reportNYIDimnameOverload
request
requires
reshape
resize
result
reuse
reversed
right
row
rule
s
same
satisfies
scalar
scalarType
scalars
scatter
second
see
seen
select
selecting
sense
seperately
serial
set
shape
show
significantly
simplicity
simplifies
simply
since
size
slice
so
some
spacing
sparse
specific
specified
sqrt
squeeze
squeezed
src
start
static
stride
strided
strides
stub
sub
subtlety
such
sum
support
supporting
supports
sure
take
taken
tensor
tensorial
tensors
term
terms
than
that
their
then
there
therefore
these
they
this
those
though
thread
three
toString
toType
toValueType
together
topk
torch
trace
tried
trivial
tuple
type
typeMetaToScalarType
types
u
ubsan
uint
unbiased
undefined
unit
unsafeGetTensorImpl
unspecified
unsqueeze
until
unvectorized
upcast
use
used
using
usual
valid
value
values
var
variance
vec
vectorize
very
via
w
was
way
we
whether
which
while
will
with
without
workloads
would
wrap
wrapped
xor
y
your
z
zero
zeros
~first
~mask
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ATen
MetalTensorUtils
Tensor
aten
batch
channels
cpp
h
height
metal
native
pytorch
size
src
tensor
u
width

A
ATen
B
NULL
PRIu
PyTorchQnnpOperator
PyTorchQnnpStatus
QNNPACK
Size
add
allocate
and
aten
b
batch
be
because
below
bytes
c
calloc
channels
compute
const
cpp
cpu
create
delete
enum
error
failed
finite
flags
format
goto
in
initialized
input
invalid
isnormal
log
max
memory
min
must
native
nc
non
not
number
op
operator
out
output
parameter
params
pixel
point
positive
properly
pytorch
qnnp
qnnpack
quantization
quantized
quint
range
ratio
scale
setup
size
sizeof
src
status
stride
structure
success
sum
type
u
ukernel
uninitialized
unsupported
with
zero
zu

ARM
ATen
CHECK
CPU
Device
DispatchKey
DispatchKeySet
MPSCNNContext
MTLBuffer
MTLResourceOptionCPUCacheModeWriteCombined
Metal
MetalCommandBuffer
MetalTensorImpl
MetalTensorImplStorage
MetalUtils
NCHW
NEON
TORCH
Tensor
TensorImpl
TensorOptions
Thus
When
any
aten
back
based
becomes
begin
buffer
bytes
cast
cfg
cmdBuffer
command
commandBuffer
compute
const
contents
contiguous
copying
cpp
currentBuffer
device
dim
end
format
fp
from
getTensorImplStorage
h
handle
id
idx
implStorage
in
kMetal
last
make
max
memcpy
memory
metal
move
mt
mtl
native
nc
nchw
newBufferWithLength
not
opaque
options
pytorch
result
sharedInstance
size
sizeof
src
static
storage
stored
strides
tensor
texture
tonc
tonchw
type
u
unsafe
unsafeGetTensorImpl
using
valid
we

ASSERT
ATen
Auto
Device
Float
INT
Integer
MAX
MIN
RNG
S
ScalarType
TRUE
TorchTensorOptions
Torchallclose
Torchempty
Torchfull
TorchkInt
ULL
VAL
actual
aten
bit
case
cast
const
constexpr
covered
cpp
device
digits
double
exp
expected
false
floating
from
froms
full
gen
generator
h
has
like
limits
lowest
make
max
min
nullopt
optional
point
pytorch
random
range
rng
same
signed
src
static
test
toType
tos
u
uint
val
vals
value
with

ATen
PyTorchQnnpAvgPoolQuantizationParams
aarch
address
aligned
assert
assume
aten
avgpool
bias
builtin
c
const
cpp
cpu
defined
do
dup
endif
ft
hi
high
increment
input
k
kc
ks
lane
lo
low
max
min
n
native
neon
none
output
params
point
pytorch
qnnpack
quantization
quantized
s
scale
src
u
uint
uintptr
ukernel
up
usize
vacc
vaddl
vaddq
vaddw
vbias
vcombine
vcvtnq
vcvtq
vdupq
vext
vfmagic
vfmax
vfmin
vget
vi
vim
vimagic
vld
vmax
vmaxq
vmin
vminq
vmov
vmulq
vout
voutput
vqaddq
vqmovn
vqmovun
vreinterpret
vreinterpretq
vscale
vshift
vshl
vst
vsubq
vsum
while
zero

ALL
AND
AT
ATen
BFloat
Bool
COMPLEX
DISPATCH
FunctionOfAMatrixUtilsKernel
Half
RESTRICT
Scalar
ScalarType
TYPES
TensorIterator
aten
cast
char
coeff
combination
compute
const
cpp
cpu
dispatch
elem
in
iter
kernel
linear
loop
n
native
out
perform
primitive
ptr
pytorch
register
reinterpret
scalar
src
stride
strides
stub
summation
summations
type
typename
using
value

ATen
C
Compute
FFFFF
Size
UINT
VQRDMUL
adds
assert
aten
bits
c
char
clamped
const
cpp
cpu
doubling
epi
epu
exponent
fp
ft
gemmlowp
high
in
input
loadu
m
max
min
mm
multiplier
n
native
none
output
packed
packs
packus
parameters
point
product
pytorch
qmax
qmin
qnnp
qnnpack
quantized
rdivbypo
requantization
requantize
s
scale
scaled
set
shift
short
si
src
sse
storeu
u
using
vim
vmultiplier
vqmax
vqmin
vqrdmulh
vzero
w
xy
xyzw
y
z
zero
zw

AT
ATen
Arc
C
CHECK
CONSUMED
CREATOR
Create
EXPERIMENTAL
Error
Expected
Factory
For
NATIVE
NOT
Number
OPENMP
PARALLEL
POOL
PTThreadPool
ParallelThreadPoolNative
REGISTER
SET
SINGLE
TBB
THREAD
TORCH
TaskThreadPoolBase
ThreadLocalState
ThreadLocalStateGuard
ThreadPoolRegistry
accepted
after
and
any
aten
atomic
bind
c
called
cannot
cfg
compare
const
cpp
create
default
defaultNumThreads
device
endif
exchange
func
function
global
guard
has
hidden
id
initialized
instance
inter
interface
internal
interop
intraop
launch
lazy
load
locals
make
move
new
no
now
nthreads
number
only
op
or
parallel
pool
positive
ptr
pytorch
set
shared
should
size
src
started
state
static
strong
thread
threadpool
threads
use
user
users
value
void
work

ARM
ATen
CLAMP
ClampMicrokernelTester
NEON
REQUIRES
SSE
TEST
U
aarch
any
arch
arm
aten
cc
cfg
clamp
cpp
cpu
div
eq
gt
inplace
iterations
lt
mod
n
native
neon
pytorch
qmax
qmin
qnnpack
quantized
src
sse
super
test
u
ukernel
use
usize

AT
ATen
CHECK
ENABLED
MKLDNN
TORCH
Tensor
UnaryOps
algorithm
aten
cfg
compiled
compute
cpp
device
eltwise
false
forward
from
ideep
itensor
kind
logistic
mkldnn
mod
move
native
new
not
opt
optTypeMetaToScalarType
options
prop
pytorch
sigmoid
src
super
support
tanh
tensor
use
with
y

ATen
ClampingParams
PyTorchQnnpU
address
assert
aten
c
const
cpp
cpu
do
dup
ft
increment
input
k
kc
ks
m
max
maxpool
min
n
native
neon
none
o
output
params
ptrdiff
pytorch
qnnpack
quantized
src
u
uint
uintptr
ukernel
usize
vi
vim
vld
vmax
vmaxq
vminq
vo
vout
voutput
vst
while

ATen
FuncType
In
OperatorKernel
ParameterList
Parameters
ReturnType
This
WrapFunctionIntoRuntimeFunctor
Wraps
an
any
args
as
aten
be
boxing
c
calling
can
case
class
core
cpp
decltype
detail
even
example
explicit
final
forward
from
func
function
functor
functors
guts
h
infer
inherits
into
invoked
kernel
lambdas
lazy
operator
or
overhead
parameter
pointer
pointers
private
public
pytorch
runtime
since
so
src
static
that
there
traits
type
typelist
typename
types
used
using
whenever

ATen
AdaptiveAveragePooling
C
CHECK
ChannelsLast
D
MOBILE
MemoryFormat
TORCH
Tensor
XNNPACK
`grad
`output`
adaptive
as
assert
aten
average
backward
batch
be
being
but
c
can
case
channels
computing
const
cpp
cpu
d
define
defined
dimension
dimensions
dispatch
doesn
done
efficiently
empty
endif
expected
format
give
global
got
grad
has
have
height
hw
in
input
input`
just
kCPU
keepdim
kernel
last
mean
memory
mkldnn
mode
more
must
n
native
nbatch
ndim
ndimension
non
options
or
out
output
output`
over
pool
pooling
pytorch
quantized
resize
since
size
spatial
src
strided
suggest
tensor
this
use
which
width
with
xnnpack
zero

ATen
CHECK
COO
CSR
Cannot
D
DeviceType
DispatchKey
DispatchKeySet
ID
If
Int
Invariants
It
MAGMA
MKL
SPMM
SPMV
ScalarType
Self
Since
SparseCsrCPU
SparseCsrCUDA
SparseCsrTensor
SparseCsrTensorImpl
SparseCsrTensorSetToDeviceType
Struct
TORCH
Tensor
TensorImpl
The
Type
TypeMeta
`
`col
`crow
`values
advantage
alright
an
and
any
are
as
aten
base
be
bit
calling
can
care
col
column
compressed
computation
construct
cpp
crow
csr
currently
denoting
device
empty
explicitly
facilitate
false
format
h
has
implementing
important
indexing
indices
indices`
initialTensorOptions
integer
interface
interfacing
kCPU
kCUDA
key
libraries
like
main
make
match
member
memory
move
must
new
nnz
non
not
optimized
options
or
over
pytorch
refresh
represents
resize
routines
row
scalar
set
shape
should
size
smooth
sparse
speed
src
stores
strides
structures
such
suggest
supported
sure
taken
tensor
tensors
that
these
three
type
typeMetaToScalarType
use
uses
value
values
with

ATen
During
Note
Tensor
TensorIterator
TensorIteratorConfig
Tensors
This
UnfoldBackward
UnfoldBackwardFn
`dim`
add
all
an
and
arange
as
aten
back
backward
because
broadcast
build
check
cpp
declare
define
determined
device
dictates
dim
dimension
dispatch
does
element
elements
ensure
false
folds
ft
grad
gradient
h
have
idx
in
indexed
information
input
inside
iter
iterate
iteration
just
kLong
kernel
know
last
make
maybe
mean
mem
min
n
naming
native
none
nonempty
not
number
options
out
output
outputs
over
overlap
owned
pop
prepare
pytorch
resize
restrided
same
set
size
squeeze
src
step
stored
stores
strided
strides
stub
that
this
type
unconventional
unfold
unsqueeze
vec
vim
we
will
wrap
wrt
zeros

?
ATen
All
C
Convs
Could
D
Failed
H
INTRA
If
LOG
MOBILE
Make
MaybeOwned
Mismatch
Mismatched
NNPACK
NNPack
Need
Note
OP
Option
Our
Out
PARALLEL
Reason
Run
Running
See
Setup
Some
SpatialConvolution
Tensor
Tensors
ThreadPool
Try
Unknown
Unsupported
Use
W
WARNING
Windows
Won
accGradWeight
activation
algorithm
alignment
allocate
and
aten
available
backward
basic
batch
be
bias
borrow
bottom
boundary
buffer
but
call
called
cases
cfg
channels
checking
comprehensive
compute
concurrency
const
constexpr
contiguous
conv
convolution
convolutionOutput
cpp
create
created
deallocate
default
defined
device
doesn
don
either
empty
endif
equals
err
error
existing
expects
failed
false
feature
filled
flag
form
free
from
grad
gradInput
gradOutput
gradWeight
gradient
hacky
hardware
has
have
height
iC
identity
ifdef
in
inference
init
initialize
initialized
input
insufficient
kCPU
kFloat
kH
kW
kernel
lazy
left
local
mask
maybe
memalign
memory
mode
more
multiple
must
native
ndimension
need
newly
nnp
nnpack
no
not
nullptr
number
oC
oH
oW
once
one
opt
optional
options
or
out
output
owned
padding
parameters
pass
per
posix
profile
pthreadpool
ptr
pytorch
reallocate
removal
requirements
right
running
runtime
safety
scalar
shape
single
size
spatial
src
static
status
str
strategy
stride
stringstream
subsample
success
successfully
sum
support
tensor
thread
threaded
threadpool
threads
throw
time
top
transform
tuple
type
types
u
unsupported
updateGradInput
updateOutput
usize
view
void
we
weight
width
with
work
workspace
wrapper
ws
yet
zeroes
zeros

ATen
CHECK
CUDA
Device
DeviceIndex
DeviceType
Display
Does
Formatter
HIP
HIPStream
HIPStreamMAsqueradingAsCUDA
HIPStreamMasqueradingAsCUDA
Hash
HipStream
Masquerading
NB
NOT
New
Note
PartialEq
Result
See
Self
Sorry
Stream
StreamId
TORCH
UNCHECKED
UNSAFE
Unchecked
Unsafely
Use
We
also
as
ascuda
aten
because
before
bits
check
coerce
coercion
constructor
cpp
cuda
current
default
derive
device
did
don
easier
enum
eq
ext
external
false
fix
fmt
from
getCurrentHIPStream
getDefaultHIPStream
getStreamFromExternal
getStreamFromPool
gets
h
have
here
high
hip
hipification
id
index
into
isHighPriority
just
let
makes
manages
masquerading
method
motivation
namespace
namespaces
new
operator
or
other
pack
packing
pool
priority
pytorch
query
range
right
s
set
setCurrentHIPStream
src
stream
synchronize
that
this
translation
u
unchecked
underlying
unpack
unwrap
us
was

AT
ATen
CPU
ERROR
Exposes
Memory
Storage
Tensor
allocator
an
as
aten
be
byte
can
cannot
cast
computeStorageNbytes
copy
cpp
cpu
debug
dense
device
empty
false
getCUDAHooks
getPinnedMemoryAllocator
has
internal
isPinnedPtr
itemsize
memory
native
only
operator
options
overlap
pin
pinned
purposes
pytorch
resizable
set
size
src
static
storage
strides
tensor
tensors
testing
toString
use

?
A
ASSERT
ATen
Activation
Add
After
Allocate
Ashkan
B
But
C
CHANNEL
CHECK
CPU
Calling
Consider
Creates
Currently
D
DimVector
Disabling
Do
DoNothing
FBGEMM
FN
GEMM
Get
Here
IMPL
INTERNAL
If
Input
IntrusivePtr
K
LIBRARY
Linear
LinearPackedParamsBase
M
MaybeOwned
NAME
NULL
NoTranspose
Note
OSS
OUT
On
Original
PYTORCH
PackAWithRowOffset
PackBMatrix
Packed
PackedLinearWeight
PackedLinearWeightsQnnp
Packs
Process
QLinearInt
QNNPACK
QuantizationGranularity
QuantizedCPU
RELU
ReQuantizeOutput
ReluFused
Resize
SELECTIVE
Sizeo
TENSOR
TODO
TORCH
Tensor
The
Therefore
This
Throws
TorchClass
Update
We
Weight
Weights
Your
\
`pmat`
across
act
activationLimits
added
adjust
affine
after
again
allocate
also
an
and
apply
are
arrays
as
assertion
aten
b
back
backend
batch
be
begin
below
bias
buffer
but
cache
calculate
call
called
cannot
case
cast
cfg
channel
channels
col
cols
column
columns
const
contig
contiguous
correctness
cpp
cpu
dequantization
device
different
dim
dimension
dimensions
do
doNothingObj
does
doing
eagerly
elements
empty
end
ensure
equal
examples
exception
executed
expect
expected
fail
failed
fallback
false
fbgemm
fbgemmPacked
fbgemmSupportedCPU
feature
first
floating
following
fp
friendly
from
further
generate
getPackedWeights
getRowOffsetBuffer
globalContext
got
groups
guarantee
hand
has
have
here
holding
id
in
index
input
integer
internally
into
intrusive
irange
jit
just
kCPU
kInt
kPerChannelAffine
kPerTensorAffine
kQInt
kQUInt
larger
lazy
ld
ldc
left
let
linear
long
loudly
m
machines
make
manages
match
matrices
matrix
max
min
mobile
models
module
move
multiplication
multiplier
must
nCol
nRow
native
needed
no
not
nullptr
numCols
numRows
number
numerics
offset
offsets
op
operation
operator
operators
optimizations
options
or
orig
original
out
outProcess
output
outputProcObj
owned
ownership
pack
packA
packB
packed
parallel
params
pass
passed
past
path
per
performed
pipeline
pmat
point
pointer
pointers
points
pre
provide
pthreadpool
ptr
pytorch
qbias
qint
qlinear
qnnp
qnnpack
qnnpackLinear
qscheme
quant
quantization
quantize
quantized
quint
rank
rather
re
register
reinterpret
release
releaseWeightsWhenPrepacking
relu
requant
requantization
requantize
requires
reset
resetting
resize
respectively
resulting
reuse
row
rows
rowwise
runStatus
s
same
scalars
scale
scales
scheme
second
should
since
size
smat
so
src
static
status
stride
strong
success
support
task
tasks
temporarily
tensor
term
than
that
then
these
this
thread
threadpool
threads
through
throw
tiles
times
trans
u
uint
unique
unpack
use
using
usize
value
values
vec
vectors
view
w
was
way
we
weight
weights
whole
will
with
within
won
wt
zero
zeros
zp

ATen
Ambiguity
Args
At
Batched
Batching
CHECK
CppFunction
Device
DimnameList
DispatchKey
For
Generator
However
IMPL
If
IntArrayRef
LIBRARY
Layout
MemoryFormat
NB
Note
Please
Random
Rules
ScalarType
Should
TENSOROPTIONS
TORCH
Tensor
Tensors
TorchCppFunction
VmapMode
VmapModeRegistrations
We
Whenever
above
all
ambiguity
an
apply
are
args
as
aten
avoid
banning
bernoulli
boxing
calling
cauchy
clear
const
cpp
d
decision
define
different
disable
dispatch
do
double
everything
example
exponential
fallback
false
feedback
from
gather
generator
geometric
have
haven
implementation
inside
instead
isn
kernel
key
lambda
lazy
like
listing
log
looking
low
m
made
makeFallthrough
makeNamedNotSupported
moment
multinomial
names
normal
not
number
one?
only
op
operations
operators
optional
or
out
outside
perform
place
poisson
pytorch
rand
randint
randn
random
randomness
randperm
re
really
register
registered
registering
same
should
so
special
src
static
support
temporarily
that
they
this
those
times
torch
types
undef
uniform
unsupported
unsupportedRandomOp
usage
used
user
vmap
we
while
with
workaround
works
yet
you
zeros

?
API
AT
ATen
BLAS
C
CHECK
EXPERIMENTAL
Environment
Experimental
Intraop
Invalid
MKL
MOBILE
NATIVE
NUM
OMP
OPENMP
OpenMP
PARALLEL
POOL
Parallel
ParallelCommon
SINGLE
String
TBB
TH
THREAD
THREADS
TORCH
TaskThreadPoolBase
Undefined
WARN
We
\n\tat
\tMKL
\tOMP
\tat
\tmkl
\tomp
and
aten
backend
be
caffe
call
catch
char
concurrency
const
cpp
cpuinfo
def
default
defaultNumThreads
determined
elif
endif
endl
env
ever
exception
false
getenv
hardware
hook
ifdef
in
info
interop
intraop
let
logic
max
mkl
mkldnn
mobile
name
native
need
not
nthreads
nullptr
omp
openmp
or
oss
ostringstream
parallel
pool
pytorch
set
should
single
size
src
ss
stoi
str
this
thread
threadpool
threads
try
u
unwrap
up
usize
utils
value
var
variable
variables
version
we
what
with

ACTIVATION
AT
ATen
Add
BATCHNORM
BN
BatchNorm
CHECK
CUDNN
ChannelsLast
CheckedFrom
Constant
CuDNN
D
ENABLED
ERROR
Float
Half
If
Is
MaybeOwned
MemoryFormat
NB
Note
OPS
Option
PER
PERSISTENT
R
ReLU
Relu
ResNeXt
SPATIAL
ScalarType
See
TODO
TORCH
Tensor
TensorArg
TensorDescriptor
The
Unused
VERSION
We
accept
access
accuracy
algorithm
allocate
and
as
aten
average
back
backward
backwards
batch
batchnorm
be
bias
borrow
but
c
call
can
cast
causes
cfg
check
checkAllContiguous
checkAllDefined
checkAllSameGPU
checkAllSameType
checkDimRange
checkNumel
checkSameSize
checkScalarType
compiled
const
contig
contiguous
contiguous?
convolution
correct
cpp
cuDNN
cudnn
cudnnBatchNormMode
cudnnBatchNormalizationBackward
cudnnBatchNormalizationBackwardEx
cudnnBatchNormalizationForwardInference
cudnnBatchNormalizationForwardTraining
cudnnBatchNormalizationForwardTrainingEx
cudnnGetBatchNormalizationBackwardExWorkspaceSize
cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize
cudnnGetBatchNormalizationTrainingExReserveSpaceSize
dataType
defined
desc
descriptor
different
dim
doesn
double
emplace
empty
endif
epsilon
etc
evaluation
exclusive
expand
expandScale
exponential
factor
fall
features
format
from
getCudnnDataType
getCudnnHandle
given
grad
hacky
handle
has
have
here
idesc
implements
in
initialize
input
introduced
kByte
like
losses
maybe
mean
memory
mode
models
native
new
norm
normal
not
nullptr
odesc
one
only
op
opt
optimization
optional
options
or
output
owned
parameter
passed
performance
philosophy
preprocessor
problems
ptr
pytorch
removal
require
required
reserve
reserveSpace
reserved
results
running
save
scalar
scale
should
size
so
space
src
start
static
such
suggest
support
tensor
tensors
that
them
this
training
tuple
type
undefined
usize
value
var
video
view
was
wdesc
we
weight
whatever
which
while
why
will
with
workspace
worth
wrapper
z
zero

ATen
Actual
AlignedAllocator
BCSRMatrix
Box
K
PYTORCH
QNNP
Size
UNLIKELY
WIN
assert
aten
back
bcsr
be
block
blocks
break
cc
cfg
col
const
cout
cpp
cpu
csr
endl
false
features
generate
goto
h
ib
include
index
indices
indices\n
input
j
jb
make
mat
matrix
must
native
nnz
not
output
pack
points
print
ptr
ptr\n
push
pytorch
qnnpack
quantized
reserve
row
scanned
size
sparse
src
u
unique
val
values
values\n
zero

?
@note
ALL
AND
ASSERT
AT
ATen
An
BFloat
Basic
BinaryClampFnAlpha
BinaryFn
BinaryFnAlpha
BinaryFnDouble
BinaryOps
Bool
Boolean
C
CHECK
COMPLEX
DISPATCH
DistributedDataParallelTest
Expected
FIXME
FUNC
FloorDivide
FloorDivideInplace
For
Half
IMPL
INTERNAL
If
In
It
Legacy
META
Make
NOT
None
Not
ONCE
Option
OutFunc
OutImpl
Output
Python
Scalar
ScalarType
Self
StringView
StructuredBinaryFn
StructuredBinaryFnAlpha
Stub
Subtraction
TODO
TORCH
TYPES
Tensor
TensorIterator
TensorIteratorBase
There
These
They
This
To
Unsupported
Use
Validate
WARN
WARNING
We
Without
\n
`
`logical
`~`
actual
actually
add
alias
aliased
all
alpha
ambiguious
an
and
any
appear
are
argument
arithmetic
atan
aten
avoid
b
backward
be
because
behavior
binary
bitwise
borrowing
both
build
but
call
cast
check
checking
clamp
comparison
complex
const
conversions
convert
copysign
cpp
create
current
currently
datatype
decide
declare
define
defined
deprecated
device
different
dim
dispatch
div
divide
division
divisor
do
doesn
doing
don
double
dtypes
during
elementwise
empty
eps
eq
equal
etc
exercise
exercised
expected
explicit
export
exposed
false
floating
floor
fmax
fmin
fmod
found
from
func
function
functions
future
gcd
ge
got
grad
gradients
greater
gt
h
has
have
heaviside
here
huber
hypot
iand
ident
igamma
igammac
ilshift
implemented
in
inconsistency
incorrect
inplace
input
instead
integral
interfaces
invert
ior
irshift
isBoolean
isComplex
isComplexType
isFloatingType
isIntegral
item
iter
ixor
kBool
kChar
kDouble
kFloat
kInt
kLong
kShort
keep
lazy
lcm
ldexp
le
less
like
logaddexp
logical
logit
lshift
lt
macro
mask
max
maximum
maybe
merely
meta
min
minimum
mode
move
mse
mul
multiply
must
name
native
ne
need
needed
negative
never
nextafter
non
normal
not
number
object
one
only
op
operations
operator
ops
optional
options
or
other
out
output
outside
overflow
overload
overloaded
perf
point
possible
pow
preserve
promotion
py
pytorch
re
reconsider
redispatch
referring
regression
relu
remainder
removal
removed
result
results
rounding
rounds
rshift
rsub
rules
s
same
scalar
scalarType
serialization
should
sigmoid
smooth
sparse
special
src
static
still
structured
stub
sub
subcmul
subtract
supported
tanh
tensor
tensors
test
testing
tests
that
this
toType
torch
toward
trait
trunc
trying
twice
type
types
undo
undocumented
unique
use
used
using
val
validates
value
values
version
view
want
we
will
with
without
wrapped
wrapper
xlog
xlogy
xor
y
yet
you
zero

API
BOOL
CU
CUDA
FILE
GENERIC
IS
REAL
THC
THCState
THCTensor
THCTensorMathPointwise
TORCH
aten
cpp
crossKernel
define
defined
dimension
endif
ft
generic
h
ifndef
lazy
none
pytorch
src
state
static
vim
void

?
ATen
Broadcasting
CHECK
CONTIGUOUS
CPU
CUDA
Compute
D
Distance
FORMAT
Follow
For
Gracefully
If
IntArrayRef
Keep
LEGACY
MEMORY
NoNamesGuard
Note
Option
See
Special
TODO
TORCH
Tensor
Tensors
The
This
Use
We
above
account
all
always
and
apply
approach
are
as
aten
autograd
automatically
backward
bad
based
batch
be
before
both
broadcasting
but
c
calculation
call
case
cat
cdist
checks
clamp
columns
compute
condition
connected
const
contiguous
cosine
cpp
cuda
dealing
declare
default
define
device
devices
dim
dimensions
dispatch
dist
distance
div
divide
do
does
doing
double
dtypes
empty
end
engine
ensure
eps
equals
euclidean
except
expand
expanded
explicit
figure
fill
fist
floating
force
forward
function
generate
got
grad
gradient
graph
guarantee
guard
h
handle
handled
has
have
here
improve
in
infer
input
insert
instead
integers
into
isFloatingType
itself
kCPU
kCUDA
keepdim
last
lazy
least
like
linearized
m
match
matmul
matrix
maybe
memory
metrics
might
min
mode
modes
mul
multiplication
multiply
must
n
namedinference
names
native
negative
non
nonempty
norm
not
number
numerical
one
ones
only
options
or
original
outnames
output
pad
pairwise
part
pass
passed
pdist
performance
point
portion
possible
pow
precision
product
propagate
properly
pytorch
r
redispatching
reference
relies
requires
resize
result
same
scalar
scipy
shape
should
similarity
simplify
size
so
sqrt
src
static
stay
step
steps
stub
subgradients
sum
supports
sync
taking
tensor
tensors
test
that
them
these
this
transpose
try
type
universally
use
using
value
values
version
view
void
w
was
we
will
with
zeros
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

?
API
ATen
Access
BUFFER
Block
Buffer
C
CHECK
COMBINED
Command
Compute
DESCRIPTOR
FN
IMAGE
IMPL
Input
IntArrayRef
It
KERNEL
LIBRARY
LIKELY
Not
OK
Object
Padding
Pool
Read
SAMPLER
STORAGE
SmallVector
Stage
TORCH
TYPE
Tensor
UNIFORM
VK
VULKAN
Vulkan
Write
access
adapter
an
and
api
appropriate
arg
async
aten
barriers
be
block
bottom
buffer
but
bypasses
cfg
command
const
context
convert
cpp
d
dim
dispatch
downcast
extents
false
final
ft
gpu
group
handle
has
have
image
implemented
implied
input
inserts
keep
lazy
left
lifetime
local
m
managed
must
native
necessary
none
not
object
only
ops
options
or
output
pad
padding
pool
pytorch
queue
reflection
resource
right
safe
size
src
static
stream
submit
synchronization
tensor
tensors
top
track
triggers
tuple
u
uniform
usize
uvec
v
vTensor
vim
vulkan
work

A
AT
ATen
Add
BitAnd
BitOr
BitXor
CHECK
CMP
Comparison
Complex
Default
Div
ERROR
EXC
Ensure
Exploit
FROUND
GT
INT
Implement
LT
MM
Most
Mul
NEAREST
NEQ
NO
NaN
OQ
Output
PartialEq
Q
Rhs
See
Self
SizeType
Sleef
Step
Sub
TO
TORCH
UNORD
UQ
V
ValueType
Vectorized
VectorizedComplexDouble
We
ZERO
`O`
`Q`
ab
abi
abs
ac
acos
ad
add
ai
align
all
an
and
angle
any
arange
arrays
asin
atan
aten
avx
b
base
bc
bd
be
because
bi
bitand
bitor
bitxor
blend
blendv
c
can
case
cast
castsi
ceil
cfg
change
cmp
com
compile
compiled
complex
conj
const
constexpr
convert
cos
cosh
count
cpp
cpu
d
derive
details
di
div
do
does
double
epi
eq
erf
erfc
exp
expd
expm
fact
false
feature
floor
gcc
github
h
hadd
half
here
hsub
https
hypot
igamma
igammac
im
imag
improve
index
initialize
instruction
instructions
isnan
issues
iz
let
ln
load
loadu
log
loop
m
map
mask
max
maximum
memcpy
memory
min
minimum
mm
mod
more
mul
multiplication
ne
neg
new
nextafter
not
number
numbers
one
ones
op
operand
operator
ops
or
os
other
output
pd
performance
permute
pi
pow
predicate
ptr
pytorch
raise
re
real
reciprocal
reinterpret
ret
rhs
root
round
rsqrt
set
setr
setzero
sgn
sign
sin
sincosd
sinh
size
sizeof
so
sqrt
src
step
store
storeu
sub
sum
super
supported
switch
tan
tanh
that
this
tmp
trigonomic
trunc
type
u
uninitialized
unpacklo
unwrap
use
using
v
val
value
values
vec
vectorized
void
we
while
windows
would
xFFFFFFFFFFFFFFFF
xor
xxyy
xy
y
z
zero

A
AFTER
ASSERT
ATen
An
Args
BITS
BitSet
C
CHECK
Can
DispatchKey
DispatchKeyExtractor
DispatchKeySet
Don
FULL
FunctionSchema
Generator
INTERNAL
If
In
It
IterArgs
Keys
LIKELY
ListType
LocalDispatchKeySet
MultiDispatchKeySet
NB
NOT
NUM
ORs
Option
OptionalType
PyTorch
Self
Set
String
TLS
TLS?
TODO
TORCH
Take
Tensor
TensorType
The
There
These
This
TorchJitStack
TorchJitpeek
UNLIKELY
Undefined
Unlike
Used
Well
You
\n
access
account
actual
actually
add
alas
all
allowed
always
an
and
anyting
applied
apply
applying
are
arg
args
argument
arguments
as
asked
aten
avoids
backend
backends
base
be
because
been
behavior
bit
bits
bitset
boxed
build
bump
but
bypass
call
calls
can
care
case
cases
certain
change
check
compute
computeDispatchKeySet
consideration
considered
const
contains
core
cpp
custom
defined
depending
deregister
determine
different
dispatch
do
does
don
dump
eliminate
entirely
entries
entry
excluded
extract
fall
fallthrough
fastpath
find
finding
folded
forwarding
from
function
gadget
gen
given
h
has
have
having
here
how
in
included
index
indices
initialized
input
instance
into
introduce
introduced
invariants
invoke
invokes
irritating
isSubtypeOf
isTensor
isTensorList
iterate
ivalue
just
k
kernel
key
keys
known
knows
ks
last
lets
list
local
locations
logical
look
make
makeBitsetForDispatchArgs
mask
may
means
method
move
multi
must
new
nice
no
non
nonFallthroughKeys
not
nothing
numbers
ofTensor
ofTensors
one
ones
only
operator
operators
opposed
order
oss
ostringstream
out
over
overridden
pass
precompute
pytorch
question
redispatch
refcount
reference
register
registered
relevant
remove
requires
reverse
s
safe
schema
set
should
since
size
skipped
skipping
small
so
some
specific
src
stack
state
stop
stored
str
supports
table
take
taking
tensor
tensors
th
that
them
then
there
they
this
through
tls
toTensorList
toTensorRef
top
tracked
troublesome
ts
type
types
unboxed
uninitialized
universal
unsafeToTensorImpl
unset
us
use
user
usize
utils
valid
value
varies
want
ways
we
what
which
why
will
win
with
would
xs
you
zero

ATen
ClampingParams
PyTorchQnnpU
assert
aten
c
const
cpp
cpu
cvtsi
do
epi
epu
extract
ft
increment
input
insert
kc
ks
load
loadl
m
max
maxpool
min
mm
n
native
none
output
params
pytorch
qnnpack
quantized
setzero
si
slli
src
srli
sse
storel
sub
u
uintptr
ukernel
unpackhi
unpacklo
usize
vi
vim
vmax
vout
voutput
while

ASSERT
ATen
CHECK
Got
Hardsigmoid
INTERNAL
PYTORCH
QEngine
QNNPACK
QnnpackOperatorDeleter
TORCH
Tensor
affine
aten
batch
cfg
channels
const
constexpr
contig
contiguous
cpp
cpu
create
createStatus
define
device
dispatch
elems
empty
endif
failed
flags
format
globalContext
hardsigmoid
ifdef
initQNNPACK
input
kCPU
kQUInt
max
memory
min
native
nc
ndimension
nullptr
o
op
operator
output
point
pthreadpool
ptr
pytorch
qEngine
qhardsigmoid
qnnp
qnnpack
quantized
quint
qx
qy
runStatus
scalar
scale
setup
setupStatus
size
src
status
stride
stub
success
suggest
tensor
threadpool
type
u
uniq
unique
usize
zero

ASSERT
ATen
DataPtr
Device
DeviceType
DispatchKey
Ensure
IMPL
LIBRARY
Layout
MSNPU
Make
MemoryFormat
Option
Scalar
ScalarType
Storage
TORCH
Tensor
TensorImpl
TypeMeta
UndefinedTensorImpl
add
aten
b
backend
byte
c
cpp
d
device
empty
extension
false
format
index
intrusive
kCPU
kMSNPU
layout
lazy
like
m
make
memory
move
non
nullopt
nullptr
op
operator
optional
override
pin
pytorch
register
size
src
static
still
stride
strided
tensor
test
that
type
use
works

ADD
APPLE
ATen
Add
After
And
Args
BEGIN
BEQ
BHI
BHS
BLO
BNE
BX
CMP
Check
ELF
END
Each
F
FUNCTION
First
For
GNU
In
It
LDR
LSL
Load
MOV
MOVLO
MOVLS
MOVNE
Now
POP
PUSH
S
SUB
SUBS
Shift
Stack
Store
TEQ
TOS
This
Thus
To
U
VADD
VCVT
VEOR
VEXT
VLD
VMLAL
VMOV
VMUL
VPOP
VPUSH
VST
VSUBL
VSWP
VTRN
We
When
aarch
accumulators
addr
address
after
align
all
and
arch
arm
armv
as
aten
atleast
avoid
b
back
base
be
because
beyond
bit
bits
block
blocks
buffer
bytes
c
can
ch
channel
const
contain
conv
cpp
cpu
create
d
do
dq
dynamic
element
endif
enter
epilogue
extra
fetched
first
fpu
frame
from
ft
function
gemm
happen
has
have
id
ids
ifdef
ifndef
implementation
in
inbetween
increment
index
indx
input
ip
iteration
just
k
last
late
lazy
load
loading
local
loop
lr
make
max
may
mr
mrxnr
multipliers
n
native
nd
needed
needs
neon
never
next
non
none
nonzero
note
nr
nrs
nrxmr
nth
number
offset
one
other
out
output
overlap
packed
packedA
params
parts
passed
per
point
pointer
points
processed
processing
produce
produces
progbits
ptr
pushing
pytorch
qnnp
qnnpack
quantization
quantized
r
reg
requant
restrict
result
row
same
scale
section
should
so
sp
sparse
src
st
stack
static
store
stored
storing
stride
sure
syntax
temp
that
this
transpose
transposed
u
ukernel
unified
union
user
usize
value
values
variables
via
vim
vmultiplier
void
vxa
vxb
w
we
weight
weights
well
which
will
with
zero
zp

?
ATen
C
CHECK
Call
Cannot
CatchAll
Channel
ChannelsLast
Conv
ConvPackedParamsBase
ConvTranspose
D
DIM
Didn
FBGEMM
FN
G
Groups
IMPL
IntrusivePtr
KCRS
KRS
LIBRARY
MakeEmptyAffineQuantizedChannelsLast
MakeEmptyPerChannelAffineQuantizedChannelsLast
MemoryFormat
NAME
Option
PYTORCH
PackedConvWeight
PackedConvWeightsQnnp
Per
QConv
QConvDilation
QConvGroups
QConvOutputPadding
QConvPackWeightInt
QConvPadding
QConvStride
QConvTranspose
QConvUnpackWeightsInt
QEngine
QNNPACK
Quantization
R
S
SELECTIVE
SPATIAL
SpatialDim
TODO
TORCH
Tensor
The
Therefore
TorchList
TransposeConvTensorUnpackConversion
Unify
Unpacked
Unsupported
We
affine
and
aten
available
axis
be
because
before
bias
blob
but
cast
cfg
channel
channels
clone
consistent
const
conv
cpp
cpu
ctx
currently
d
dSqueezeDim
dTensor
dUnpackWeightsInt
defined
deprecated
device
different
dilation
disabled
empty
enable
endif
engine
expects
false
fbgemm
feature
find
first
format
from
globalContext
groups
h
have
height
how
however
ifdef
in
input
inputChannels
its
kCPU
kConv
kDouble
kFloat
kInt
kLong
kPerChannelAffine
kPerTensorAffine
kQInt
kSpatialDim
kernel
lazy
loading
logical
m
memory
name
native
new
not
now
nullopt
only
operation
optional
or
orig
output
outputChannels
packed
packing
padding
per
physical
please
points
ptr
pytorch
qEngine
qconv
qint
qnnpack
qscheme
quant
quantized
ready
reinterpret
results
right
s
same
scale
scales
scheme
setReleaseOriginalWeights
shape
size
squeeze
src
static
stores
stride
supports
tensor
that
tie
toString
toType
transpose
transposed
tuple
unpack
unpacked
unpacking
use
using
usize
utils
w
want
we
weight
weights
width
with
would
zero
zp

ANY
AS
ATen
All
Apache
BASIS
C
CONDITIONS
Copyright
Google
INT
IS
Inc
It
KIND
LICENSE
License
Licensed
MIN
MM
OF
OR
QNNPACK
Reserved
Rights
SHUFFLE
SSE
SSSE
See
The
UINT
Unless
Version
WARRANTIES
WITHOUT
You
abs
account
adapted
add
agreed
all
an
and
andnot
apache
applicable
aten
b
be
below
benchmarks
bit
blend
blendv
but
castps
castsi
cmpeq
cmpgt
cmplt
comparative
compliance
const
copy
cpp
cpu
cvtsi
distributed
do
doubled
doubling
either
endif
epi
epu
except
exponent
express
fff
fields
file
from
ft
gemmlowp
gives
governing
h
happen
high
http
ifdef
implied
in
into
itself
lanes
language
law
library
licenses
limitations
m
mask
may
min
mm
mul
multiplication
native
nd
neg
negate
negative
none
not
nudge
obtain
only
or
org
overflowed
part
permissions
positive
pre
products
promote
ps
pytorch
qnnpack
quantized
rdivbypo
remainder
requantization
required
result
rounded
rounding
s
saturate
saturated
saturation
set
setzero
shuffle
si
sign
slli
software
specific
sra
srai
src
srli
sse
st
sub
take
tests
that
this
those
threshold
under
unit
us
use
used
uses
vim
vqrdmulh
which
will
with
writing
www
xCC
xor
you
zero

ATen
C
Utils
and
aten
avx
bf
bw
ceil
check
copy
cpp
cpuinfo
device
dilation
dq
h
has
initialize
input
kernel
lr
mkldnn
mode
native
output
pad
padding
pool
pooling
pytorch
r
shape
size
src
stride
usize
vl

ATen
Apply
ArgNames
Args
BENCHMARK
Benchmark
BenchmarkState
C
CAPTURE
H
K
MAIN
Max
MaxPool
NO
P
PYTORCH
Pooling
QNNPACK
S
SetBytesProcessed
ShuffleNet
SkipWithError
SqueezeNet
SqueezeNetV
VGG
W
aten
b
batchSize
begin
bench
bind
cc
channels
const
cpp
cpu
create
d
delete
device
dilation
distribution
end
endif
failed
fill
flags
generate
height
ifndef
initialize
input
inputHeight
inputPixelStride
inputWidth
iterations
lazy
max
mt
native
net
netv
nhwc
nullptr
operator
output
outputHeight
outputPixelStride
outputWidth
paddingSize
pool
pooling
poolingOperator
poolingSize
pytorch
qnnp
qnnpack
quantized
random
randomDevice
range
ref
rng
setup
shuffle
shufflenet
sizeof
squeeze
squeezenet
src
state
static
status
stride
success
thread
u
uniform
usize
v
vgg
width
xA

ATen
NULL
PyTorchQnnpOperator
PyTorchQnnpStatus
QNNPACK
Size
allocate
and
aten
average
avgpool
batch
be
because
buffer
bytes
c
calloc
channels
compute
const
cpp
cpu
create
delete
enum
error
failed
finite
flags
format
global
goto
in
initialized
input
invalid
isnormal
log
max
memory
min
must
native
non
not
number
nwc
op
operator
out
output
padding
parameter
params
pixel
point
pointer
pooling
positive
properly
pytorch
qnnp
qnnpack
quantization
quantized
quint
range
ratio
scale
setup
size
sizeof
src
status
stride
structure
success
type
u
ukernel
uninitialized
unsupported
void
width
with
zero
zu

A
API
ARCH
ASSERT
AT
ATEN
ATen
AVX
ArgTypes
Args
At
Atomic
CAPABILITY
CPU
CPUCapability
CUDA
CUDACC
Compiler
DECLARE
DEFAULT
DEFINE
DEFINITION
DISPATCH
DeviceType
Dispatch
DispatchStub
DispatchStubImpl
ERROR
Example
Fixing
FnPtr
GridSampleKernel
HAVE
HIP
HIPCC
HIPify
INTERNAL
Implements
In
Kernels
MSVC
MyKernel
NB
NUM
OPTIONS
PyTorch
REGISTER
RegisterCUDADispatch
RegisterHIPDispatch
See
Self
So
Some
TODO
TORCH
Tensor
The
This
To
Use
VSX
WARN
Windows
`fn`
`using`
about
above
actual
adding
all
an
and
anonymous
arch
are
args
argument
assertions
aten
available
avx
backward
based
be
bloat
build
builds
call
capability
case
cast
causes
cfg
choose
chosen
class
com
compiled
compiler
complain
compute
conflict
const
contains
cpp
cpu
cpuinfo
cu
cuda
cut
d
debug
declaration
declare
decltype
decreasing
default
define
defined
delete
details
device
different
dispatch
do
don
due
duplicated
elsewhere
endif
enum
envar
error
even
expansion
export
expr
extern
fastest
features
file
flags
fma
folded
forward
found
fptr
from
function
getenv
github
grid
h
has
helper
here
hip
https
ifdef
ignore
ignoring
in
initialize
inlined
instruction
into
invalid
issues
kCPU
kernel
lazy
like
load
macro
main
make
mavx
may
mechanism
memory
method
methods
missing
more
multiple
must
name
namespace
native
need
needs
new
no
none
not
nullptr
number
once
one
operator
or
order
os
otherwise
outline
over
parentheses
pass
point
pointers
possible
powerpc
preference
pretending
private
ptr
public
purpose
put
pytorch
rT
race
register
reinterpret
relaxed
reported
rid
rules
runtime
s
same
sampler
selection
set
sets
should
significant
since
size
so
sole
specialization
specialized
specific
src
static
still
stop
store
strcmp
stub
such
switch
table
tensor
that
there
they
things
this
threads
times
try
tuple
ty
type
typename
unsupported
use
used
using
value
vars
versions
void
vsx
warnings
we
whatever
will
windows
with
won
work
workarounds
you

ASSERT
ATen
CHECK
Conv
ConvPackedParams
ConvPackedParamsBase
ConvParamsSerializationType
Convolution
Didn
FBGEMM
Fields
INTERNAL
IValue
IntrusivePtr
Note
ONNX
PYTORCH
PackedConvWeight
PackedConvWeightsQnnp
Parses
QEngine
QNNPACK
SpatialDim
TORCH
Tensor
TensorOptions
TorchList
Unable
Unexpected
Version
and
any
are
aten
automatically
back
bad
based
begin
bias
blob
cast
clone
const
containers
contents
conv
convert
cpp
cpu
create
ctx
current
d
default
deserialize
deserializing
determine
dilation
does
elem
elements
emplace
end
endif
engine
exist
expected
false
fill
find
firstElement
format
from
globalContext
got
groups
h
handle
happy
historical
idx
ifdef
in
insert
into
ints
isString
isTensor
isTensorList
isTuple
item
kShort
kSpatialDim
lazy
length
list
make
manually
move
native
non
not
note
now
only
optional
or
output
ownership
packed
padding
parameters
params
parse
parsing
prepack
prepacked
push
pytorch
qEngine
qconv
quantized
retain
serialization
serialize
serialized
setstate
size
skip
so
src
state
static
str
stride
supported
supports
tensor
tensors
tie
toList
toOptional
toString
toStringRef
toTensor
toTensorList
toTuple
transpose
tuple
u
unpack
up
using
v
value
vec
version
versions
we
weight

?
ATen
AVX
Align
All
Also
Args
Because
But
C
CANNOT
CPU
Cast
CastImpl
Clang
Complex
DATA
DEFINE
DO
Default
Do
E
First
GCC
GLOBAL
HEADER
IEEE
IN
If
Implements
MSC
Make
NB
NOT
NOTE
NaN
Note
Now
ODR
Op
PC
R
SFINAE
STATIC
Scalar
See
Self
SizeType
So
Specifically
Step
THIS
TPtr
The
There
This
U
VER
Vectorized
VectorizedIdE
Vectorizedi
We
Why
Without
ZN
`
`maximum`
`minimum`
above
abs
according
acos
actually
addr
against
align
all
along
also
always
an
and
angle
anonymous
another
any
apply
arange
architecture
are
arg
arr
asin
ask
assert
atan
aten
avoid
avx
b
back
base
be
because
best
binary
bit
bits
bitwise
blend
blendv
buffer
bug
buggy
c
calc
can
capability
case
cast
ceil
changed
changing
check
clamp
clarity
class
coerce
com
compile
compiled
compiler
completely
complex
conj
const
constexpr
constraint
containing
conversion
convert
copysign
correctly
cos
cosh
could
count
cpp
cpu
cut
deal
default
define
defined
definition
deinterleave
derivations
detection
determinations
different
divide
do
does
doesn
don
double
dst
either
element
elements
emulates
enable
enabled
enclose
endif
eq
equal
erf
erfc
erfinv
error
even
exp
expm
fPIC
feature
file
files
floating
floor
fmadd
fmod
four
frac
function
functions
gather
ge
generic
github
given
global
good
greater
gt
guard
h
half
handle
handled
hard
have
here
higher
highest
https
hypot
identifier
idx
ifdef
ifndef
igamma
igammac
ignore
imag
implies
in
incuring
index
initialization
initializer
initializers
inlined
input
instance
instruction
instructions
integer
intended
inter
interleave
intmax
inverse
isnan
issues
its
just
lambda
lazy
le
less
let
lgamma
lies
like
live
loadu
log
logical
lt
m
make
making
map
mask
max
maximum
member
members
memcpy
memset
midway
might
min
minimum
mm
more
multiple
must
n
naive
namespace
native
ne
nearest
need
needed
neg
negative
never
new
next
nextafter
no
non
not
numbers
object
obligated
odr
one
ones
only
op
operation
operations
operator
operator~
or
ordinary
other
others
otherwise
out
overload
pair
partially
past
perform
place
point
pow
pragma
precision
pred
problem
promotion
propagate
propagates
properly
provide
ptr
purposes
put
pytorch
rather
real
recall
reciprocal
recompile
reinterpret
relocation
replace
representation
require
requires
resolve
result
results
ret
returns
root
round
rsqrt
rules
s
same
scale
seems
set
sets
sgn
shared
should
si
sign
sin
since
single
sinh
sites
size
sizeE
sizeof
so
some
sometimes
specialize
specific
sqrt
src
standard
static
step
store
storeu
sure
symbol
tan
tanh
technically
tell
templated
ternary
than
that
their
there
therefore
these
they
thing
this
times
tptr
trailing
translated
trivial
trunc
type
typename
types
u
ubsan
unable
unary
undefined
unroll
unwrap
usage
use
used
using
val
vals
value
values
variable
variable?
vec
vectorized
versions
versus
vindex
void
want
was
way
we
were
whether
which
will
with
within
without
won
would
wouldn
xFF
xor
you
zero
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A
AND
ARM
ATen
BLOCK
But
COL
CONVERT
Convert
Each
FP
In
K
Load
M
MM
MR
Note
Now
Offset
Original
PACKED
Packed
Per
PyTorchQnnpConvDynamicQuantizationParams
SHUFFLE
SIZE
Similarly
Size
TO
TRANSPOSE
That
Then
This
Thus
Transform
We
Where
acc
acculated
activation
activations
add
adjacent
alls
already
also
and
appropriate
arbitrary
are
as
assume
aten
b
baseline
be
because
bit
bits
block
blocks
broadcast
c
can
care
channel
col
column
compressed
const
contain
containing
contiguous
convert
copying
correctness
corresponding
cpp
cpu
cvtepi
cvtss
d
do
doing
dq
epi
establish
export
first
format
fp
ft
functional
gemm
given
group
half
has
have
helps
high
how
id
ident
ids
in
index
input
instead
into
ints
just
k
kernel
kx
load
loading
loadl
loadu
locality
low
m
macro
mat
matrix
memory
mm
mostly
mr
mul
mulhi
mullo
multiple
multipliers
n
native
none
not
nr
offset
optimal
out
output
packed
packedA
packeda
params
perf
pi
placed
point
points
probably
ps
ptr
pytorch
qnnpack
quantization
quantized
reference
register
registers
rest
result
row
rows
rules
s
second
set
setzero
should
shuffle
si
since
sparse
spills
src
sse
start
starts
stored
storel
storeu
stride
sub
taken
temp
this
those
tmp
transformed
transpose
transposed
u
uint
ukernel
unpackhi
unpacklo
va
vacc
valid
values
vbias
vim
vmultiplier
vout
vxa
vxb
vzero
w
way
we
weight
while
wil
will
with
work
writing
zero

?
A
A@B
A@B`
AND
ASSERT
AT
ATen
Aj
Ap
Ax
B
BF
Bank
Bi
Bj
Bp
Bx
C
CHECK
COMPLEX
COO
CSR
Cj
Compute
Cp
Craig
Cx
D
DISPATCH
Douglas
E
Expands
FLOATING
INTERNAL
Inputs
K
M
Matrix
Multiplication
Note
Output
Outputs
Package
Randolph
SMMP
Scalar
Sparse
SparseMatMul
TORCH
TYPES
Tensor
The
This
`
`A`
`Aj
`Ap
`Ap`
`Ax
`Bi`
`Bj
`Bp
`Bx
`C
`C`
`Cj
`Cp
`Cx
`mat
`n
aj
algorithm
an
and
ap
are
arrays
aten
ax
be
bi
bj
bp
buffer
bx
cannot
cj
clear
coalesce
col
col`
column
columns
compatible
compressed
contiguous
coo
cp
cpp
cpu
csr
cx
dense
dim
dimensions
does
doi
empty
end
entries
expected
format
got
head
https
implementation
in
indices
indptr
into
j
jj
k
kLong
kernel
kk
length
like
mask
mat
match
matmul
matmult
matrices
matrix
maxnnz
multiplication
multiplied
must
n
native
needed
next
nnz
nonzeros
not
number
operation
org
output
pointer
preallocated
proper
ptr
pytorch
resize
row
row`
rows
scalar
select
shapes
should
size
sparse
src
start
structure
sums
temp
tensors
their
type
v
values
which

?
@
@brief
@param
@return
A
ALL
AND
API
ASSERT
AT
ATen
AddrFn
After
Algorithm
Algorithms
Allocates
Also
Anal
Appl
Applications
Apply
Approximation
Array
As
BLAS
Backend
Bader
Based
Benchmarking
Big
Blanes
Blob
Bool
Boolean
Bs
C
CHECK
CHECKs
COMPLEX
CPU
CUDA
Calculate
Calculates
Callers
Can
Cannot
Casas
Cast
Casting
Chain
Chapter
Check
CheckedFrom
Checks
Clone
ComplexDouble
ComplexFloat
Compute
Computing
Concatenate
Contiguous
Cormen
Creates
D
DEGS
DISPATCH
DeviceType
DimVector
Double
ERROR
Edition
Ensure
Expect
Expected
Exponential
F
FORTRAN
FUNC
Failed
Fast
Fill
Float
Follows
For
Frobenius
Func
Functions
GRAIN
Got
GradMode
Guard
Helper
Here
Hermitian
However
IMPL
INFINITY
INTERNAL
If
Implements
In
Incompatible
Inference
InferenceMode
InitializerList
IntArrayRef
Introduction
Invalid
Invert
Issue
It
J
KronImpl
Kronecker
LAPACK
LU
Last
Layout
Let
LinalgVectorNormFn
LinearAlgebra
Long
Longer
META
MKL
MOBILE
Makes
Mathematics
Mathias
Matrices
Matrix
MaybeOwned
MemoryFormat
More
Moving
NAN
NOTE
Need
NoNamesGuard
NoTF
NoTranspose
None
Note
NumPy
Numerical
O
OMP
ONCE
Only
Optimized
Option
Order
Otherwise
P
Performs
Please
Polynomial
PyTorch
Raises
Ran
RecordFunction
Recursively
Requires
Reshape
Resize
Return
Roy
Rule
S
SIAM
SIZE
SVD
Samsung
Scalar
ScalarType
Scale
See
Self
Since
SmallVector
So
Square
Step
Strided
String
StringView
TLS
TODO
TORCH
TOTAL
TYPES
Taylor
Tensor
TensorIterator
TensorIteratorConfig
TensorList
TensorOptions
Tensors
The
There
They
Third
This
Thus
TotalNDegs
Transpose
Ts
TupleTensorRefs
U
UnpackPivotsFn
Use
V
Validates
Variant
View
WARN
WARNING
We
When
Why
XLA
\
\pm
`
`a`
`b`
`buffer`
`device
`dims`
`matmul`
`norm
`num
`other`
`self
`self`
`unpacked
about
above
abs
abslogdet
absolute
accepts
access
accessor
account
accumulate
accumulation
add
addbmm
addmm
addr
adjusted
after
al
algorithm
alias
all
allocate
allocated
allow
allowed
along
alpha
already
also
always
amax
an
analytic
and
another
appended
applied
apply
appropriately
approx
approximation
arange
are
arg
args
argument
arguments
arithmetic
as
ascending
assert
assigns
assume
assumed
aten
atol
autograd
automatic
avoid
avoided
avoiding
axes
b
back
backend
backends
backshift
backward
baddbmm
based
batch
batched
batches
be
because
been
before
begin
behavior
behaviour
below
benchmark
benchmarked
benchmarks
beta
better
binary
bit
bits
blas
blob
block
bmm
book
borrowed
both
bound
break
breaks
broadcastable
broadcasted
bs
buffer
buffers
build
but
c
calculate
call
called
calling
calls
can
canCast
cannot
care
case
cases
cast
casting
catch
causes
cbegin
cdouble
ceil
cend
cfloat
chain
change
characteristics
check
checkAllSameDim
checkDeviceType
checkDim
checkLinalgCompatibleDtype
checkOnCPU
checkSameDevice
checking
checks
circument
clone
coefficients
cols
column
com
combination
combining
common
comparison
comparisons
compatibility
compatible
compiled
complete
complex
composite
composition
computation
computations
compute
computed
computes
computing
concern
cond
condition
conj
consider
consisting
const
constant
constexpr
construct
contains
contiguous
contraction
control
controls
convert
converted
copies
copy
correct
cost
could
cpp
cpu
cpu`
cpublas
create
cross
cuda
curr
currently
cut
d
decide
decision
declare
decomposition
default
define
defined
definition
deg
degree
degs
depending
depends
deprecated
descending
designed
desired
dest
det
details
device
devices
diag
diagonal
different
differentiation
differently
dim
dimension
dimensional
dimensionality
dimensions
dims
direct
directly
disable
discussion
dispatch
dispatchKeySet
dispatcher
div
do
does
doesn
done
dot
double
dtypes
dynamic
early
eigen
eigendecomposition
eigenvalue
eigenvalues
eigh
eigvalsh
either
elements
embed
emplace
empty
enable
enabled
end
endif
enforce
ensure
epsilon
equal
equation
error
errors
et
etc
even
ex
example
except
exchanges
existing
exp
expand
expanded
expansions
expecitly
expect
expected
expects
explicitly
exponent
exponential
exponentials
extract
eye
facing
fact
factorization
fake
false
faster
favor
fewer
fill
final
find
fire
first
fix
flatten
flattened
floating
fmod
fold
folding
following
follows
form
format
forth
found
fro
frobenius
from
full
func
function
functions
further
future
gather
ge
gemm
generalized
ger
github
give
given
gives
going
got
grad
gradmode
grain
graph
greater
guaranteed
guard
h
handle
handling
has
hasMKL
have
helper
helps
hence
here
hermitian
highest
hold
how
https
id
idea
ideal
identical
identify
identity
idx
ifdef
ignored
implement
implementaion
implementation
implemented
implicitely
in
include
incompatible
incorporate
ind
index
indexing
indices
inf
infer
infinite
infinities
infinity
info
informative
infos
infs
inner
inplace
input
insert
instead
integer
integers
integral
intended
intermediate
internal
interval
into
introduce
ints
inv
invalid
inverse
inversion
invert
inverte
invertible
iota
irange
isBoolean
isComplexType
isFloatingType
isIntegral
issues
item
items
iter
its
j
js
just
k
kBFloat
kCPU
kCUDA
kComplexDouble
kComplexFloat
kDouble
kFloat
kHalf
kInt
kLong
keepdim
keeping
kernel
kind
know
kron
ks
lambdas
large
larger
last
layer
layout
lazy
lda
ldb
ldc
ldm
leading
least
len
length
let
like
likely
linalg
linear
list
local
locations
log
logabsdet
logdet
logic
lower
lu
m
machinery
made
main
make
manually
masked
mat
match
math
mathematical
matmul
matric
matrices
matrices?
matrices`
matries
matrix
matter
max
maxdim
maximum
may
maybe
mean
means
mem
memory
message
messages
meta
method
methods
mexp
middle
migrated
min
minimizes
minimum
minus
mismatch
mkl
mm
mobile
mode
more
most
move
moved
movedim
moving
much
mul
multi
multiple
multiplication
multiplications
multiplied
multiplies
multiply
multiplying
multithreaded
multithreading
must
mv
n
naive
name
namedinference
names
nan
nans
narrow
native
ndim
ndimension
need
needed
negative
neither
new
nicer
no
non
none
nonempty
nonzero
norm
normally
norms
not
note
now
nuc
nuclear
nullopt
number
numerical
numpy
off
offset
omit
ommited
once
one
ones
only
op
operand
operate
operation
operations
operator
ops
opt
optimal
optimally
optimization
optimizations
optional
options
or
ord
order
orders
original
othe
other
otherwise
out
outer
outf
outnames
output
outputs
over
overall
overlap
owned
pad
parallel
parallelized
parameter
parameters
parenthesizing
passed
pathological
paths
perform
performance
performed
performs
perm
permutation
permute
permuted
pinv
pinverse
pivot
pivoted
pivots
pivots`
pivs
place
point
pointer
pop
portion
pos
positive
possible
pow
power
powers
practical
prepended
probably
problem
prod
prods
produce
produced
produces
product
products
programming
promote
promotion
propagate
propagated
propagates
proper
provided
pseudoinv
pt
ptr
public
pull
purpose
purposes
push
put
py
pytorch
question
quick
r
raise
raised
raises
raising
rank
rather
rcond
real
reciprocal
recommended
recursive
reduce
reduced
reduction
reductions
reinterpret
related
relations
relative
release
relies
removed
renamed
replace
require
required
requirement
requires
res
reshape
reshaped
resize
respectively
respects
rest
result
resulting
results
returned
returns
reverse
right
room
routine
row
rows
rtol
s
safe
same
satisfy
saves
scalar
scalarType
scalars
scale
scaled
scatter
second
see
seems
select
separate
separately
series
set
settled
sgn
shape
shapes
shift
should
sigle
sign
significant
simplicity
simplify
since
single
singular
size
slice
slightly
slogdet
slow
small
so
solve
some
something
sorted
sparse
special
specify
specifying
split
sqrt
square
squareCheckInputs
squared
squash
squeeze
src
stage
start
starting
state
static
storage
store
str
strictly
stride
strided
strides
structured
stub
style
sub
subset
subtract
such
sum
support
supported
supports
svd
svdvals
swap
swapped
symmetric
synchronization
take
takes
tensor
tensordot
tensorinv
tensors
tensorsolve
term
tesnsors
test
tf
than
that
their
them
then
therefore
theta
thetas
they
this
thoroughly
though
thread
threads
three
threshold
through
throw
thrown
thus
tie
tmp
toComplexDouble
toDouble
toListOfOptionalTensors
toString
toValueType
together
togther
tol
tolerance
tolerances
torch
total
track
transpose
transposed
tries
tril
triu
trivial
try
tuple
type
typeMetaToScalarType
types
u
unchanged
undefined
under
uninitialized
unlock
unpack
unpacked
unsafe
unsqueeze
unwrap
upper
us
use
used
user
uses
using
usize
usually
utilize
uv
v
val
valid
vals
value
valued
values
variant
various
vdot
vec
version
versions
view
viewed
visit
void
vs
want
was
way
we
went
whether
which
while
why
will
with
won
work
would
wrap
write
wrong
xla
y
yet
z
zero
zeros
~
±
±inf

ATen
CHECK
Container
Dimension
E
INDEX
SmallVector
TORCH
TensorImpl
TensorList
We
Wrap
WrapDimUtils
all
allowed
also
and
are
as
aten
backwards
be
behavior
both
but
can
cat
checking
compatibility
container
contiguous
continue
could
cpp
dim
dimension
dimensional
dimensions
dims
empty
error
expected
expr
false
got
h
implementation
in
legacy
let
list
maintain
make
max
maybe
min
n
ndims
necessary
negative
not
number
only
or
other
out
possible
post
previously
ptensor
pytorch
range
rely
scalar
size
skipped
so
specific
src
taking
tensor
tensors
these
this
throw
thus
total
underlying
unless
unwrap
usize
wasn
we
were
will
with
wrap

ATen
INTERNAL
PYTORCH
QNNP
aten
clamping
const
cpp
cpu
declare
export
function
h
ident
increment
kc
ks
macro
maxpool
n
name
native
neon
params
pytorch
qnnp
qnnpack
quantized
rules
src
sse
sub
u
ukernel
union
usize
void
y

API
Add
BFloat
Bool
Byte
COMPLEXDOUBLE
COMPLEXFLOAT
COPY
Char
ComplexDouble
ComplexFloat
Double
FILE
Float
GENERIC
Half
IMPLEMENT
IS
Int
Long
NOTE
QInt
QUInt
REAL
Scalar
Short
Storage
Support
TH
THArgCheck
THBFloat
THBoolStorage
THByteStorage
THCharStorage
THComplexDoubleStorage
THComplexFloatStorage
THDoubleStorage
THFloatStorage
THHalfStorage
THIntStorage
THLongStorage
THQINT
THQInt
THQUINT
THQUInt
THShortStorage
THStorage
THStorageCopy
TODO
TYPENAMESRC
\
aten
calls
cast
complex
copy
copyBFloat
copyBool
copyByte
copyChar
copyComplexDouble
copyComplexFloat
copyDouble
copyFloat
copyHalf
copyInt
copyLong
copyQInt
copyQUInt
copyShort
cpp
cross
define
defined
different
endif
ft
generally
generic
h
ifdef
ifndef
in
inner
lazy
loops
macros
mismatch
nbytes
none
performance
pointer
pytorch
rather
raw
repeated
scalar
size
sizeof
src
static
storage
than
these
types
u
use
vim
void

ANY
ASSERT
ATEN
ATen
Backend
CPU
CUDA
Can
DISPATCH
DeprecatedTypeProperties
EXPECT
FALSE
Issue
Mean
PERF?
RESOLVED
Reduction
Requires
STATIC
Scalar
Sparse
StartsWith
TEST
THROW
THROWS
TODO
TRUE
Tensor
TensorOptions
Test
TestAbsValue
TestAdd
TestAddingAValueWithScalar
TestCopy
TestCopyBroadcasting
TestDispatch
TestIndexingByScalar
TestIndexingByZerodimTensor
TestIndexingMixedDevice
TestIntArrayRefExpansion
TestIsContiguous
TestLoadOfAddsWithCopy
TestLoadsOfAdds
TestMm
TestNegativeDim
TestOnesAndDot
TestPermute
TestRandperm
TestResize
TestSelect
TestSort
TestSqueeze
TestToCFloat
TestToString
TestView
TestZeroDim
TestZeros
Testing
This
Throw
Torchis
Variable
Y
\n
abs
add
adding
addmv
adds
allclose
an
and
arange
are
assert
aten
available
avalue
avoid
b
backend
basic
be
begin
broadcasting
bug
bugs
but
c
cast
cat
catch
cause
cfloat
chrono
clock
com
context
contiguous
copy
count
cout
cpp
cpu
cuda
d
dec
default
defined
details
device
different
dim
dispatch
do
dot
double
duration
empty
end
endif
endl
eq
equal
equals
everything
exception
expansion
expect
export
factory
false
from
github
globalContext
grad
half
hasCUDA
hex
high
https
ident
ifndef
implemented
index
indexing
integral
isEQ
isLE
isLT
issues
item
kCPU
kCUDA
kDouble
kFloat
kHalf
kInt
kLong
kSparse
kStrided
layout
like
load
loads
loss
m
macro
manual
max
memory
methods
milliseconds
mixed
mm
ms
mse
mv
nbytes
negative
norm
not
now
nullptr
one
ones
only
operator
options
other
out
overloads
path
permute
pin
pinned
pool
proto
ptr
pull
pytorch
r
rand
randn
randperm
ref
relu
requires
requiring
reshape
resize
resolution
resolved
result
ri
rules
rv
s
scalar
scalarType
scalars
see
seed
select
send
set
setting
should
simple
size
sort
sorted
squeeze
srand
src
ss
static
str
strides
stringstream
substr
sum
tensor
tensors
test
testcuda
that
this
tie
time
transpose
ty
type
u
usize
value
values
variants
view
viewed
was
which
will
with
work
z
zach
zero
zerodim
zeros

ATen
Error
For
NULL
PackBMatrix
QNNPACK
Runtime
Scale
Self
allocate
and
assert
aten
be
bias
bytes
cc
channel
channels
connected
const
conv
cpp
cpu
create
d
error
failed
fc
finite
fully
gemm
input
isnormal
k
kernel
kr
log
malloc
must
n
native
new
nr
operator
output
pack
packed
packing
params
points
positive
prepack
pytorch
qnnp
qnnpack
quantization
quantized
requant
requantization
runtime
scale
scales
sizeof
src
stride
u
usize
weights
with
wrq
zero
zu

ADD
ALIGN
ATen
Add
Adjust
Args
B
BEGIN
CMP
CODE
CSEL
Channel
Compute
DIRECTIVES
ELF
END
EOR
EXT
FADD
FMOV
FMUL
FUNCTION
GNU
HS
IGNORE
LD
LDP
LDR
LO
LS
LSL
Load
MOV
Mul
NE
POINT
R
RET
S
SCVTF
SMLAL
ST
STP
SUB
SUBS
Skip
TOS
USHL
USUBL
V
ZERO
aarch
align
assembly
aten
b
base
bias
byte
c
ch
channel
const
conv
cpp
cpu
d
dq
endif
ft
gemm
h
ifdef
ifndef
include
index
indx
k
lazy
mr
multiplier
native
neon
none
note
nr
offset
out
output
over
params
passed
per
point
pointer
points
progbits
pytorch
qnnp
qnnpack
quantization
quantized
requantization
restrict
runtime
s
section
shift
sp
src
stack
static
stride
u
ukernel
union
usize
v
va
vacc
vb
via
vim
void
w
zero

ATen
Q
S
Workaround
aarch
always
and
asm
aten
b
builtin
const
cpp
cpu
d
gcc
h
in
intrinsics
ld
missing
neon
o
oi
poly
pytorch
ret
s
simd
src
st
u
uint
val
vec
vld
volatile
vst
w

ATen
Arc
Default
Impl
MPSImageWrapper
MetalTensorImplStorage
Self
aten
const
copy
cpp
defined
derive
dim
from
h
host
input
metal
native
new
pytorch
set
src
strides
texture
