hello! 

I am writing a rust crate called caffe2-aten. 

It is part of workspace containing the rust
translation of the caffe2 operator library.

I would like you to help me write a rust crate
description for it.

Please write the description in the following format:

<short description tag used in the crate header>
<double newline>
<longer, in depth description and mathematical analysis>
<END>

Please indicate that the crate is in the
process of being translated from C++ to
Rust. It is possible some of the function
bodies are in the process of translation.

Below are listed some tokens from this crate. 

Please describe the mathematical ideas
identified by this information alongside the
relevant mathematical equations in unicode.

thanks!

please format your response in the Markdown file format.

here are the tokens:
pytorch_u8maxpool_ukernel_sub16_neon
cat_out_quantized_cpu
cat_quantized_cpu
define_dispatch
is_cat_nhwc_fast_path
is_valid_quantization_scheme
lazy_static
qcat
qcat_out
quantized_cat_impl
StackFunc
require_equal_tensor_list
test
test_chunk
test_matmul
test_native_cpu
test_native_gpu
test_size
test_split
test_stack_a
test_stack_b
test_standard_gamma_grad
test_where
pytorch_qnnp_ukernel_type
lazy_static
max_pool2d
use_max_pool2d
average_pooling_op_setup_changing_height
average_pooling_op_setup_changing_width
average_pooling_op_setup_decreasing_batch
average_pooling_op_setup_increasing_batch
average_pooling_op_setup_swap_height_and_width
average_pooling_op_small_batch_few_channels
average_pooling_op_small_batch_many_channels_pool
average_pooling_op_unit_batch_few_channels_1xm_pool
average_pooling_op_unit_batch_few_channels_1xm_pool_with_padding
average_pooling_op_unit_batch_few_channels_1xm_pool_with_stride
average_pooling_op_unit_batch_few_channels_mx1_pool
average_pooling_op_unit_batch_few_channels_mx1_pool_with_padding
average_pooling_op_unit_batch_few_channels_mx1_pool_with_stride
average_pooling_op_unit_batch_few_channels_with_qmax
average_pooling_op_unit_batch_few_channels_with_qmin
average_pooling_op_unit_batch_many_channels_small_1xm_pool
average_pooling_op_unit_batch_many_channels_small_1xm_pool_with_padding
average_pooling_op_unit_batch_many_channels_small_1xm_pool_with_stride
average_pooling_op_unit_batch_many_channels_small_mx1_pool
average_pooling_op_unit_batch_many_channels_small_mx1_pool_with_padding
average_pooling_op_unit_batch_many_channels_small_mx1_pool_with_stride
average_pooling_op_unit_batch_many_channels_small_pool_with_qmax
average_pooling_op_unit_batch_many_channels_small_pool_with_qmin
average_pooling_op_zero_batch
OpaqueTensorImpl
copy_tensor_metadata
has_storage
opaque_handle
release_resources
set_size
set_storage_offset
set_stride
shallow_copy_and_detach
shallow_copy_from
stride
strides
tensorimpl_type_name
unsafe_opaque_handle
c10_register_guard_impl
CholeskyFn
CholeskyInverseFn
EigFn
GeqrfFn
Int
LapackLstsqDriverType
LinalgEigFn
LinalgEighFn
LstsqFn
LuFn
LuSolveFn
OrgqrFn
OrmqrFn
TriangularSolveFn
apply_cholesky_solve
apply_inverse
apply_lstsq
apply_solve
apply_svd
apply_symeig
cholesky
cholesky_inverse
cholesky_inverse_out
cholesky_inverse_out_info
cholesky_out
cholesky_solve
cholesky_solve_helper_cpu
cholesky_solve_out
declare_dispatch
define_dispatch
dispatching
eig
eig_out
for
geqrf
geqrf_out
geqrf_out_helper
get_default_lstsq_driver
householder_product_out_helper
inverse
inverse_helper_cpu
inverse_out
lapackLstsq_impl
lapack_cholesky
lapack_cholesky_complex_double
lapack_cholesky_complex_float
lapack_cholesky_double
lapack_cholesky_float
lapack_cholesky_inverse
lapack_cholesky_inverse_complex_double
lapack_cholesky_inverse_complex_float
lapack_cholesky_inverse_double
lapack_cholesky_inverse_float
lapack_cholesky_solve
lapack_cholesky_solve_complex_double
lapack_cholesky_solve_complex_float
lapack_cholesky_solve_double
lapack_cholesky_solve_float
lapack_eig
lapack_eig_complex_double_double
lapack_eig_complex_float_float
lapack_eig_double
lapack_eig_float
lapack_gels
lapack_gels_complex_double
lapack_gels_complex_float
lapack_gels_double
lapack_gels_float
lapack_gelsd
lapack_gelsd_complex_double_double
lapack_gelsd_complex_float_float
lapack_gelsd_double
lapack_gelsd_float
lapack_gelss
lapack_gelss_complex_double_double
lapack_gelss_complex_float_float
lapack_gelss_double
lapack_gelss_float
lapack_gelsy
lapack_gelsy_complex_double_double
lapack_gelsy_complex_float_float
lapack_gelsy_double
lapack_gelsy_float
lapack_geqrf
lapack_geqrf_complex_double
lapack_geqrf_complex_float
lapack_geqrf_double
lapack_geqrf_float
lapack_getri
lapack_getri_complex_double
lapack_getri_complex_float
lapack_getri_double
lapack_getri_float
lapack_lstsq
lapack_lu
lapack_lu_complex_double
lapack_lu_complex_float
lapack_lu_double
lapack_lu_float
lapack_lu_solve
lapack_lu_solve_complex_double
lapack_lu_solve_complex_float
lapack_lu_solve_double
lapack_lu_solve_float
lapack_orgqr
lapack_orgqr_complex_double
lapack_orgqr_complex_float
lapack_orgqr_double
lapack_orgqr_float
lapack_ormqr
lapack_ormqr_complex_double
lapack_ormqr_complex_float
lapack_ormqr_double
lapack_ormqr_float
lapack_solve
lapack_solve_complex_double
lapack_solve_complex_float
lapack_solve_double
lapack_solve_float
lapack_svd
lapack_svd_complex_double_double
lapack_svd_complex_float_float
lapack_svd_double
lapack_svd_float
lapack_syevd
lapack_syevd_complex_double_double
lapack_syevd_complex_float_float
lapack_syevd_double
lapack_syevd_float
lapack_symeig
lapack_symeig_complex_double_double
lapack_symeig_complex_float_float
lapack_symeig_double
lapack_symeig_float
lapack_triangular_solve
lapack_triangular_solve_complex_double
lapack_triangular_solve_complex_float
lapack_triangular_solve_double
lapack_triangular_solve_float
lazy_static
legacy_lstsq
legacy_lstsq_out
linalg_cholesky
linalg_cholesky_ex
linalg_cholesky_ex_out
linalg_cholesky_out
linalg_cholesky_out_info
linalg_eig
linalg_eig_make_complex_eigenvectors
linalg_eig_make_complex_eigenvectors_impl
linalg_eig_out
linalg_eig_out_info
linalg_eigh
linalg_eigh_out
linalg_eigh_out_info
linalg_eigvals
linalg_eigvals_out
linalg_eigvalsh
linalg_eigvalsh_out
linalg_householder_product
linalg_householder_product_out
linalg_inv
linalg_inv_ex
linalg_inv_ex_out
linalg_inv_out
linalg_inv_out_helper_cpu
linalg_inv_out_info
linalg_lstsq
linalg_lstsq_out
linalg_lstsq_out_info
linalg_qr
linalg_qr_helper_default
linalg_qr_out
linalg_qr_out_helper
linalg_solve
linalg_solve_out
linalg_solve_out_helper_cpu
linalg_solve_out_info
linalg_svd
linalg_svd_out
linalg_svdvals
linalg_svdvals_out
lu_solve
lu_solve_out
lu_with_info
orgqr
orgqr_out
ormqr
ormqr_out
ormqr_out_helper
promotion
qr
qr_out
solve
solve_helper_cpu
solve_out
svd
svd_helper_cpu
svd_out
svd_resize_and_copy
symeig
symeig_helper_cpu
symeig_out
triangular_solve
triangular_solve_out
triangular_solve_out_info
set_cuda
set_storage_cuda
operators_test_aten_fn
operators_test_function_decltype
operators_test_method_only_decltype
operators_test_out_variant_is_faithful
pass_through_wrapper
can_use_fast_route_a
can_use_fast_route_b
check_fast_path_restrictions
check_foreach_api_restrictions_a
check_foreach_api_restrictions_b
check_foreach_api_restrictions_c
check_foreach_api_restrictions_d
check_foreach_api_restrictions_e
has_bool_tensor
has_integral_tensor
promotion
will_promote_tensor
Argument
FunctionSchema
N
alias_analysis
alias_info
check_schema
clone_with_name
clone_with_remapped_types
clone_with_returns
clone_with_type
default_value
dump
eq
fmt
format_type_mismatch_msg
get_namespace
has_any_alias_info
inherits
is_backward_compatible_with
is_default_alias_analysis_kind
is_inferred_type
is_mutable
is_subtype_of
is_varret
must
name
operator_name
overload_name
returns
set_alias_analysis
set_namespace_if_not_set
should
to_string
ty
lazy_static
CuSolverDnPoolType
create_cusolver_dn_handle
destroy_cusolver_dn_handle
get_current_cuda_solver_dn_handle
fully_connected_op_integration_test_dynamic
fully_connected_op_integration_test_dynamic_per_channel
fully_connected_op_integration_test_runtime
fully_connected_op_integration_test_runtime_per_channel
fully_connected_op_integration_test_static
fully_connected_op_integration_test_static_per_channel
fully_connected_op_small_batch_dynamic
fully_connected_op_small_batch_dynamic_per_channel
fully_connected_op_small_batch_runtime
fully_connected_op_small_batch_runtime_per_channel
fully_connected_op_small_batch_static
fully_connected_op_small_batch_static_per_channel
fully_connected_op_small_batch_with_qmax
fully_connected_op_small_batch_with_qmax_dynamic
fully_connected_op_small_batch_with_qmax_dynamic_per_channel
fully_connected_op_small_batch_with_qmax_per_channel
fully_connected_op_small_batch_with_qmax_runtime
fully_connected_op_small_batch_with_qmax_runtime_per_channel
fully_connected_op_small_batch_with_qmin_dynamic
fully_connected_op_small_batch_with_qmin_dynamic_per_channel
fully_connected_op_small_batch_with_qmin_runtime
fully_connected_op_small_batch_with_qmin_runtime_per_channel
fully_connected_op_small_batch_with_qmin_static
fully_connected_op_small_batch_with_qmin_static_per_channel
fully_connected_op_unit_batch_dynamic
fully_connected_op_unit_batch_dynamic_per_channel
fully_connected_op_unit_batch_runtime
fully_connected_op_unit_batch_runtime_per_channel
fully_connected_op_unit_batch_static
fully_connected_op_unit_batch_static_per_channel
fully_connected_op_unit_batch_with_qmax_dynamic
fully_connected_op_unit_batch_with_qmax_dynamic_per_channel
fully_connected_op_unit_batch_with_qmax_runtime
fully_connected_op_unit_batch_with_qmax_runtime_per_channel
fully_connected_op_unit_batch_with_qmax_static
fully_connected_op_unit_batch_with_qmax_static_per_channel
fully_connected_op_unit_batch_with_qmin_dynamic
fully_connected_op_unit_batch_with_qmin_dynamic_per_channel
fully_connected_op_unit_batch_with_qmin_runtime
fully_connected_op_unit_batch_with_qmin_runtime_per_channel
fully_connected_op_unit_batch_with_qmin_static
fully_connected_op_unit_batch_with_qmin_static_per_channel
fully_connected_op_zero_batch_dynamic
fully_connected_op_zero_batch_dynamic_per_channel
fully_connected_op_zero_batch_runtime
fully_connected_op_zero_batch_runtime_per_channel
fully_connected_op_zero_batch_static
fully_connected_op_zero_batch_static_per_channel
lazy_static
pytorch_q8conv_ukernel_4x4c2_sse2
declare_dispatch
define_dispatch
fill_a
fill_b
fill_diagonal
fill_meta_a
fill_meta_b
fill_out_a
fill_out_b
zero
zero_cpu
zero_meta
define_dispatch
lazy_static
upsample_nearest2d
upsample_nearest2d_backward
PytorchRequantizationFunction
declare_pytorch_requantization_function
Block
Conv2dMethod
Conv2dOpContext
Conv2dOpContextPacked
Conv2dOpContextUnpacked
Image3D
State
TransformBlock
all_lessthan
available
conv2d
conv2d_clamp_prepack
conv2d_clamp_run
conv2d_dw
conv2d_old
conv2d_pw
conv2d_winograd_2_3
convolution
create
determine_method
is_depthwise
is_pointwise
is_winograd_n_3
lazy_static
pack_biases
pack_filter
pack_params
pack_weights
pack_weights_2d
pack_weights_2d_old
pack_weights_2d_winograd_2_3
pack_weights_dw
run
unpack
usable
pytorch_qnnp_requantize_gemmlowp_neon
pytorch_u8maxpool_ukernel_16x9p8q_sse2
bilinear
einsum
einsum_check_label
einsum_index_to_label
einsum_label_to_index
linear
promotion
sumproduct_pair
tensordot
tensordot_out
trilinear
pytorch_q8conv_ukernel_8x8_neon
ConstStridedRandomAccessor
DefaultPtrTraits
RestrictPtrTraits
StridedRandomAccessor
Todo
lazy_static
expand_scale
miopen_batch_norm
miopen_batch_norm_backward
pixel_shuffle
pixel_unshuffle
gather_shape_check
scatter_gather_dtype_check
scatter_shape_check
InvStd
RenormScaleFactorFn
Var
batch_norm
batch_norm_backward_cpu
batch_norm_backward_cpu_template
batch_norm_cpu
batch_norm_cpu_update_stats_template
batch_norm_impl_index
batch_norm_impl_index_backward
batch_norm_update_stats_cpu
declare_dispatch
define_dispatch
instance_norm
invoke
is_contiguous
lazy_static
repeat_if_defined
add_clamp_kernel
add_kernel
atan2_kernel
bitwise_and_kernel
bitwise_or_kernel
bitwise_xor_kernel
copysign_kernel
div_floor_kernel
div_true_kernel
div_trunc_kernel
does
eq_kernel
fmax_kernel
fmin_kernel
fmod_kernel
gcd_kernel
ge_kernel
gt_kernel
heaviside_kernel
huber_kernel
hypot_kernel
igamma_kernel
igammac_kernel
lcm_kernel
le_kernel
logaddexp2_kernel
logaddexp_kernel
logical_and_kernel
logical_or_kernel
logical_xor_kernel
logit_backward_kernel
lshift_kernel
lt_kernel
maximum_kernel
minimum_kernel
mse_kernel
mul_kernel
ne_kernel
nextafter_kernel
register_dispatch
remainder_kernel
rshift_kernel
sigmoid_backward_kernel
smooth_l1_kernel
sub_kernel
tanh_backward_kernel
tensors
xlog1py_kernel
xlogy_kernel
lazy_static
random_tensor_for_type
tensor_iterator_test_cpu_scalar
tensor_iterator_test_fail_non_promoting_binary_op
tensor_iterator_test_mixed_devices
tensor_iterator_test_serial_loop_single_thread
Entry
lazy_static
qual_name_for_entry
apply
apply_dynamic
apply_dynamic_relu
apply_out
apply_relu
apply_relu_out
bias
set_bias
unpack
Enum1
Enum2
Enum3
EnumName
func
invoke
lazy_static
variant_test_basic
pytorch_sgemm_ukernel_6x8_neon
VAddMicrokernelTester
a_scale
a_zero_point
b_scale
b_zero_point
inplacea
inplaceb
iterations
n
qmax
qmin
test
y_scale
y_zero_point
as_strided_batching_rule
binary_pointwise_batching_rule
bmm_batching_rule
cat_batching_rule
check_basic_as_strided_valid_for_slice
check_batch_dims_at_front_in_layout
chunk_batching_rule
clamp_batching_rule
clamp_max_batching_rule
clamp_min_batching_rule
clone_batching_rule
comparison_pointwise_batching_rule
contiguous_batching_rule
diagonal_backward_batching_rule
diagonal_batching_rule
dot_batching_rule
expand_batching_rule
fill_inplace_scalar_batching_rule
fill_inplace_tensor_batching_rule
is_allowed_dim_on_scalar_tensor
is_physical_scalar_tensor
lazy_static
maximum_indexable_location
mm_batching_rule
movedim_batching_rule
mv_batching_rule
permute_batching_rule
pow_scalar_tensor_batching_rule
reshape_batching_rule
select_backward_batching_rule
select_batching_rule
slice_backward_batching_rule
slice_batching_rule
split_batching_rule
split_with_sizes_batching_rule
squeeze_batching_rule
squeeze_dim_batching_rule
stack_batching_rule
sum_batching_rule
tensor_split_indices_batching_rule
tensor_split_sections_batching_rule
to_dtype_layout_batching_rule
trace_backward_batching_rule
trace_batching_rule
transpose_int_batching_rule
unbind_batching_rule
unfold_batching_rule
unsqueeze_batching_rule
unwrap_and_call
unwrap_and_call_method
view_as_complex_batching_rule
view_batching_rule
zero_inplace_batching_rule
max_pooling_op_setup_changing_height
max_pooling_op_setup_changing_width
max_pooling_op_setup_decreasing_batch
max_pooling_op_setup_increasing_batch
max_pooling_op_setup_swap_height_and_width
max_pooling_op_small_batch_few_channels
max_pooling_op_small_batch_many_channels_pool
max_pooling_op_unit_batch_few_channels_1xm_pool
max_pooling_op_unit_batch_few_channels_1xm_pool_with_dilation
max_pooling_op_unit_batch_few_channels_1xm_pool_with_padding
max_pooling_op_unit_batch_few_channels_1xm_pool_with_stride
max_pooling_op_unit_batch_few_channels_mx1_pool
max_pooling_op_unit_batch_few_channels_mx1_pool_with_dilation
max_pooling_op_unit_batch_few_channels_mx1_pool_with_padding
max_pooling_op_unit_batch_few_channels_mx1_pool_with_stride
max_pooling_op_unit_batch_few_channels_with_qmax
max_pooling_op_unit_batch_few_channels_with_qmin
max_pooling_op_unit_batch_many_channels_small_1xm_pool
max_pooling_op_unit_batch_many_channels_small_1xm_pool_with_dilation
max_pooling_op_unit_batch_many_channels_small_1xm_pool_with_padding
max_pooling_op_unit_batch_many_channels_small_1xm_pool_with_stride
max_pooling_op_unit_batch_many_channels_small_mx1_pool
max_pooling_op_unit_batch_many_channels_small_mx1_pool_with_dilation
max_pooling_op_unit_batch_many_channels_small_mx1_pool_with_padding
max_pooling_op_unit_batch_many_channels_small_mx1_pool_with_stride
max_pooling_op_unit_batch_many_channels_small_pool_with_qmax
max_pooling_op_unit_batch_many_channels_small_pool_with_qmin
max_pooling_op_zero_batch
PTThreadPool
define_dispatch
lazy_static
upsample_nearest3d_backward_cpu
upsample_nearest3d_cpu
MemOverlap
MemOverlapStatus
assert_no_internal_overlap
assert_no_overlap
assert_no_partial_overlap
get_overlap_status
has_internal_overlap
check_unify
check_unify_error
dimname_from_string
dimnames_equal
named_tensor_test_alias
named_tensor_test_attach_metadata
named_tensor_test_dimname_to_position
named_tensor_test_empty
named_tensor_test_internal_set_names_inplace
named_tensor_test_is
named_tensor_test_name_print
named_tensor_test_names_check_unique
named_tensor_test_no_names_guard
named_tensor_test_unify_from_right
nchw
tensornames_unify_from_right
declare_dispatch
end_index
lazy_static
start_index
dim_list_to_bitset
custom_class_methods
custom_class_schemas_for_bcc_heck
custom_classes
get_custom_class
is_custom_class
register_custom_class
register_custom_class_method
ResultTypeState
can_cast
combine_categories
has_compatible_shallow_copy_type
is_complex
is_conj
is_cuda
is_distributed
is_floating_point
is_inference
is_quantized
is_signed
is_sparse
is_sparse_csr
promote_skip_undefined
promote_types
result_type_a
result_type_b
result_type_c
result_type_d
result_type_e
result_type_f
so
type_as
update_result_type_state_a
update_result_type_state_b
lazy_static
pytorch_pack_hgemm_w
pytorch_pack_q8conv_wdq
pytorch_pack_q8conv_wrq
pytorch_pack_q8deconv_wdq
pytorch_pack_q8deconv_wrq
pytorch_pack_q8dw_w_dilation
pytorch_pack_q8dw_wdq
pytorch_pack_q8dw_wrq
pytorch_pack_q8gemm_wdq
pytorch_pack_q8gemm_wrq
pytorch_pack_sconv_w
pytorch_pack_sgemm_w
pytorch_pack_swizzle_q8gemm_bdq
pytorch_pack_swizzle_q8gemm_brq
ClampFn
IsInfinityOpFn
IsinDefaultFn
ModeFn
ReduceMinmaxFn
WhereFn
allclose
aminmax
aminmax_out_impl
assert_async_cpu
check_for_unsupported_isin_dtype
clamp_a
clamp_b
clamp_c
clamp_d
clamp_max_a
clamp_max_b
clamp_max_c
clamp_max_d
clamp_max_out_a
clamp_max_out_b
clamp_min_a
clamp_min_b
clamp_min_c
clamp_min_d
clamp_min_out_a
clamp_min_out_b
clamp_out_a
clamp_out_b
clip_a
clip_b
clip_c
clip_d
clip_out_a
clip_out_b
declare_dispatch
define_dispatch
encountered
is_nonzero
isclose
isfinite
isin_sorting
isinf
isnan
isneginf
isneginf_out
isposinf
isposinf_out
isreal
lazy_static
max_a
max_b
max_out_a
max_out_b
max_out_impl
min_a
min_b
min_out_a
min_out_b
min_out_impl
mode_a
mode_b
mode_out_a
mode_out_b
promotion
s_where
scalar_to_tensor_default_dtype
then
where_a
where_b
where_c
where_d
where_e
wrapped_scalar_tensor_default_dtype
lazy_static
THCIpcDeleter
delete_thc_ipc_deleter
make_data_ptr
adaptive_avg_pool
adaptive_avg_pool2d_quantized_cpu
adaptive_avg_pool3d_out_quantized_cpu
adaptive_avg_pool3d_quantized_cpu
adaptive_avg_pool_single_out_frame
define_dispatch
enable_qnnpack_for_ada_avgpool
end_index
q_adaptive_avg_pool2d
q_adaptive_avg_pool3d
qnnpack_adaptive_avg_pool2d
start_index
conv2d
prepack_conv2d
hardswish_q8
lazy_static
_stub_1
cu_launch_kernel
cu_link_add_data
cu_module_load_data_ex
cuda_stub1
cuda_stub2
cuda_stub3
cuda_stub4
get_alt_lib_name
get_cuda_library
get_lib_name
get_lib_version
get_nvrtc_library
lazy_static
nvrtc_create_program
nvrtc_stub1
nvrtc_stub2
nvrtc_stub3
MetaAllocator
allocate
deleter
empty_meta
empty_strided_meta
get_meta_allocator
lazy_static
raw_deleter
ContextDeleter
TensorMaker
compute_storage_size
conj
context
conv1d
conv2d
conv3d
deleter
for_blob
from_blob_a
from_blob_b
from_blob_c
from_blob_d
is_complex
is_conj
is_floating_point
is_inference
is_signed
lazy_static
make_data_ptr_from_context
make_data_ptr_from_deleter
make_temp_sizes
make_tensor
noop_delete
numel
options
size
std
std_mean
stride
strides
var
var_mean
lazy_static
MaxPoolMicrokernelTester
iterations
kc
kh
kr
ks
kw
mr
n
packed_ks
packedn
qmax
qmin
qr
s
test
x_stride
y_stride
MetalImplRegistrar
is_metal_available
lazy_static
metal_copy
cpublas_axpy_impl
cpublas_copy_impl
cpublas_gemm_impl
gemm_core
gemm_notrans
gemm_transa
gemm_transab
gemm_transb
register_dispatch
scale
NeuronType
neuron_type
neuron_type_with_range
LerpFnScalar
LerpFnTensor
declare_dispatch
define_dispatch
lerp_cpu_scalar_a
lerp_cpu_scalar_b
lerp_cpu_scalar_out
lerp_cpu_tensor_a
lerp_cpu_tensor_b
lerp_cpu_tensor_out
check_long_tensor
pack_padded_sequence
pack_padded_sequence_backward
pad_packed_sequence
pad_sequence
generate_sizes
lazy_static
tensor_add
cusolver_get_error_message
geqrf
geqrf_buffer_size
geqrf_buffer_size_complex_double
geqrf_buffer_size_complex_float
geqrf_buffer_size_double
geqrf_buffer_size_float
geqrf_complex_double
geqrf_complex_float
geqrf_double
geqrf_float
gesvdj
gesvdj_batched
gesvdj_batched_complex_double
gesvdj_batched_complex_float
gesvdj_batched_double
gesvdj_batched_float
gesvdj_complex_double
gesvdj_complex_float
gesvdj_double
gesvdj_float
get_cusolver_datatype
get_cusolver_datatype_complex_double
get_cusolver_datatype_complex_float
get_cusolver_datatype_double
get_cusolver_datatype_float
getrf
getrf_complex_double
getrf_complex_float
getrf_double
getrf_float
getrs
getrs_complex_double
getrs_complex_float
getrs_double
getrs_float
orgqr
orgqr_buffersize
orgqr_buffersize_complex_double
orgqr_buffersize_complex_float
orgqr_buffersize_double
orgqr_buffersize_float
orgqr_complex_double
orgqr_complex_float
orgqr_double
orgqr_float
ormqr
ormqr_buffer_size
ormqr_buffer_size_complex_double
ormqr_buffer_size_complex_float
ormqr_buffer_size_double
ormqr_buffer_size_float
ormqr_complex_double
ormqr_complex_float
ormqr_double
ormqr_float
potrf
potrf_batched
potrf_batched_complex_double
potrf_batched_complex_float
potrf_batched_double
potrf_batched_float
potrf_buffersize
potrf_buffersize_complex_double
potrf_buffersize_complex_float
potrf_buffersize_double
potrf_buffersize_float
potrf_complex_double
potrf_complex_float
potrf_double
potrf_float
potrs
potrs_batched
potrs_batched_complex_double
potrs_batched_complex_float
potrs_batched_double
potrs_batched_float
potrs_complex_double
potrs_complex_float
potrs_double
potrs_float
syevd
syevd_buffer_size
syevd_buffer_size_complex_double2
syevd_buffer_size_complex_float2
syevd_buffer_size_double
syevd_buffer_size_float
syevd_complex_double2
syevd_complex_float2
syevd_double
syevd_float
syevj
syevj_batched
syevj_batched_buffer_size
syevj_batched_buffer_size_complex_double2
syevj_batched_buffer_size_complex_float2
syevj_batched_buffer_size_double
syevj_batched_buffer_size_float
syevj_batched_complex_double2
syevj_batched_complex_float2
syevj_batched_double
syevj_batched_float
syevj_buffer_size
syevj_buffer_size_complex_double2
syevj_buffer_size_complex_float2
syevj_buffer_size_double
syevj_buffer_size_float
syevj_complex_double2
syevj_complex_float2
syevj_double
syevj_float
xgeqrf
xgeqrf_buffer_size
xgeqrf_buffer_size_complex_double
xgeqrf_buffer_size_complex_float
xgeqrf_buffer_size_double
xgeqrf_buffer_size_float
xgeqrf_complex_double
xgeqrf_complex_float
xgeqrf_double
xgeqrf_float
xpotrf
xpotrf_buffersize
xpotrs
xsyevd
xsyevd_buffer_size
xsyevd_buffer_size_complex_double2
xsyevd_buffer_size_complex_float2
xsyevd_buffer_size_double
xsyevd_buffer_size_float
xsyevd_complex_double2
xsyevd_complex_float2
xsyevd_double
xsyevd_float
rowwise_prune
rowwise_prune_helper
empty_affine_quantized
empty_affine_quantized_other_backends_stub
empty_per_channel_affine_quantized
empty_per_channel_affine_quantized_other_backends_stub
empty_quantized
for
tensor_dim_apply3
add_batch_dim
has_level
movedim
remove_batch_dim
remove_existing_batch_dim
angle_impl
ceil_impl
conj_impl
floor_impl
imag_impl
max_impl
min_impl
real_impl
round_impl
sgn_impl
trunc_impl
zabs
AddOperatorTester
a_scale
a_stride
a_zero_point
b_scale
b_stride
b_zero_point
batch_size
channels
iterations
qmax
qmin
testq8
y_scale
y_stride
y_zero_point
ClampMicrokernelTester
inplace
iterations
n
qmax
qmin
test
define_dispatch
lazy_static
upsample_nearest1d
upsample_nearest1d_backward
GridSamplerInterpolation
GridSamplerPadding
add_value_bounded
clip_coordinates
clip_coordinates_set_grad
compute_coordinates
define_dispatch
get_cubic_coefficients_grad
get_value_bounded
grid_sampler
grid_sampler_2d_backward_cpu
grid_sampler_2d_cpu
grid_sampler_2d_cpu_fallback
grid_sampler_2d_cpu_fallback_backward
grid_sampler_3d_backward_cpu
grid_sampler_3d_backward_cpu_impl
grid_sampler_3d_cpu
grid_sampler_3d_cpu_impl
grid_sampler_compute_source_index
grid_sampler_compute_source_index_set_grad
grid_sampler_unnormalize
grid_sampler_unnormalize_set_grad
reflect_coordinates
reflect_coordinates_set_grad
safe_add_2d
safe_add_3d
within_bounds_2d
within_bounds_3d
QEmbedding
QEmbeddingBag
combination
create_empty_from
embedding_bag_4bit_helper
embedding_bag_4bit_impl
embedding_bag_4bit_rowwise_offsets
embedding_bag_4bit_rowwise_offsets_out
embedding_bag_byte_helper
embedding_bag_byte_impl
embedding_bag_byte_rowwise_offsets
embedding_bag_byte_rowwise_offsets_out
embedding_lookup_fallback_impl
embeddingbag_4bit
embeddingbag_byte
lazy_static
register_embedding_params
run
cross_entropy_loss
nll_loss
nll_loss_backward_cpu
nll_loss_backward_out_cpu
nll_loss_backward_out_cpu_template
nll_loss_backward_out_frame
nll_loss_forward_cpu
nll_loss_forward_out_cpu
nll_loss_forward_out_cpu_template
nll_loss_nd
nll_loss_out
nll_loss_out_frame
optional_contiguous
optional_data
unique2_cpu
unique_consecutive_cpu
unique_consecutive_cpu_template
unique_cpu
unique_cpu_template
unique_dim_consecutive_cpu
unique_dim_cpu
unique_dim_cpu_impl
unique_dim_cpu_template
sub_zero_point
cast
convert_to_int_of_same_size
deinterleave2
fmt
interleave2
lazy_static
Scale
UpsamplingBicubic2d
UpsamplingBilinear2d
UpsamplingLinear1d
UpsamplingNearest1d
UpsamplingNearest2d
UpsamplingNearest3d
UpsamplingTrilinear3d
area_pixel_compute_scale
area_pixel_compute_source_index
compute_scales_value
compute_source_index_and_lambda
cubic_convolution1
cubic_convolution2
cubic_interp1d
declare_dispatch
get_cubic_upsample_coefficients
nearest_neighbor_compute_source_index
upsample_1d_common_check
upsample_2d_common_check
upsample_2d_shape_check
upsample_3d_common_check
upsample_get_scale_value
upsample_get_value_bounded
upsample_increment_value_bounded
exec_fft
fft_c2c_mkl
fft_c2c_mkl_out
fft_c2r_mkl
fft_c2r_mkl_out
fft_fill_with_conjugate_symmetry_cpu
fft_fill_with_conjugate_symmetry_slice
fft_r2c_mkl
fft_r2c_mkl_out
plan_mkl_fft
sort_dims
pytorch_q8gavgpool_ukernel_up8xm_neon
LoadImpl
accumulate_result
accumulate_result_array
lazy_static
load
multi_row_sum
register_dispatch
row_sum
scalar_inner_sum
scalar_outer_sum
sum_kernel_impl
vectorized_inner_sum
vectorized_outer_sum
conv_tbc
conv_tbc_backward
cuda_caffe_2to_pytorch_op
cuda_caffe_2to_pytorch_simple
cuda_caffe_2to_pytorch_simple_legacy
cuda_get
cuda_pytorch_to_caffe2_mutual_resizes
cuda_pytorch_to_caffe2_op
cuda_pytorch_to_caffe2_shared_storage_write
cuda_set
q8gavgpool_mp8x7p7q_neon_n_div_8_2pass_all_m
q8gavgpool_mp8x7p7q_neon_n_div_8_2pass_few_m
q8gavgpool_mp8x7p7q_neon_n_div_8_multipass_all_m
q8gavgpool_mp8x7p7q_neon_n_div_8_multipass_all_m_with_x_stride
q8gavgpool_mp8x7p7q_neon_n_eq_8_2pass_all_m
q8gavgpool_mp8x7p7q_neon_n_eq_8_2pass_all_m_with_x_scale
q8gavgpool_mp8x7p7q_neon_n_eq_8_2pass_all_m_with_x_stride
q8gavgpool_mp8x7p7q_neon_n_eq_8_2pass_all_m_with_x_zero_point
q8gavgpool_mp8x7p7q_neon_n_eq_8_2pass_all_m_with_y_max
q8gavgpool_mp8x7p7q_neon_n_eq_8_2pass_all_m_with_y_min
q8gavgpool_mp8x7p7q_neon_n_eq_8_2pass_all_m_with_y_scale
q8gavgpool_mp8x7p7q_neon_n_eq_8_2pass_all_m_with_y_zero_point
q8gavgpool_mp8x7p7q_neon_n_eq_8_2pass_few_m
q8gavgpool_mp8x7p7q_neon_n_eq_8_2pass_few_m_with_x_stride
q8gavgpool_mp8x7p7q_neon_n_eq_8_multipass_all_m
q8gavgpool_mp8x7p7q_neon_n_eq_8_multipass_all_m_with_x_stride
q8gavgpool_mp8x7p7q_neon_n_gt_8_2pass_all_m
q8gavgpool_mp8x7p7q_neon_n_gt_8_2pass_all_m_with_x_scale
q8gavgpool_mp8x7p7q_neon_n_gt_8_2pass_all_m_with_x_zero_point
q8gavgpool_mp8x7p7q_neon_n_gt_8_2pass_all_m_with_y_max
q8gavgpool_mp8x7p7q_neon_n_gt_8_2pass_all_m_with_y_min
q8gavgpool_mp8x7p7q_neon_n_gt_8_2pass_all_m_with_y_scale
q8gavgpool_mp8x7p7q_neon_n_gt_8_2pass_all_m_with_y_zero_point
q8gavgpool_mp8x7p7q_neon_n_gt_8_2pass_few_m
q8gavgpool_mp8x7p7q_neon_n_gt_8_multipass_all_m
q8gavgpool_mp8x7p7q_neon_n_gt_8_multipass_all_m_with_x_stride
q8gavgpool_mp8x7p7q_sse2_n_div_8_2pass_all_m
q8gavgpool_mp8x7p7q_sse2_n_div_8_2pass_few_m
q8gavgpool_mp8x7p7q_sse2_n_div_8_multipass_all_m
q8gavgpool_mp8x7p7q_sse2_n_div_8_multipass_all_m_with_x_stride
q8gavgpool_mp8x7p7q_sse2_n_eq_8_2pass_all_m
q8gavgpool_mp8x7p7q_sse2_n_eq_8_2pass_all_m_with_x_scale
q8gavgpool_mp8x7p7q_sse2_n_eq_8_2pass_all_m_with_x_stride
q8gavgpool_mp8x7p7q_sse2_n_eq_8_2pass_all_m_with_x_zero_point
q8gavgpool_mp8x7p7q_sse2_n_eq_8_2pass_all_m_with_y_max
q8gavgpool_mp8x7p7q_sse2_n_eq_8_2pass_all_m_with_y_min
q8gavgpool_mp8x7p7q_sse2_n_eq_8_2pass_all_m_with_y_scale
q8gavgpool_mp8x7p7q_sse2_n_eq_8_2pass_all_m_with_y_zero_point
q8gavgpool_mp8x7p7q_sse2_n_eq_8_2pass_few_m
q8gavgpool_mp8x7p7q_sse2_n_eq_8_2pass_few_m_with_x_stride
q8gavgpool_mp8x7p7q_sse2_n_eq_8_multipass_all_m
q8gavgpool_mp8x7p7q_sse2_n_eq_8_multipass_all_m_with_x_stride
q8gavgpool_mp8x7p7q_sse2_n_gt_8_2pass_all_m
q8gavgpool_mp8x7p7q_sse2_n_gt_8_2pass_all_m_with_x_scale
q8gavgpool_mp8x7p7q_sse2_n_gt_8_2pass_all_m_with_x_zero_point
q8gavgpool_mp8x7p7q_sse2_n_gt_8_2pass_all_m_with_y_max
q8gavgpool_mp8x7p7q_sse2_n_gt_8_2pass_all_m_with_y_min
q8gavgpool_mp8x7p7q_sse2_n_gt_8_2pass_all_m_with_y_scale
q8gavgpool_mp8x7p7q_sse2_n_gt_8_2pass_all_m_with_y_zero_point
q8gavgpool_mp8x7p7q_sse2_n_gt_8_2pass_few_m
q8gavgpool_mp8x7p7q_sse2_n_gt_8_multipass_all_m
q8gavgpool_mp8x7p7q_sse2_n_gt_8_multipass_all_m_with_x_stride
q8gavgpool_up8x7_neon_n_div_8_all_m
q8gavgpool_up8x7_neon_n_div_8_few_m
q8gavgpool_up8x7_neon_n_eq_8_all_m
q8gavgpool_up8x7_neon_n_eq_8_all_m_with_x_scale
q8gavgpool_up8x7_neon_n_eq_8_all_m_with_x_stride
q8gavgpool_up8x7_neon_n_eq_8_all_m_with_x_zero_point
q8gavgpool_up8x7_neon_n_eq_8_all_m_with_y_max
q8gavgpool_up8x7_neon_n_eq_8_all_m_with_y_min
q8gavgpool_up8x7_neon_n_eq_8_all_m_with_y_scale
q8gavgpool_up8x7_neon_n_eq_8_all_m_with_y_zero_point
q8gavgpool_up8x7_neon_n_eq_8_few_m
q8gavgpool_up8x7_neon_n_gt_8_all_m
q8gavgpool_up8x7_neon_n_gt_8_all_m_with_x_scale
q8gavgpool_up8x7_neon_n_gt_8_all_m_with_x_zero_point
q8gavgpool_up8x7_neon_n_gt_8_all_m_with_y_max
q8gavgpool_up8x7_neon_n_gt_8_all_m_with_y_min
q8gavgpool_up8x7_neon_n_gt_8_all_m_with_y_scale
q8gavgpool_up8x7_neon_n_gt_8_all_m_with_y_zero_point
q8gavgpool_up8x7_neon_n_gt_8_few_m
q8gavgpool_up8x7_sse2_n_div_8_all_m
q8gavgpool_up8x7_sse2_n_div_8_few_m
q8gavgpool_up8x7_sse2_n_eq_8_all_m
q8gavgpool_up8x7_sse2_n_eq_8_all_m_with_x_scale
q8gavgpool_up8x7_sse2_n_eq_8_all_m_with_x_stride
q8gavgpool_up8x7_sse2_n_eq_8_all_m_with_x_zero_point
q8gavgpool_up8x7_sse2_n_eq_8_all_m_with_y_max
q8gavgpool_up8x7_sse2_n_eq_8_all_m_with_y_min
q8gavgpool_up8x7_sse2_n_eq_8_all_m_with_y_scale
q8gavgpool_up8x7_sse2_n_eq_8_all_m_with_y_zero_point
q8gavgpool_up8x7_sse2_n_eq_8_few_m
q8gavgpool_up8x7_sse2_n_gt_8_all_m
q8gavgpool_up8x7_sse2_n_gt_8_all_m_with_x_scale
q8gavgpool_up8x7_sse2_n_gt_8_all_m_with_x_zero_point
q8gavgpool_up8x7_sse2_n_gt_8_all_m_with_y_max
q8gavgpool_up8x7_sse2_n_gt_8_all_m_with_y_min
q8gavgpool_up8x7_sse2_n_gt_8_all_m_with_y_scale
q8gavgpool_up8x7_sse2_n_gt_8_all_m_with_y_zero_point
q8gavgpool_up8x7_sse2_n_gt_8_few_m
q8gavgpool_up8xm_neon_n_lt_8_small_m
q8gavgpool_up8xm_neon_n_lt_8_with_x_scale
q8gavgpool_up8xm_neon_n_lt_8_with_x_zero_point
q8gavgpool_up8xm_neon_n_lt_8_with_y_max
q8gavgpool_up8xm_neon_n_lt_8_with_y_min
q8gavgpool_up8xm_neon_n_lt_8_with_y_scale
q8gavgpool_up8xm_neon_n_lt_8_with_y_zero_point
q8gavgpool_up8xm_sse2_n_lt_8_small_m
q8gavgpool_up8xm_sse2_n_lt_8_with_x_scale
q8gavgpool_up8xm_sse2_n_lt_8_with_x_zero_point
q8gavgpool_up8xm_sse2_n_lt_8_with_y_max
q8gavgpool_up8xm_sse2_n_lt_8_with_y_min
q8gavgpool_up8xm_sse2_n_lt_8_with_y_scale
q8gavgpool_up8xm_sse2_n_lt_8_with_y_zero_point
pytorch_q8vadd_ukernel_sse2
Output
lazy_static
register_aten_type_functions
to_cpu
to_device_opt
lazy_static
ConvolutionParams
convolution_shape_check
cudnn_convolution
cudnn_convolution_add_relu
cudnn_convolution_backward
cudnn_convolution_backward_weight
cudnn_convolution_forward
cudnn_convolution_relu
cudnn_convolution_transpose
cudnn_convolution_transpose_backward
cudnn_convolution_transpose_backward_weight
cudnn_convolution_transpose_forward
fmt
is
raw_cudnn_convolution_add_relu_out
raw_cudnn_convolution_backward_weight_out
raw_cudnn_convolution_forward_out
set_convolution_params
cpu_fixed_free
cpu_fixed_malloc
cpu_fixed_realloc
lazy_static
XLAAllocator
allocate
raw_deleter
xla_free
xla_malloc
xla_tensor_test_no_storage
decrement_kernel
error_kernel
expect_calls_decrement
expect_calls_increment
expect_cannot_call_concat_boxed
increment_kernel
kernel_for_schema_inference
kernel_func
lazy_static
operator_registration_test_function_based_kernel_given_catch_all_when_registered_with_torch_library_and_fn_then_can_be_called
operator_registration_test_function_based_kernel_given_mismatched_with_different_num_returns_when_registering_then_fails
operator_registration_test_function_based_kernel_given_mismatched_with_different_return_types_when_registering_then_fails
operator_registration_test_function_based_kernel_given_multiple_operators_and_kernels_when_registered_in_one_registrar_then_calls_right
operator_registration_test_function_based_kernel_given_multiple_operators_and_kernels_when_registered_in_registrars_then_calls_right
operator_registration_test_function_based_kernel_given_when_registered_then_can_be_called
operator_registration_test_function_based_kernel_given_when_registered_then_can_be_called_unboxed
operator_registration_test_function_based_kernel_given_when_registered_with_torch_library_and_fn_then_can_be_called
operator_registration_test_function_based_kernel_given_when_registered_without_specifying_schema_then_infers
HardsigmoidOperatorTester
batch_size
channels
iterations
qmax
qmin
testq8
FormatGuard
defaultfloat
drop
fmt
print
print_format
print_indent
print_matrix
print_scale
print_tensor
int64
max_unpooling2d_backward_cpu
max_unpooling2d_backward_out_cpu
max_unpooling2d_backward_out_cpu_frame
max_unpooling2d_forward_cpu
max_unpooling2d_forward_out_cpu
max_unpooling2d_forward_out_cpu_frame
max_unpooling3d_backward_cpu
max_unpooling3d_backward_out_cpu
max_unpooling3d_backward_out_cpu_frame
max_unpooling3d_forward_cpu
max_unpooling3d_forward_out_cpu
max_unpooling3d_forward_out_cpu_frame
max_unpooling3d_shape_check
CUDAGraph
capture_begin
capture_end
drop
graph_pool_handle
pool
replay
reset
CuBlasPoolType
calculations
create_cublas_handle
destroy_cublas_handle
get_current_cuda_blas_handle
pytorch_q8avgpool_ukernel_up8x9_sse2
assert_eq_float
assert_eq_int
int
lazy_static
pow_test_double_scalar_all_tensors
pow_test_double_tensor
pow_test_double_tensor_all_scalars
pow_test_float_scalar_all_tensors
pow_test_float_tensor
pow_test_float_tensor_all_scalars
pow_test_int_scalar_all_tensors
pow_test_int_tensor
pow_test_int_tensor_all_scalars
pow_test_integral
pow_test_long_scalar_all_tensors
pow_test_long_tensor
pow_test_long_tensor_all_scalars
scalar_pow_tensor
tensor_pow_scalar
tensor_pow_tensor
test_cubed
test_inverse
test_pow_one
test_squared
typed_pow_float
typed_pow_int
item
local_scalar_dense_cpu
Requantization
divide_round_up
lazy_static
min
n
round_up
set_up
tear_down
call
call_boxed
call_unboxed_kernel_function
is_fallthrough
is_valid
is_valid_unboxed
make_ambiguous_autograd_other
make_boxed_function
make_fallthrough
make_from_boxed_function
make_from_unboxed_function
make_from_unboxed_functor
make_from_unboxed_runtime_function
make_named_not_supported
hardswish_op_small_batch
hardswish_op_small_batch_with_qmax
hardswish_op_small_batch_with_qmin
hardswish_op_strided_batch
hardswish_op_strided_batch_with_qmax
hardswish_op_strided_batch_with_qmin
hardswish_op_unit_batch
hardswish_op_unit_batch_with_qmax
hardswish_op_unit_batch_with_qmin
hardswish_op_zero_batch
S
binary_function_traits
function_traits
lazy_static
nullary_function_traits
type
unary_function_traits
lazy_static
one_hot
vitals_api
vitals_basic
vitals_multi_string
vitals_on_and_off
cpu_caching_allocator_test_check_alloc_free
cpu_caching_allocator_test_check_alloc_inside_free_outside
cpu_caching_allocator_test_check_alloc_outside_free_inside
main
GlobalAveragePoolingOperatorTester
batch_size
channels
iterations
testq8
width
DLContext
DLDataType
DLDataTypeCode
DLDevice
DLDeviceType
DLManagedTensor
DLTensor
code
for
in
of
per
the
used
values
define_dispatch
host_softmax
host_softmax_backward
log_softmax_a
log_softmax_b
log_softmax_backward_cpu
log_softmax_c
log_softmax_cpu
softmax_a
softmax_b
softmax_backward_cpu
softmax_c
softmax_cpu
lazy_static
ceil_log2
data_index_init
data_index_step
data_index_step_base_case
elu_backward_kernel
elu_kernel
gelu_backward_kernel_impl
gelu_backward_mkl_kernel_impl
gelu_kernel_impl
gelu_mkl_kernel_impl
glu_backward_kernel
glu_kernel
hardshrink_kernel
hardsigmoid_backward_kernel
hardsigmoid_kernel
hardswish_backward_kernel
hardswish_kernel
hardtanh_backward_kernel
lazy_static
leaky_relu_backward_kernel
leaky_relu_kernel
log_sigmoid_backward_cpu_kernel
log_sigmoid_cpu_kernel
mish_backward_kernel
mish_kernel
register_dispatch
shrink_backward_kernel
silu_backward_kernel
silu_kernel
softplus_backward_kernel
softplus_kernel
softshrink_kernel
threshold_kernel
vec_log_sigmoid
deprecated_at_dispatch_all_types_and_half
deprecated_at_dispatch_all_types_and_half_and_complex
lazy_static
scalar_type
scalar_type_with_deprecated_type_properties
should_include_kernel_dtype
promotion
searchsorted_dims_matched_before_last_dim
searchsorted_pre_check
searchsorted_scalar_tensor
Activation
PackedConvWeightsQnnp
PackedLinearWeightsQnnp
QnnpackOperatorDeleter
activation_limits
apply
apply_dynamic
apply_dynamic_impl
apply_dynamic_relu
apply_impl
apply_relu
bias
dilation
for
generate_requantization_scales
groups
invoke
make_zero_points_and_scales_tensor
padding
prepack
pytorch_qnnp_operator
qnnpack_avg_pool2d
quantize_uint8
round
stride
transpose
unpack
pytorch_qnnp_x8zip_x4_sse2
lazy_static
QEmbeddingPackWeights
lazy_static
prepack
qembeddingbag_2bit_prepack
qembeddingbag_4bit_prepack
qembeddingbag_byte_prepack
qembeddingbag_nbit_prepack_helper
register_embedding_params
run
lazy_static
TypePtr
WorkerId
confirmed_by_owner
is_owner
owner
owner_name
ty
ConvolutionDescriptor
Descriptor
DescriptorDeleter
FilterDescriptor
RNNDescriptor
TensorDescriptor
data_size
desc
fix_size_one_dim_stride
fmt
get_data_type
init
invoke
lazy_static
miopen_type_to_string
mut_desc
print
set
the
lazy_static
needs_dynamic_casting
of
clone_with_remapped_types
fmt
format_type_mismatch_msg
is_backward_compatible_with
is_subtype_of
is_subtype_of_list
pytorch_qnnp_create_tanh_nc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_tanh_nc_q8
pytorch_qnnp_status
channel_shuffle_op_four_groups_small_batch
channel_shuffle_op_four_groups_unit_batch
channel_shuffle_op_many_groups_small_batch
channel_shuffle_op_many_groups_unit_batch
channel_shuffle_op_three_groups_small_batch
channel_shuffle_op_three_groups_unit_batch
channel_shuffle_op_two_groups_small_batch
channel_shuffle_op_two_groups_unit_batch
channel_shuffle_op_zero_batch
apply_cross
cross_kernel_impl
register_dispatch
check_gradout_shape_nll_loss2d
nll_loss2d
nll_loss2d_backward_cpu
nll_loss2d_backward_out_cpu
nll_loss2d_backward_out_cpu_template
nll_loss2d_backward_out_frame
nll_loss2d_forward_cpu
nll_loss2d_forward_out_cpu
nll_loss2d_forward_out_cpu_template
nll_loss2d_forward_out_frame
nll_loss2d_out
optional_contiguous
optional_data
asr_s32
asr_s64
lazy_static
pytorch_scalar_requantize_precise
Type
acc_type
for
map_accumulate_type
to_accumulate_type
SizeAndStride
THCDescBuff
compare_size_and_stride
thc_tensor_all_32bit_indexable
thc_tensor_all_contiguous
thc_tensor_all_same_device
thc_tensor_can_use_32bit_index_math
thc_tensor_free
thc_tensor_get_device
thc_tensor_maybe_overlapping_indices
thc_tensor_n_dimension
thc_tensor_n_dimension_legacy_all
thc_tensor_n_dimension_legacy_no_scalars
thc_tensor_n_element
thc_tensor_preserve_reduce_dim_semantics
thc_tensor_resize
thc_tensor_resize_as
thc_tensor_resize_nd
thc_tensor_retain
thc_tensor_set
thc_tensor_set_storage
thc_tensor_size
thc_tensor_size_legacy_no_scalars
thc_tensor_squeeze1d
thc_tensor_stride
thc_tensor_stride_legacy_no_scalars
thc_tensor_unsqueeze1d
Output
add_assign
assign_from
at_forall_binary_ops
bitand_assign
bitor_assign
bitxor_assign
div_assign
index
mul_assign
neg
operator_tilde
sub_assign
DeconvolutionOperatorTester
adjustment_height
adjustment_width
batch_size
create_deconvolution_op
dilated_kernel_height
dilated_kernel_width
dilation
dilation_height
dilation_width
groups
iterations
kernel_height
kernel_size
kernel_width
padding
padding_bottom
padding_height
padding_left
padding_right
padding_top
padding_width
per_channel
pytorch_qnnp_operator
qmax
qmin
stride
stride_height
stride_width
testq8
GET_BLOCKS
define_dispatch
lazy_static
upsample_trilinear3d
upsample_trilinear3d_backward
int_repr_quantized_cpu
COMPUTE_ROW_SUM_Op
ColVectorMap
GEMMLOWP
GemmlowpOutputPipeline
Pipeline
Q8GEMM
Q8GEMM_L1
Q8GEMM_Op
Q8GEMM_XZP
Q8GEMM_XZP_L1
Q8GEMM_XZP_Op
a
a_row_sums
b
c
divide_round_up
k
kc
kc_stride
kr
lazy_static
make
mc
mr
nc
nc_stride
np
nr
q8gemm_compute_row_sum
quantization_params
requantization_params
round_up
set_up
tear_down
w
PackBMatrix
PrePackConvWeights
drop
get_packed_weights
qnnpack_conv
qnnpack_de_conv
qnnpack_linear
qnnpack_linear_dynamic
bernoulli_impl_a
bernoulli_impl_b
bernoulli_out_impl
cauchy_impl
check_from_to_in_range
exponential_impl
geometric_impl
log_normal_impl
normal_impl_a
normal_impl_b
normal_impl_f64_f64
normal_impl_tensor
normal_out_impl_f64_tensor
normal_out_impl_tensor_f64
normal_out_impl_tensor_tensor
random_from_to_impl
random_impl
uniform_impl
update_from
update_to
calculate_quant_loss
choose_qparams_optimized
choose_qparams_per_tensor
dequantize_cpu
dequantize_quantized_cpu
dequantize_tensors_quantized_cpu
equal_quantized_cpu
make_per_channel_quantized_tensor_cpu
q_per_channel_axis
q_per_channel_scales
q_per_channel_zero_points
q_scale_quant
q_zero_point_quant
qscheme_quant
quantize_per_channel_cpu
quantize_per_tensor
quantize_per_tensor_list_cpu
quantize_per_tensor_tensor_qparams
quantized_clone
set_storage_quantized
im2col_backward_cpu
im2col_backward_out_cpu
im2col_backward_out_cpu_template
im2col_cpu
im2col_out_cpu
im2col_out_cpu_template
lazy_static
BlendRegs
Output
SizeType
ValueType
VectorizedFloat
abs
acos
add
angle
arange
asin
atan
atan2
bitand
bitor
bitxor
blend
blendv
ceil
clamp
clamp_max
clamp_min
conj
convert
copysign
cos
cosh
div
eq
erf
erfc
erfinv
exp
expm1
floor
fmadd
fmod
frac
ge
get_high
get_low
gt
hypot
i0
i0e
igamma
igammac
imag
impl_mask_val_false
impl_mask_val_true
index
index_mut
isnan
le
lgamma
loadu
log
log10
log1p
log2
lt
map
maximum
minimum
mul
ne
neg
nextafter
operator_float_32x4x2_t
partial_cmp
pow
real
reciprocal
round
rsqrt
set
sin
sinh
size
sqrt
store
sub
tan
tanh
trunc
zero_mask
resize_named_tensor
storage_size_for
QLinearUnpackWeightInt8
lazy_static
register_linear_params
run
unpack
ConcatKernel
DecrementKernel
ErrorKernel
IncrementKernel
KernelForSchemaInference
KernelFunc
KernelWithCache
KernelWithConstructorArg
KernelWithDictInputWithOutput
KernelWithDictInputWithoutOutput
KernelWithDictOutput
KernelWithIntInputWithOutput
KernelWithIntInputWithoutOutput
KernelWithIntListInputWithOutput
KernelWithIntListInputWithoutOutput
KernelWithIntListOutput
KernelWithIntOutput
KernelWithMultipleConstructorArgs
KernelWithMultipleOutputs
KernelWithOptInputWithMultipleOutputs
KernelWithOptInputWithOutput
KernelWithOptInputWithoutOutput
KernelWithTensorInputByReferenceWithOutput
KernelWithTensorInputByReferenceWithoutOutput
KernelWithTensorInputByValueWithOutput
KernelWithTensorInputByValueWithoutOutput
KernelWithTensorListInputWithOutput
KernelWithTensorListInputWithoutOutput
KernelWithTensorListOutput
KernelWithTensorOutput
KernelWithTupleInput
KernelWithZeroOutputs
KernelWithoutInputs
KernelWithoutOutput
KernelWithoutTensorInputs
default
expect_calls_concat_unboxed
expect_calls_decrement
expect_calls_increment
invoke
lazy_static
operator_registration_test_functor_based_kernel_given_mismatched_with_different_num_returns_when_registering_then_fails
operator_registration_test_functor_based_kernel_given_mismatched_with_different_return_types_when_registering_then_fails
operator_registration_test_functor_based_kernel_given_multiple_operators_and_kernels_when_registered_in_one_registrar_then_calls_right
operator_registration_test_functor_based_kernel_given_multiple_operators_and_kernels_when_registered_in_registrars_then_calls_right
operator_registration_test_functor_based_kernel_given_when_registered_catch_all_without_specifying_schema_then_infers
operator_registration_test_functor_based_kernel_given_when_registered_then_can_be_called
operator_registration_test_functor_based_kernel_given_when_registered_then_can_be_called_unboxed
operator_registration_test_functor_based_kernel_given_when_registered_without_specifying_schema_then_infers
operator_registration_test_functor_based_kernel_given_with_cache_then_is_kept_correctly
CheckAlmostAllZeroStrides
CheckAlmostAllZeroStrides0
HelperInterpBase
HelperInterpCubic
HelperInterpLinear
HelperInterpNearest
Interpolate
Interpolate1
Interpolate1Interp1
Interpolate1Interp2
InterpolateNInterp1
InterpolateNInterp2
basic_loop
check_almost_all_zero_stride
compute_indices_weights
cpu_upsample_generic
cpu_upsample_linear_channels_last
cpu_upsample_nearest_backward
cpu_upsample_nearest_channels_last
eval
for
init_indices_weights
interpolate
is_contiguous_stride
is_zero_stride
nearest_idx
register_dispatch
scale_t
upsample_bicubic2d_kernel_impl
upsample_bilinear2d_kernel_impl
upsample_generic_nd_kernel_impl
upsample_linear1d_kernel_impl
upsample_nearest1d_backward_kernel_impl
upsample_nearest1d_kernel_impl
upsample_nearest2d_backward_kernel_impl
upsample_nearest2d_kernel_impl
upsample_nearest3d_backward_kernel_impl
upsample_nearest3d_kernel_impl
upsample_trilinear3d_kernel_impl
using
lazy_static
lazy_static
Context
VulkanImpl
acquire_queue
available
bind
command
context
create_device
descriptor
device
dispatch
dispatch_epilogue
dispatch_prologue
drop
flush
gpu
is_vulkan_available
lazy_static
pipeline
queue
resource
shader
vulkan_copy
UnaryFn
UnaryFnWithScalar
abs
abs_mut
abs_out
absolute
absolute_mut
absolute_out
angle
angle_out
arccos_a
arccos_b
arccos_out
arccosh
arccosh_mut
arccosh_out
arcsin
arcsin_mut
arcsin_out
arcsinh
arcsinh_mut
arcsinh_out
arctan
arctan_mut
arctan_out
arctanh
arctanh_mut
arctanh_out
calc_ndtr
can
conj
conj_physical_a
conj_physical_b
conj_physical_c
conj_physical_out
create_unary_float_meta_func
create_unary_meta_func
create_unary_torch_impl_func
declare_dispatch
define_dispatch
deg2rad_a
deg2rad_b
deg2rad_out
fix
fix_mut
fix_out
frexp
frexp_out
imag
lazy_static
logical_not
logical_not_mut
logical_not_out
logit
logit_mut
logit_out
mvlgamma_a
mvlgamma_b
mvlgamma_check
nan_to_num
nan_to_num_mut
nan_to_num_out
negative
negative_mut
negative_out
polygamma
positive
rad2deg_a
rad2deg_b
rad2deg_out
real
resolve_conj
should
signbit
signbit_out
special_digamma
special_digamma_out
special_erf
special_erf_out
special_erfc
special_erfc_out
special_erfinv
special_erfinv_out
special_exp2
special_exp2_out
special_expit
special_expit_out
special_expm1
special_expm1_out
special_gammaln
special_gammaln_out
special_i0
special_i0_out
special_logit
special_logit_out
special_ndtr
special_ndtr_out
special_psi
special_psi_out
square
square_mut
square_out
unary_op_impl
unary_op_impl_float
unary_op_impl_float_out
unary_op_impl_mut
unary_op_impl_out
unary_op_impl_with_complex_to_float
unary_op_impl_with_complex_to_float_out
NVRTC
lazy_static
load_nvrtc
fxdiv_divisor_Size
fxdiv_result_Size
pytorch_qnnp_indirection_init_conv2d
pytorch_qnnp_indirection_init_deconv2d
pytorch_qnnp_indirection_init_dwconv2d
pytorch_qnnp_indirection_init_maxpool2d
QConv1dInt8
QConvInt8
QConvInt8ForBC
apply
apply_impl
apply_relu
compute_deconv_shape
conv_dim_checks
get_bias_data
get_quantization_params
lazy_static
run
Output
SizeType
ValueType
VectorizedComplexFloat
abs
abs_2
acos
add
angle
arange
asin
atan
atan2
bitand
bitor
bitxor
blend
blendv
ceil
conj
cos
cosh
div
eq
erf
erfc
exp
expm1
floor
hypot
igamma
igammac
imag
loadu
log
log10
log1p
log2
map
maximum
minimum
mul
ne
neg
nextafter
operator_m256
pow
real
reciprocal
round
rsqrt
set
sgn
sin
sinh
size
sqrt
store
sub
tan
tanh
trunc
QLinearDynamicFp16
QLinearDynamicInt8
apply_dynamic
apply_dynamic_impl
apply_dynamic_relu
lazy_static
register_linear_params
run
set_bias
pytorch_q8gavgpool_ukernel_up8x7_sse2
lazy_static
declare_pytorch_q8mpavgpool_ukernel_function
declare_pytorch_q8upavgpool_ukernel_function
pytorch_qnnp_create_clamp_nc_u8
pytorch_qnnp_operator
pytorch_qnnp_setup_clamp_nc_u8
pytorch_qnnp_status
ConstQuantizerPtr
&[Tensor]
SigmoidOperatorTester
batch_size
channels
iterations
qmax
qmin
testq8
upsample_nearest2d_out_frame
upsample_nearest2d_out_frame_nhwc
upsample_nearest2d_quantized_cpu
upsample_nearest2d_quantized_cpu_with_scales
CastPolicy
ValType
WeakrefType
WrapFunction
WrapFunction_
and
binary_cross_entropy_banned
cached_cast
cached_cast_overload_for_optional_tensor
cached_cast_overload_for_tensor_lists
clear_cache
decrement_nesting
for
from
get_autocast_cpu_dtype
get_autocast_dispatch_key_from_device_type
get_lower_precision_fp_from_device_type
increment_nesting
is
is_autocast_eligible
is_cpu_enabled
is_eligible
is_enabled
prioritize
prioritize_with_list
prioritize_with_tensor
promote_type
promote_type_tail_case
set_autocast_cpu_dtype
set_cpu_enabled
set_enabled
set_opt_dtype
set_opt_dtype_overload_for_dtype_flags
q8conv_4x4c2_sse2_k_div_8
q8conv_4x4c2_sse2_k_div_8_strided_c
q8conv_4x4c2_sse2_k_div_8_subtile
q8conv_4x4c2_sse2_k_eq_8
q8conv_4x4c2_sse2_k_eq_8_azp_only
q8conv_4x4c2_sse2_k_eq_8_bzp_only
q8conv_4x4c2_sse2_k_eq_8_qmax128
q8conv_4x4c2_sse2_k_eq_8_qmin128
q8conv_4x4c2_sse2_k_eq_8_strided_c
q8conv_4x4c2_sse2_k_gt_8
q8conv_4x4c2_sse2_k_gt_8_azp_only
q8conv_4x4c2_sse2_k_gt_8_bzp_only
q8conv_4x4c2_sse2_k_gt_8_strided_c
q8conv_4x4c2_sse2_k_gt_8_subtile
q8conv_4x8_aarch32_neon_k_div_8
q8conv_4x8_aarch32_neon_k_div_8_strided_c
q8conv_4x8_aarch32_neon_k_div_8_subtile
q8conv_4x8_aarch32_neon_k_eq_8
q8conv_4x8_aarch32_neon_k_eq_8_azp_only
q8conv_4x8_aarch32_neon_k_eq_8_bzp_only
q8conv_4x8_aarch32_neon_k_eq_8_qmax128
q8conv_4x8_aarch32_neon_k_eq_8_qmin128
q8conv_4x8_aarch32_neon_k_eq_8_strided_c
q8conv_4x8_aarch32_neon_k_gt_8
q8conv_4x8_aarch32_neon_k_gt_8_azp_only
q8conv_4x8_aarch32_neon_k_gt_8_bzp_only
q8conv_4x8_aarch32_neon_k_gt_8_strided_c
q8conv_4x8_aarch32_neon_k_gt_8_subtile
q8conv_4x8_neon_k_div_8
q8conv_4x8_neon_k_div_8_strided_c
q8conv_4x8_neon_k_div_8_subtile
q8conv_4x8_neon_k_eq_8
q8conv_4x8_neon_k_eq_8_azp_only
q8conv_4x8_neon_k_eq_8_bzp_only
q8conv_4x8_neon_k_eq_8_qmax128
q8conv_4x8_neon_k_eq_8_qmin128
q8conv_4x8_neon_k_eq_8_strided_c
q8conv_4x8_neon_k_gt_8
q8conv_4x8_neon_k_gt_8_azp_only
q8conv_4x8_neon_k_gt_8_bzp_only
q8conv_4x8_neon_k_gt_8_strided_c
q8conv_4x8_neon_k_gt_8_subtile
q8conv_8x8_aarch64_neon_k_div_8
q8conv_8x8_aarch64_neon_k_div_8_strided_c
q8conv_8x8_aarch64_neon_k_div_8_subtile
q8conv_8x8_aarch64_neon_k_eq_8
q8conv_8x8_aarch64_neon_k_eq_8_azp_only
q8conv_8x8_aarch64_neon_k_eq_8_bzp_only
q8conv_8x8_aarch64_neon_k_eq_8_qmax128
q8conv_8x8_aarch64_neon_k_eq_8_qmin128
q8conv_8x8_aarch64_neon_k_eq_8_strided_c
q8conv_8x8_aarch64_neon_k_gt_8
q8conv_8x8_aarch64_neon_k_gt_8_azp_only
q8conv_8x8_aarch64_neon_k_gt_8_bzp_only
q8conv_8x8_aarch64_neon_k_gt_8_strided_c
q8conv_8x8_aarch64_neon_k_gt_8_subtile
q8conv_8x8_neon_k_div_8
q8conv_8x8_neon_k_div_8_strided_c
q8conv_8x8_neon_k_div_8_subtile
q8conv_8x8_neon_k_eq_8
q8conv_8x8_neon_k_eq_8_azp_only
q8conv_8x8_neon_k_eq_8_bzp_only
q8conv_8x8_neon_k_eq_8_qmax128
q8conv_8x8_neon_k_eq_8_qmin128
q8conv_8x8_neon_k_eq_8_strided_c
q8conv_8x8_neon_k_gt_8
q8conv_8x8_neon_k_gt_8_azp_only
q8conv_8x8_neon_k_gt_8_bzp_only
q8conv_8x8_neon_k_gt_8_strided_c
q8conv_8x8_neon_k_gt_8_subtile
q8gemm_8x4c1x4_sse2_packeda_k_div_8
q8gemm_8x4c1x4_sse2_packeda_k_div_8_strided_a
q8gemm_8x4c1x4_sse2_packeda_k_div_8_strided_c
q8gemm_8x4c1x4_sse2_packeda_k_div_8_subtile
q8gemm_8x4c1x4_sse2_packeda_k_eq_8
q8gemm_8x4c1x4_sse2_packeda_k_eq_8_azp0
q8gemm_8x4c1x4_sse2_packeda_k_eq_8_bzp0
q8gemm_8x4c1x4_sse2_packeda_k_eq_8_nozp
q8gemm_8x4c1x4_sse2_packeda_k_eq_8_qmax128
q8gemm_8x4c1x4_sse2_packeda_k_eq_8_qmin128
q8gemm_8x4c1x4_sse2_packeda_k_eq_8_strided_a
q8gemm_8x4c1x4_sse2_packeda_k_eq_8_strided_c
q8gemm_8x4c1x4_sse2_packeda_k_gt_8
q8gemm_8x4c1x4_sse2_packeda_k_gt_8_azp0
q8gemm_8x4c1x4_sse2_packeda_k_gt_8_bzp0
q8gemm_8x4c1x4_sse2_packeda_k_gt_8_nozp
q8gemm_8x4c1x4_sse2_packeda_k_gt_8_strided_a
q8gemm_8x4c1x4_sse2_packeda_k_gt_8_strided_c
q8gemm_8x4c1x4_sse2_packeda_k_gt_8_subtile
q8gemm_8x4c1x4_sse2_packeda_k_lt_4
q8gemm_8x4c1x4_sse2_packeda_k_lt_4_azp0
q8gemm_8x4c1x4_sse2_packeda_k_lt_4_bzp0
q8gemm_8x4c1x4_sse2_packeda_k_lt_4_nozp
q8gemm_8x4c1x4_sse2_packeda_k_lt_4_qmax128
q8gemm_8x4c1x4_sse2_packeda_k_lt_4_qmin128
q8gemm_8x4c1x4_sse2_packeda_k_lt_4_strided_a
q8gemm_8x4c1x4_sse2_packeda_k_lt_4_strided_c
q8gemm_8x4c1x4_sse2_packeda_k_lt_8
q8gemm_8x4c1x4_sse2_packeda_k_lt_8_azp0
q8gemm_8x4c1x4_sse2_packeda_k_lt_8_bzp0
q8gemm_8x4c1x4_sse2_packeda_k_lt_8_nozp
q8gemm_8x4c1x4_sse2_packeda_k_lt_8_qmax128
q8gemm_8x4c1x4_sse2_packeda_k_lt_8_qmin128
q8gemm_8x4c1x4_sse2_packeda_k_lt_8_strided_a
q8gemm_8x4c1x4_sse2_packeda_k_lt_8_strided_c
pytorch_u8clamp_ukernel_neon
apply
apply_relu
dilation
groups
padding
stride
transpose
unpack
cudnn_convolution
cudnn_convolution_add_relu
cudnn_convolution_backward
cudnn_convolution_backward_weight
cudnn_convolution_deprecated
cudnn_convolution_deprecated2
cudnn_convolution_relu
cudnn_convolution_transpose
cudnn_convolution_transpose_backward
cudnn_convolution_transpose_backward_weight
cudnn_convolution_transpose_deprecated
cudnn_convolution_transpose_deprecated2
raw_cudnn_convolution_backward_weight_out
raw_cudnn_convolution_forward_out
col2im_backward_cpu
col2im_backward_out_cpu
col2im_backward_out_cpu_template
col2im_cpu
col2im_out_cpu
col2im_out_cpu_template
initial_tensor_options
slow_conv3d
slow_conv3d_backward_cpu
slow_conv3d_backward_out_cpu
slow_conv3d_backward_out_cpu_template
slow_conv3d_backward_parameters_frame
slow_conv3d_backward_parameters_out_cpu_template
slow_conv3d_forward_cpu
slow_conv3d_forward_out_cpu
slow_conv3d_out
slow_conv3d_shape_check
view_weight_2d
hardsigmoid_op_small_batch
hardsigmoid_op_small_batch_with_qmax
hardsigmoid_op_small_batch_with_qmin
hardsigmoid_op_strided_batch
hardsigmoid_op_strided_batch_with_qmax
hardsigmoid_op_strided_batch_with_qmin
hardsigmoid_op_unit_batch
hardsigmoid_op_unit_batch_with_qmax
hardsigmoid_op_unit_batch_with_qmin
hardsigmoid_op_zero_batch
gemv
slow_conv_transpose2d_acc_grad_parameters_cpu
slow_conv_transpose2d_backward_cpu
slow_conv_transpose2d_backward_out_cpu
slow_conv_transpose2d_backward_out_cpu_template
slow_conv_transpose2d_cpu
slow_conv_transpose2d_out_cpu
slow_conv_transpose2d_out_cpu_template
slow_conv_transpose2d_shape_check
declare_dispatch
define_dispatch
float_power_a
float_power_b
float_power_c
float_power_d
float_power_e
float_power_out_a
float_power_out_b
float_power_out_c
lazy_static
powi_impl
powi_signed
powi_unsigned
conversion
i_value_test_basic
i_value_test_basic_future
i_value_test_complex
i_value_test_complex_dict
i_value_test_complex_ivalue_print
i_value_test_copy_assign
i_value_test_copy_construct
i_value_test_dict_equality
i_value_test_dict_equality_different_order
i_value_test_enum_equality
i_value_test_equality
i_value_test_future_callbacks
i_value_test_future_exceptions
i_value_test_future_set_error
i_value_test_get_sub_values
i_value_test_identity_comparison_and_hashing
i_value_test_internal_to_pointer
i_value_test_is_alias_of
i_value_test_is_ptr_type
i_value_test_list_equality
i_value_test_list_nested_equality
i_value_test_move_assign
i_value_test_move_construct
i_value_test_scalar_bool
i_value_test_stream_equality
i_value_test_swap
i_value_test_tensor_equality
i_value_test_tuple
i_value_test_tuple_print
i_value_test_unsafe_remove_attr
inspect_tuple_construction
make_more_sample_ivalues
make_sample_ivalues
pytorch_q8gemm_dq_ukernel_4x4c2_sse2
Type
addr_kernel
linalg_vector_norm_kernel_cpu
linalg_vector_norm_kernel_cpu_impl
register_dispatch
unpack_pivots_cpu_kernel
add_nc_q8
add_nc_q8_inplace
lazy_static
norm_except_dim
weight_norm
weight_norm_differentiable_backward
lazy_static
Dist
IDistCalc
LttDistCalc
ODistCalc
PDistCalc
TDistCalc
VectorizedScalar
ZDistCalc
abs
apply_backward_cdist
apply_backward_pdist
apply_cdist
apply_pdist
backward
backward_down_column_cdist
backward_down_column_pdist
cdist_backward_kernel_impl
cdist_kernel_impl
ceil
finish
map
max
min
pdist_backward_kernel_impl
pdist_forward_kernel_impl
pow
red
register_dispatch
run_backward_parallel_cdist
run_backward_parallel_pdist
run_parallel_cdist
run_parallel_pdist
sign
with
MaxPoolingOperatorTester
batch_size
channels
dilated_pooling_height
dilated_pooling_width
dilation
dilation_height
dilation_width
iterations
next_batch_size
padding
padding_bottom
padding_height
padding_left
padding_right
padding_top
padding_width
pooling_height
pooling_size
pooling_width
qmax
qmin
stride
stride_height
stride_width
test_setupu8
testu8
TupleOutput
TupleOutputBaseCase
basic_loop
cpu_kernel
cpu_kernel_vec
cpu_serial_kernel
cpu_serial_kernel_vec
cpu_serial_kernel_vec_with_range
cpu_serial_kernel_with_range
dereference
dereference_impl
dereference_vec
dereference_vec_impl
execute_op
handle
unroll_contiguous_scalar_checks
vectorized_loop
pytorch_qnnp_requantize_precise_psimd
lazy_static
check_maxpool2d_params
define_dispatch
lazy_static
q_maxpool_2d
qnnpack_maxpool2d
quantized_max_pool1d
quantized_max_pool2d
run
spatial_dilated_max_pooling
embedding
embedding_backward
embedding_dense_backward_cpu
embedding_renorm_cpu
embedding_sparse_backward
lazy_static
quantized_resize_cpu
LUTMicrokernelTester
inplace
iterations
n
test
QLinearPackWeightFp16
QLinearPackWeightFp16Legacy
QLinearPackWeightInt8
QLinearPackWeightInt8Legacy
calc_col_offsets_transpose
lazy_static
prepack
register_linear_params
run
saturate_weight_to_fp16
Output
Range
div
fmt
size
slow_conv2d_backward_cpu
slow_conv2d_backward_out_cpu
slow_conv2d_backward_out_cpu_template
slow_conv2d_backward_parameters_frame
slow_conv2d_backward_parameters_out_cpu_template
slow_conv2d_forward_cpu
slow_conv2d_forward_out_cpu
slow_conv2d_shape_check
thnn_conv2d
thnn_conv2d_out
view_weight_2d
gemv
slow_conv_transpose3d_acc_grad_parameters_cpu
slow_conv_transpose3d_backward_cpu
slow_conv_transpose3d_backward_out_cpu
slow_conv_transpose3d_backward_out_cpu_template
slow_conv_transpose3d_cpu
slow_conv_transpose3d_out_cpu
slow_conv_transpose3d_out_cpu_template
slow_conv_transpose3d_shape_check
IsContiguous
eval
lazy_static
Block
adaptive_avg_pool2d
avg_pool2d
lazy_static
max_pool2d
pool2d
MKL_DFTI_CHECK
MetalTensorImpl
is_contiguous_custom
stride
strides
tensorimpl_type_name
Q8ConvContext
QnnpackDeleter
compute_q8conv
invoke
q8conv_context
qnnpack_de_conv
SerializedModel
SerializedOperand
SerializedOperation
SerializedValue
SourceType
load_nnapi_model
value_physical_size
constant_pad_nd
fill_inplace
scalar_fill
scalar_tensor_static
scalar_to_tensor
wrapped_scalar_tensor
pytorch_q8gemm_ukernel_4x8_neon
ConstantString
EnumHolder
FakeType
Future
FutureError
GuardedUnsignedLongUniqueDummy
Object
Shared
TaggedCapsule
Tuple
add_callback
as
check_custom_class_type
collect_all
collect_any
compilation_unit
completed
const_value
copy_
create
create_instance
create_named
create_vector_from_list
data_ptrs
deepcopy
define_to
devices
dynamic_intrusive_pointer_cast
element_type
elements
ensure_is_subset_of_devices
exception_ptr
extract_data_ptrs
extract_tensors
format_set_of_devices
from
generic_to_tuple_impl
generic_to_with_fake_type_arrayn_elem
generic_to_with_fake_type_dict
generic_to_with_fake_type_hashmap
generic_to_with_fake_type_list_elem
generic_to_with_fake_type_option_t
generic_to_with_fake_type_optional_array_t
generic_to_with_fake_type_t
generic_to_with_fake_type_tagged_capsule_t
generic_to_with_fake_type_vec_elem
get_attr
get_devices_of_data_ptrs
get_py_object
get_slot
get_type_of_devices
has_error
has_value
hash
information
invoke_callback
is
is_custom_class
is_same_identity
it
lazy_static
likelike
make_capsule
mark_completed
might
move_to_intrusive_ptr
name
operator_const_string_ref
operator_const_vector_i_value_ref
qualified_class_name
resize_object
set_attr
set_error
set_error_if_needed
set_error_internal
set_slot
slots
sort_and_deduplicate_devices
static_intrusive_pointer_cast
string
string_view
synchronize_with_current_streams
that
then
then_async
to
to_blob
to_bool_list
to_capsule
to_complex_double
to_complex_double_list
to_complex_double_vector
to_custom_class
to_double_list
to_double_vector
to_enum_holder
to_future
to_generator
to_generic_dict
to_int_list
to_int_vector
to_intrusive_ptr
to_ivalue
to_list
to_list_ref
to_object
to_object_ref
to_optional
to_optional_string_ref
to_py_object
to_py_object_holder
to_quantizer
to_rref
to_storage
to_str
to_stream
to_string
to_string_ref
to_string_view
to_tensor
to_tensor_list
to_tensor_vector
to_tuple
try_retrieve_error_message
try_retrieve_error_message_internal
try_to_infer_type
ty
unqualified_class_name
unsafe_remove_attr
unsafe_remove_slot
value
wait
wait_and_throw
what
mobile_allocate_padded_contiguous_if_needed
mobile_empty_with_tail_padding
declare_pytorch_q8conv_ukernel_function
upsample_nearest3d_out_frame
upsample_nearest3d_out_frame_nhwc
upsample_nearest3d_quantized_cpu
upsample_nearest3d_quantized_cpu_with_scales
BatchDim
BatchDims
BatchDimsRef
BatchedTensorImpl
actual_dim
add_batch_dim
bdims
check_invariants
create_batch_dim_bitset
create_vmap_levels_bitset
dim
fmt
has_storage
inplace_is_vmap_compatible
is_batched_tensor
is_contiguous_custom
level
make_batched
maybe_get_batched_impl
set_size
set_storage_offset
set_stride
tensorimpl_type_name
unsafe_get_batched_impl
value
LaunchParams
compute_mps_align_offset
kernel_for
Arguments
convolution_depthwise3x3_winograd
convolution_depthwise3x3_winograd_impl
declare_dispatch
lazy_static
register_dispatch
v4f_transpose4x4_neon
vmuladdq_f32
vmulsubq_f32
winograd_f2k3_kernel_transform_neon
pytorch_qnnp_x8zip_xm_sse2
sgemm_5x8_neon_k_div_2
sgemm_5x8_neon_k_div_2_strided_a
sgemm_5x8_neon_k_div_2_strided_c
sgemm_5x8_neon_k_div_2_subtile
sgemm_5x8_neon_k_eq_2
sgemm_5x8_neon_k_eq_2_strided_a
sgemm_5x8_neon_k_eq_2_strided_c
sgemm_5x8_neon_k_eq_8_qmax128
sgemm_5x8_neon_k_eq_8_rmin128
sgemm_5x8_neon_k_gt_2
sgemm_5x8_neon_k_gt_2_strided_a
sgemm_5x8_neon_k_gt_2_strided_c
sgemm_5x8_neon_k_gt_2_subtile
sgemm_6x8_neon_k_div_2
sgemm_6x8_neon_k_div_2_strided_a
sgemm_6x8_neon_k_div_2_strided_c
sgemm_6x8_neon_k_div_2_subtile
sgemm_6x8_neon_k_eq_2
sgemm_6x8_neon_k_eq_2_strided_a
sgemm_6x8_neon_k_eq_2_strided_c
sgemm_6x8_neon_k_eq_8_qmax128
sgemm_6x8_neon_k_eq_8_qmin128
sgemm_6x8_neon_k_gt_2
sgemm_6x8_neon_k_gt_2_strided_a
sgemm_6x8_neon_k_gt_2_strided_c
sgemm_6x8_neon_k_gt_2_subtile
sgemm_6x8_psimd_k_div_2
sgemm_6x8_psimd_k_div_2_strided_a
sgemm_6x8_psimd_k_div_2_strided_c
sgemm_6x8_psimd_k_div_2_subtile
sgemm_6x8_psimd_k_eq_2
sgemm_6x8_psimd_k_eq_2_strided_a
sgemm_6x8_psimd_k_eq_2_strided_c
sgemm_6x8_psimd_k_eq_8_qmax128
sgemm_6x8_psimd_k_eq_8_qmin128
sgemm_6x8_psimd_k_gt_2
sgemm_6x8_psimd_k_gt_2_strided_a
sgemm_6x8_psimd_k_gt_2_strided_c
sgemm_6x8_psimd_k_gt_2_subtile
lazy_static
PackedLinearWeightQnnp
apply
apply_dynamic
apply_dynamic_impl
apply_dynamic_relu
apply_impl
apply_relu
bias
prepack
unpack
ensure_has_index
to_a
to_b
to_c
to_cpu
to_d
to_dense_backward
to_impl
to_mkldnn_backward
view_dtype
ConstReference
GenericList
InternalConstReferenceType
InternalReferenceType
Iterator
List
ListElementConstReferenceTraits
ListElementReference
ListImpl
ListIterator
ListType
Output
ReverseIterator
SizeType
T
TypePtr
ValueType
append
assign_from
begin
clear
cmp
copy_
element_type
emplace
emplace_back
empty
end
eq
erase
extract
get
index
index_mut
insert
is
of
operator_minus
operator_minus_equals
operator_mul
operator_plus
operator_plus_equals
operator_t
out_of_range
partial_cmp
pop_back
prefix_decrement
prefix_increment
ptr_to_first_element
push_back
reserve
resize
set
size
swap
to_list
to_typed_list
unsafe_set_element_type
use_count
vec
test_qtensor_empty_perchannel_quantized
test_qtensor_empty_quantized
test_qtensor_item
test_qtensor_quant_dequant_api_s
test_qtensor_quantize_per_channel4d
test_qtensor_quantize_per_channel4d_channels_last
test_qtensor_rounding_mode
DimMask
IdxVec
allreduce_return_trivial
and
check_scalar_type_device_layout_equal
create_reduction_result
dimreduce_return_trivial
dimreduce_return_trivial_no_ident
dimreduce_setup
ensure_nonempty_dim
ensure_nonempty_size
ensure_nonempty_stride
ensure_nonempty_vec
integer_upcast
lower_bound
make_dim_mask
make_reduction_a
make_reduction_b
make_reduction_c
make_reduction_d
promotion
resize_reduction_result
restride_dim
review_reduce_result
shape_from_dim_mask
upper_bound
zero_numel_check_dims_a
zero_numel_check_dims_b
zero_numel_tensor_resize
FftNormMode
declare_dispatch
fft_fill_with_conjugate_symmetry
fft_fill_with_conjugate_symmetry_fn
infer_ft_complex_to_real_onesided_size
infer_ft_real_to_complex_onesided_size
BackwardFn
ForwardFn
declare_dispatch
define_dispatch
group_norm
math_group_norm
native_group_norm
native_group_norm_backward
HardswishOperatorTester
batch_size
channels
iterations
qmax
qmin
testq8
ThreadLocalState
ThreadLocalStateGuard
drop
set_thread_local_state
wrap_propagate_tls_state
avg_pool3d_quantized_cpu
define_dispatch
get_kernel
get_padding
get_stride
q_avg_pool3d
check_and_update_common_device_a
check_and_update_common_device_b
check_and_update_common_device_c
check_and_update_common_device_d
check_tensor_options_and_extract_memory_format
common_device_check_failure
QTensorImpl
copy_tensor_metadata
quantizer
set_quantizer
shallow_copy_and_detach
shallow_copy_from
tensorimpl_type_name
pytorch_u8clamp_ukernel_sse2
FloatVecReturnType
IntVecReturnType
SizeType
ValueType
VecInternalMaskType
VecInternalType
VectorizedQint8Alt0
VectorizedQint8Alt1
dequantize
dump
float_num_vecs
int_num_vecs
loadu
maximum
minimum
of
quantize
relu
relu6
requantize_from_int
size
store
vec0
vec1
widening_subtract
get_cxx_flags
get_mkl_version
get_mkldnn_version
get_openmp_version
show_config
used_cpu_capability
lazy_static
global_average_pooling_q8
lazy_static
IndexToOffset
Indexer
cpu_index_kernel
cpu_masked_fill_kernel
cpu_masked_scatter_kernel
cpu_masked_select_kernel
cpu_masked_select_serial_kernel
cpu_take_put_kernel
flip_kernel
get
index_copy_kernel
index_fill_kernel
index_kernel
index_put_kernel
is_constant_index
masked_fill_kernel
masked_scatter_kernel
masked_select_kernel
masked_select_serial_kernel
put_kernel
register_dispatch
take_kernel
DftiDescriptor
DftiDescriptorDeleter
get
init
invoke
hardsigmoid_q8
lazy_static
sparse_op_test
QLeakyRelu
QRelu6
define_dispatch
lazy_static
leaky_relu_out_quantized_cpu
leaky_relu_quantized_cpu
leaky_relu_quantized_cpu_mut
qnnpack_relu
quantized_relu6
quantized_relu6_mut
relu_quantized_cpu
relu_quantized_cpu_mut
run
QSigmoid
define_dispatch
lazy_static
qnnpack_sigmoid
run
sigmoid_quantized_cpu
pytorch_qnnp_x8zip_xm_neon
custom_string
domain_prefix
domain_string
from_domain_and_unqual_string
from_qual_string
global_strings
ns
string
symbol
to_display_string
to_qual_string
to_unqual_string
dense_to_mkldnn
mkldnn_reorder_conv2d_weight
mkldnn_reorder_conv3d_weight
mkldnn_to_dense
FloatVecReturnType
IntVecReturnType
Output
SizeType
ValueType
VectorizedQint32
VectorizedQint8
VectorizedQuantizedConverter
VectorizedQuint8
Vectorizedqi
add
cvtepi8_epi32
cvtepu8_epi32
default
dequantize
dump
float_num_vecs
int_num_vecs
loadu
maximum
minimum
mul
of
operator_m_256i
pack_saturate_and_clamp
pack_saturate_and_clamp_i32
pack_saturate_and_clamp_i8
pack_saturate_and_clamp_u8
quantize
quantize_avx2
relu
relu6
requantize_avx2
requantize_from_int
size
store
widening_subtract
div_rtn
Conv2dOpContext
LinearOpContext
SerializationTypeConv2dPrePack
SerializationTypeLinearPrePack
SerializationTypeTransposeConv2dPrePack
TransposeConv2dOpContext
XNNPackConv2dOpContext
XNNPackLinearOpContext
XNNPackTransposeConv2dOpContext
create_context
free_orig_weight_and_bias
run
unpack
lazy_static
IterArgs
apply
invoke
self_
short_circuit
lazy_static
view
cudnn_affine_grid_generator_backward
cudnn_affine_grid_generator_forward
set_sampler_descriptor
FlipFn
Output
atleast_1d
atleast_2d
atleast_3d
declare_dispatch
define_dispatch
flip
fliplr
flipud
roll_common
roll_cpu
rot90
available
create
create_and_run
create_linear_clamp_pre_pack_op_context
linear
linear_clamp_run
run
usable
use_linear
thnn_conv_depthwise2d
thnn_conv_depthwise2d_out
add_out_dense_sparse_csr_cpu
add_out_sparse_csr_cpu
add_sparse_csr
add_sparse_csr_mut
addmm_out_sparse_csr_dense_cpu
addmm_out_sparse_dense_worker
addmm_sparse_csr_dense
is_mkl_supported
is_square_or_vec
sparse_csr_addmm
sparse_csr_mm_out
u8lut32norm_scalar_n_eq_1
u8lut32norm_scalar_n_eq_1_inplace
u8lut32norm_scalar_small_n
u8lut32norm_scalar_small_n_inplace
Cache
Deleter
Descriptor
Handle
Shader
ShaderDescriptor
ShaderDescriptorShaderBinary
ShaderDescriptorShaderSource
ShaderDescriptorType
ShaderFactory
ShaderFactoryCompiler
ShaderFactoryHasher
ShaderLayout
ShaderLayoutCache
ShaderLayoutDescriptor
ShaderLayoutFactory
ShaderLayoutFactoryHasher
ShaderLayoutObject
Signature
WorkGroup
assign_from
compile
default
defines
eq
invoke
of
operator_bool
purge
retrieve
AvgPoolMicrokernelTester
iterations
kc
kh
kr
ks
kw
mr
n
packed_ks
packedn
qr
s
test
x_scale
x_stride
x_zero_point
y_max
y_min
y_scale
y_stride
y_zero_point
pytorch_q8gemm_sparse_packa_ukernel_8x4_sse2
CUDAHooksArgs
batchnorm_min_epsilon_cu_dnn
c10_declare_registry
c10_define_registry
compiled_with_cu_dnn
compiled_with_mio_pen
cu_fft_clear_plan_cache
cu_fft_get_plan_cache_max_size
cu_fft_get_plan_cache_size
cu_fft_set_plan_cache_max_size
current_device
device_synchronize
get_cuda_device_allocator
get_cuda_hooks
get_default_cuda_generator
get_devce_index_with_primary_context
get_device_from_ptr
get_num_gpu_s
get_pinned_memory_allocator
has_cu_dnn
has_primary_context
hascuda
hascudart
hasmagma
initcuda
is_pinned_ptr
lazy_static
nvrtc
show_config
supports_depthwise_convolution_with_cu_dnn
supports_dilated_convolution_with_cu_dnn
version_cu_dnn
versioncudart
CompositeRandomAccessorCPU
TupleInfoCPU
lazy_static
swap
tie
CuFFTConfig
CuFFTDataLayout
CuFFTDimVector
CuFFTHandle
CuFFTParams
CuFFTParamsLRUCache
CuFFTTransformType
as_cufft_embed
assign_from
clear
const_assert
cufft_Sizeype
cufft_clear_plan_cache_impl
cufft_get_plan_cache_max_size_impl
cufft_get_plan_cache_size_impl
cufft_set_plan_cache_max_size_impl
cufft_simple_embed
data_type
default
drop
from
get
get_cu_fft_transform_type
has
is
is_pow_of_two
lookup
max_size
plan
resize
set_max_size
size
static_assert
transform_type
usize
with
workspace_size
VulkanImpl
VulkanTensorImpl
adaptive_avg_pool2d
add
add_mut
add_scalar
addmm
avg_pool2d
cat
clamp
clamp_mut
contiguous
convolution
convolution_prepack_weights
convolution_prepacked
copy_from_vulkan
copy_to_vulkan
empty
empty_strided
hardtanh
hardtanh_mut
is_vulkan_available
lazy_static
max_pool2d
mean
mm
mul_scalar
normalize_dim
relu
reshape
select
slice
transpose
transpose_mut
unsqueeze
upsample_nearest2d
view
vtensor
vtensor_from_vulkan
vtensor_from_vulkan_mut
vtensor_mut
vulkan_copy
vulkan_copy_impl
define_dispatch
lazy_static
quantized_threshold_impl
threshold_quantized_cpu
pytorch_q8dwconv_ukernel_up8x9_per_channel_sse2
CaptureKernelCall
Dispatcher
OperatorDef
OperatorHandle
RegistrationListenerList
add_listener
add_registration_listener
call_boxed
call_on_operator_deregistered
call_on_operator_registered
check_invariants
check_schema_compatibility
cleanup
debug
deregister_def
deregister_fallback
deregister_impl
deregister_library
deregister_name
dump_computed_table
dump_state
find_dangling_impls
find_op
find_or_register_name
find_or_register_schema
find_schema
find_schema_or_throw
get_all_op_names
has_schema
lazy_static
on_operator_deregistered
on_operator_registered
operator_name
real_singleton
redispatch_boxed
register_def
register_fallback
register_impl
register_library
register_name
run_record_function
schema
sequence_number_for_running_record_function
singleton
typed
ShapeAndDims
as_complex
cufft_clear_plan_cache
cufft_get_plan_cache_max_size
cufft_get_plan_cache_size
cufft_set_plan_cache_max_size
default_alldims
define_dispatch
fft_c2c
fft_c2r
fft_fft
fft_fft2
fft_fft2_out
fft_fft_out
fft_fftfreq
fft_fftfreq_out
fft_fftn
fft_fftn_out
fft_fftshift
fft_fill_with_conjugate_symmetry
fft_hfft
fft_hfft_out
fft_ifft
fft_ifft2
fft_ifft2_out
fft_ifft_out
fft_ifftn
fft_ifftn_out
fft_ifftshift
fft_ihfft
fft_ihfft_out
fft_irfft
fft_irfft2
fft_irfft2_out
fft_irfft_out
fft_irfftn
fft_irfftn_impl
fft_irfftn_out
fft_r2c
fft_rfft
fft_rfft2
fft_rfft2_out
fft_rfft_out
fft_rfftfreq
fft_rfftfreq_out
fft_rfftn
fft_rfftn_impl
fft_rfftn_out
fftn_c2c
istft_a
istft_b
norm_from_string
promote_tensor_fft
promote_type_fft
stft_a
stft_b
values
write_opt
pytorch_qnnp_create_convolution2d_nhwc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_convolution2d_nhwc_q8
pytorch_qnnp_status
pytorch_qnnp_ukernel_type
pytorch_qnnp_x8zip_x4_neon
cross
cross_out
declare_dispatch
define_dispatch
lazy_static
cudnn_ctc_loss
use_cudnn_ctc_loss
pytorch_q8conv_ukernel_4x8_neon
pytorch_qnnp_create_sigmoid_nc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_sigmoid_nc_q8
pytorch_qnnp_status
allocator
infer_scalar_type
infer_scalar_type_with_tensor_list
options
th_copy_ignoring_overlaps
th_cross_kernel
th_cross_kernel_out
th_gels
th_gels_out
thnn_conv2d_backward
thnn_conv2d_backward_out
thnn_conv2d_forward
thnn_conv2d_forward_out
thnn_conv_depthwise2d_backward
thnn_conv_depthwise2d_backward_out
thnn_conv_depthwise2d_forward
thnn_conv_depthwise2d_forward_out
thnn_glu_backward
thnn_glu_backward_out
thnn_glu_forward
thnn_glu_forward_out
thnn_log_sigmoid_backward
thnn_log_sigmoid_backward_out
thnn_log_sigmoid_forward
thnn_log_sigmoid_forward_out
thnn_nll_loss2d_backward
thnn_nll_loss2d_backward_out
thnn_nll_loss2d_forward
thnn_nll_loss2d_forward_out
thnn_nll_loss_backward
thnn_nll_loss_backward_out
thnn_nll_loss_forward
thnn_nll_loss_forward_out
lazy_static
call_op
call_op_unboxed
call_op_unboxed_with_dispatch_key
call_op_unboxed_with_precomputed_dispatch_key_set
dummy_tensor
expect_doesnt_find_kernel
expect_doesnt_find_operator
expect_list_equals
expect_throws
extract_dispatch_key
make_stack
sets
empty_binary_op
mkldnn_add
mkldnn_add_mut
mkldnn_add_out
mkldnn_mul
mkldnn_mul_mut
mkldnn_mul_out
empty_mkldnn
declare_pytorch_u8rmax_ukernel_function
lazy_static
bf16_isnan
complex_isnan
exp
exp_double
float_isnan
half_isnan
integer_isnan
log
log_double
tan
tan_double
get_miopen_data_type
miopen_version
Reduction
lazy_static
cpu_upsample_linear_backward
register_dispatch
scale_t
upsample_bilinear2d_backward_kernel_impl
upsample_linear1d_backward_kernel_impl
upsample_trilinear3d_backward_kernel_impl
BinaryFn
arange_a
arange_b
arange_c
arange_out_a
arange_out_b
bartlett_window_a
bartlett_window_b
blackman_window_a
blackman_window_b
cast
check_supported_max_int_with_precision
clone
complex
complex_check_dtype
complex_check_floating
complex_out
declare_dispatch
define_dispatch
dim_arange
empty
empty_cpu
empty_like
empty_out
empty_strided_cpu
eye_a
eye_b
eye_out_cpu_a
eye_out_cpu_b
from_file
full_a
full_b
full_like
full_out
get_tril_size
hamming_window_a
hamming_window_b
hamming_window_c
hamming_window_d
hann_window_a
hann_window_b
infer_full_options
kaiser_window_a
kaiser_window_b
kaiser_window_c
lazy_static
linspace
linspace_logspace_infer_options
logspace
normal
normal_out
ones_a
ones_b
ones_like
ones_out
polar
polar_out
rand_a
rand_b
rand_c
rand_d
rand_like
rand_out_a
rand_out_b
randint_a
randint_b
randint_c
randint_d
randint_like_a
randint_like_b
randint_out_a
randint_out_b
randint_out_c
randint_out_d
randn_a
randn_b
randn_c
randn_d
randn_like
randn_out_a
randn_out_b
randperm_a
randperm_b
randperm_cpu
randperm_out
randperm_out_cpu
range_a
range_b
scalar_tensor
tensor_backend
tensor_complex_backend
tensor_complex_cpu
tensor_cpu
tril_indices_cpu
triu_indices_cpu
vander
window_function_checks
zeros_a
zeros_b
zeros_like
zeros_out
ConstTypePtr
Type
TypeKind
TypePrinter
TypePtr
annotation_str
annotation_str_impl
cast
cast_raw
constructor
contained_types
contains
create_with_contained
expect
expect_ref
has_free_variables
is
is_module
is_subtype_of
is_subtype_of_ext
kind
of
repr_str
requires_grad
str_
type_kind_to_string
will
with
with_contained
VecHostSoftMaxBackwardLastDim
VecHostSoftMaxLastDim
VecSoftMax
apply
log_softmax_backward_lastdim_kernel_impl
log_softmax_lastdim_kernel_impl
register_dispatch
softmax_backward_lastdim_kernel_impl
softmax_kernel_impl
softmax_lastdim_kernel_impl
vec_host_softmax_backward_lastdim
vec_log_softmax_lastdim
vec_softmax
vec_softmax_lastdim
avg_pool2d_backward_kernel_impl
avg_pool2d_kernel_impl
cpu_avg_pool
cpu_avg_pool_backward
cpu_avg_pool_backward_channels_last
cpu_avg_pool_channels_last
register_dispatch
can_device_access_peer
get_cuda_device_allocator
get_current_device_properties
get_device_properties
get_num_gpu_s
init_cuda_context_vectors
init_device_property
is_available
lazy_static
warp_size
CudaEvent
assign_from
block
cmp
create_event
device
device_index
drop
elapsed_time
event
ipc_handle
is_created
move_helper
operator_cuda_event_t
partial_cmp
query
record
record_once
synchronize
pytorch_qnnp_requantize_fp32_scalar_lrintf
pytorch_qnnp_requantize_fp32_scalar_magic
PyTorchQnnpFormat
PyTorchQnnpOperator
PyTorchQnnpUKernelType
SparseMatrix
pytorch_qnnp_operator_get_log2_bias_element_size
pytorch_qnnp_operator_get_log2_kernel_element_size
pytorch_qnnp_create_leaky_relu_nc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_leaky_relu_nc_q8
pytorch_qnnp_status
lazy_static
sequence_number_get_and_increment
sequence_number_peek
u8rmax_neon_n_div_16
u8rmax_neon_n_eq_16
u8rmax_neon_n_gt_16
u8rmax_neon_n_lt_16
u8rmax_sse2_n_div_16
u8rmax_sse2_n_eq_16
u8rmax_sse2_n_gt_16
u8rmax_sse2_n_lt_16
TanHOperatorTester
batch_size
channels
iterations
qmax
qmin
testq8
lazy_static
GenericWrapperTensorImpl
backend_fallback_test_fallthrough
backend_fallback_test_with_mode
backend_fallback_test_with_wrapper
generic_mode_fallback
generic_wrapper_fallback
lazy_static
DispatchKeySet
KernelFunction
Stack
ambiguous_autogradother_kernel
call
call_boxed
dump_state
equals_boxed_and_unboxed
fallthrough_kernel
get_functor
is_valid_unboxed
make_ambiguous_autograd_other
make_boxed_function
make_fallthrough
make_from_boxed_function
make_from_unboxed_function
make_from_unboxed_functor
make_from_unboxed_runtime_function
make_named_not_supported
named_not_supported_kernel
lazy_static
pytorch_q8gemm_ukernel_6x4_neon
pytorch_qnnp_requantize_fp32_psimd
pytorch_q8gavgpool_ukernel_up8x7_neon
BaseSampler
BernoulliStub
CauchyStub
ExponentialStub
GeometricStub
LogNormalStub
NormalStub
RandomFromToMeta
RandomFromToStub
RandomStub
UniformMeta
UniformStub
bernoulli_a
bernoulli_b
bernoulli_c
bernoulli_d
bernoulli_out
beta_grad_alpha_mid
beta_grad_alpha_small
beta_grad_beta_small
binomial_inversion
btrs
cauchy
define_dispatch
digamma_one
dirichlet_grad_cpu
dirichlet_grad_one
exponential
geometric
invoke
lazy_static
log_normal
multinomial
multinomial_out
normal_a
normal_b
normal_c
normal_d
normal_meta
normal_out_a
normal_out_b
normal_out_tensor_tensor
parameter
polevl
random
random_meta
random_meta_range
random_meta_until
random_range
random_to
s_binomial_cpu
s_dirichlet_cpu
s_gamma_cpu
s_poisson_cpu
sample
sample_binomial
sample_gamma
sample_poisson
standard_gamma_grad_cpu
standard_gamma_grad_one
stirling_approx_tail
uniform
uniform_meta
map
map2
map2_reduce_all
map3
map3_reduce_all
map4
map_reduce_all
reduce2_all
reduce_all
vec_reduce_all
ArgTypeTestKernel
DummyKernel
DummyKernelWithIntParam
MockKernel
OpRegistrationListenerForDelayedListenerTest
TestArgTypes
TestLegacyAPI
TestModernAPI
TestModernAndLegacyAPI
and
autograd_kernel
autograd_kernel_redispatching_with_dispatch_key_set
autograd_kernel_redispatching_without_dispatch_key_set
backend_fallback_kernel
cpu_kernel
dummy_fn
has
ids
invoke
lazy_static
nonautograd_kernel
on_operator_deregistered
on_operator_registered
operator_registration_test_autograd_backend_overrides_kernel
operator_registration_test_autograd_xla_overrides_kernel
operator_registration_test_call_kernels_with_dispatch_key_set_convention_boxed_redispatches_to_lower_priority
operator_registration_test_call_kernels_with_dispatch_key_set_convention_mixed_calling_conventions_redispatches_to_lower_priority
operator_registration_test_call_kernels_with_dispatch_key_set_convention_redispatches_to_lower_priority
operator_registration_test_given_lambda_kernel_when_accessing_catch_all_with_mismatching_cpp_signatures_then_fails
operator_registration_test_given_lambda_kernel_when_accessing_with_mismatching_cpp_signatures_then_fails
operator_registration_test_given_lambda_kernel_when_registering_backend_and_catch_all_with_mismatching_cpp_signatures_then_fails
operator_registration_test_given_lambda_kernel_when_registering_catch_all_and_backend_with_mismatching_cpp_signatures_then_fails
operator_registration_test_given_lambda_kernel_when_registering_with_mismatching_cpp_signatures_then_fails
operator_registration_test_given_multiple_catchall_kernels_when_registering_in_same_op_call_then_fails
operator_registration_test_given_multiple_kernels_with_same_dispatch_key_when_registering_in_op_call_then_fails
operator_registration_test_given_op_with_catchall_kernel_out_of_scope_when_registering_dispatched_and_calling_then_calls
operator_registration_test_given_op_with_catchall_kernel_when_calling_then_calls
operator_registration_test_given_op_with_dispatched_kernel_out_of_scope_when_registering_catchall_and_calling_then_calls
operator_registration_test_given_op_without_kernels_when_registering_schema_then_fails
operator_registration_test_given_op_without_kernels_when_registering_with_schema_then_only_registers
operator_registration_test_given_op_without_kernels_when_running_out_of_scope_then_schema_is_gone
operator_registration_test_given_torch_library_when_accessing_catch_all_with_mismatching_cpp_signatures_then_fails
operator_registration_test_given_torch_library_when_accessing_with_mismatching_cpp_signatures_then_fails
operator_registration_test_given_torch_library_when_registering_with_mismatching_cpp_signatures_then_fails
operator_registration_test_when_calling_op_with_wrong_dispatch_key_then_fails
operator_registration_test_when_register_with_xla_kernel_and_catch_all_autograd_is_not_filled
operator_registration_test_when_registering_autograd_kernel_then_can_call
operator_registration_test_when_registering_autograd_kernel_with_catch_all_then_can_call
operator_registration_test_when_registering_autograd_kernel_with_catch_all_then_can_call_catchall
operator_registration_test_when_registering_autograd_kernel_with_regular_then_can_call
operator_registration_test_when_registering_backend_fallback_kernel_and_regular_for_different_then_can_be_called
operator_registration_test_when_registering_backend_fallback_kernel_and_regular_for_same_then_calls
operator_registration_test_when_registering_backend_fallback_kernel_for_wrong_then_cannot_be_called
operator_registration_test_when_registering_backend_fallback_kernel_then_can_be_called
operator_registration_test_when_registering_cpu_tensor_type_then_can_only_call_unboxed_with_dispatch_key
operator_registration_test_when_registering_mismatching_kernels_in_same_op_call_then_fails
operator_registration_test_when_registering_multiple_kernels_by_name_and_none_can_infer_schema_then_fails
operator_registration_test_when_registering_multiple_kernels_by_name_and_only_one_can_infer_schema_then_succeeds
operator_registration_test_when_registering_multiple_kernels_by_schema_and_none_can_infer_then_succeeds
operator_registration_test_when_registering_multiple_kernels_by_schema_and_only_one_can_infer_then_succeeds
operator_registration_test_when_registering_multiple_kernels_in_same_op_call_and_calling_then_calls_correct_kernel
operator_registration_test_when_registering_with_name_after_kernel_in_options_object_then_can_be_called
operator_registration_test_when_registering_with_name_before_kernel_in_options_object_then_can_be_called
operator_registration_test_when_registering_with_schema_after_kernel_in_options_object_then_can_be_called
operator_registration_test_when_registering_with_schema_before_kernel_in_options_object_then_can_be_called
operator_registration_test_when_registering_without_schema_then_fails
stack_based_kernel
test
tracing_kernel_redispatching_with_dispatch_key_set
works
Algo
AlgorithmSearch
AlgorithmSearchMiOpenConvBwdDataAlgorithm
AlgorithmSearchMiOpenConvBwdWeightsAlgorithm
AlgorithmSearchMiopenConvFwdAlgorithm
BenchmarkCache
ConvolutionArgs
ConvolutionParams
ParamsEqual
ParamsHash
Perf
Workspace
cache
choose_algorithm
convolution_shape_check
drop
find
find_algorithm
for
get_best_algorithm
get_workspace_size
insert
invoke
is
lazy_static
miopen_convolution
miopen_convolution_add_bias
miopen_convolution_backward
miopen_convolution_backward_bias
miopen_convolution_backward_weight
miopen_convolution_forward
miopen_convolution_transpose
miopen_convolution_transpose_backward
miopen_convolution_transpose_backward_weight
miopen_convolution_transpose_forward
miopen_depthwise_convolution
miopen_depthwise_convolution_backward
miopen_depthwise_convolution_backward_weight
miopen_depthwise_convolution_forward
narrow_group
raw_miopen_convolution_backward_weight_out
raw_miopen_convolution_forward_out
raw_miopen_depthwise_convolution_backward_weight_out
raw_miopen_depthwise_convolution_forward_out
set_convolution_params
static_assert
wsscache
GemmBlockSparseMicrokernelTester
a_stride
a_zero_point
b_zero_point
biasn
c_stride
col_block_size
fill_block_sparse_weights
iterations
k
ks
m
mr
multiplier
n
nr
print_matrix_f32
print_matrix_u8
pytorch_qnnp_conv_dynamic_quantization_params
qmax
qmin
row_block_size
sparsity
test
test_packed
col2im_shape_check
im2col_shape_check
Quantizer
QuantizerPtr
dequantize
equal_to
intrusive_from_this
qscheme
quantize
scalar_type
should
since
lazy_static
sigmoid_q8
max_pool1d_impl
max_pool1d_kernel
register_dispatch
lazy_static
memory_overlap_test_contiguous_expanded_tensor
memory_overlap_test_contiguous_tensor
memory_overlap_test_non_contiguous_expanded_tensor
memory_overlap_test_non_contiguous_tensor
memory_overlap_test_scalar_expanded
memory_overlap_test_tensor_expanded
lazy_static
define_dispatch
fake_quantize_learnable_per_tensor_affine
fake_quantize_learnable_per_tensor_affine_backward
fake_quantize_per_tensor_affine
fake_quantize_per_tensor_affine_cachemask
fake_quantize_per_tensor_affine_cachemask_backward
get_zero_point_from_tensor
GET_BLOCKS
MIOpenPoolType
create_mio_pen_handle
destroy_mio_pen_handle
get_miopen_handle
declare_pytorch_u8clamp_ukernel_function
Atest
Float
atest
atest_add_operators
atest_eq_operators
atest_fmod_tensor_operators
atest_ge_operators
atest_gt_operators
atest_le_operators
atest_logical_and_operators
atest_logical_or_operators
atest_logical_xor_operators
atest_lt_operators
atest_max_operators
atest_min_operators
atest_ne_operators
atest_operators
atest_sigmoid_backward_operator
run_binary_ops_test
set_up
trace
unit_binary_ops_test
beta_backward
compute_internal_gradients
gamma_backward
group_norm_backward_kernel_impl
group_norm_backward_kernel_impl_internal
group_norm_kernel_impl
group_norm_kernel_impl_internal
register_dispatch
add_op_small_batch
add_op_small_batch_with_a_scale
add_op_small_batch_with_a_stride
add_op_small_batch_with_a_zero_point
add_op_small_batch_with_b_scale
add_op_small_batch_with_b_stride
add_op_small_batch_with_b_zero_point
add_op_small_batch_with_qmax
add_op_small_batch_with_qmin
add_op_small_batch_with_y_scale
add_op_small_batch_with_y_stride
add_op_small_batch_with_y_zero_point
add_op_strided_batch
add_op_strided_batch_with_a_scale
add_op_strided_batch_with_a_zero_point
add_op_strided_batch_with_b_scale
add_op_strided_batch_with_b_zero_point
add_op_strided_batch_with_qmax
add_op_strided_batch_with_qmin
add_op_strided_batch_with_y_scale
add_op_strided_batch_with_y_zero_point
add_op_unit_batch
add_op_unit_batch_with_a_scale
add_op_unit_batch_with_a_zero_point
add_op_unit_batch_with_b_scale
add_op_unit_batch_with_b_zero_point
add_op_unit_batch_with_qmax
add_op_unit_batch_with_qmin
add_op_unit_batch_with_y_scale
add_op_unit_batch_with_y_zero_point
add_op_zero_batch
import_type
type_custom_printer_basic
type_custom_printer_contained_types
type_custom_printer_named_tuples
type_equality_class_basic
type_equality_class_inequality
type_equality_interface
type_equality_interface_inequality
type_equality_named_tuple
type_equality_tuple
copy_
copy_impl
copy_same_type_transpose
copy_transpose_valid
declare_dispatch
define_dispatch
is_supported_device
lazy_static
VGG
divide_round_up
lazy_static
mobile_netv1
mobile_netv2
res_net18
res_net50
round_up
sgemm
sgemm_5x8_neon
sgemm_6x8_neon
sgemm_6x8_psimd
sgemm_benchmark
sgemm_in_l1
shuffle_netv1g1
shuffle_netv1g2
shuffle_netv1g3
shuffle_netv1g4
shuffle_netv1g8
shuffle_netv2x05
shuffle_netv2x10
shuffle_netv2x15
shuffle_netv2x20
squeeze_netv10
squeeze_netv11
dispatch_key_allowlist_check
op_allowlist_check
op_allowlist_contains
op_allowlist_contains_name_in_schema
schema_allowlist_check
lerp_kernel_scalar
lerp_kernel_tensor
register_dispatch
Block
lazy_static
mean
pytorch_q8gavgpool_ukernel_up8xm_sse2
independent
th_storage_free
th_storage_resize_bytes
th_storage_retain
adapative_avg_pool2d_backward_kernel_impl
adaptive_avg_pool2d_kernel_impl
cpu_adaptive_avg_pool
cpu_adaptive_avg_pool_backward
cpu_adaptive_avg_pool_backward_channels_last
cpu_adaptive_avg_pool_channels_last
register_dispatch
multinomial_with_replacement_apply
multinomial_with_replacement_kernel_impl
register_dispatch
sub_zero_point
Vectorizedi
invert
lazy_static
operator_m_256i
VulkanImplRegistrar
is_vulkan_available
lazy_static
vulkan_copy
addmm_out_cuda_impl
baddbmm_cuda
baddbmm_cuda_mut
baddbmm_out_cuda
baddbmm_out_cuda_impl
bmm_cuda
bmm_out_cuda
dot_check
dot_cuda
lazy_static
prepare_batch_matrix_for_cublas
prepare_matrix_for_cublas
vdot_cuda
lazy_static
broadcast_test
test_empty_tensor
test_explicit_dim_basic
test_explicit_dim_with_mismatched_sizes
test_explicit_dim_with_scalar
test_in_2basic
test_in_2expand_error
test_in_2with_scalar
test_in_3basic
test_in_3expand_error
test_in_3with_scalar
test_out_2basic
test_out_2mismatched_sizes
test_out_2old_fallback
test_out_2with_scalar
test_out_3basic
test_out_3mismatched_sizes
test_out_3old_fallback
test_out_3with_scalar
mkldnn_clone
mkldnn_reshape
mkldnn_transpose
mkldnn_transpose_mut
mkldnn_view
lazy_static
QualifiedName
atoms
cache_accessors
eq
hash
is_prefix_of
join
lazy_static
name
prefix
qualified_name
clamp_op_qmin_and_qmax_equal_uint8_max
clamp_op_small_batch
clamp_op_unit_batch
clamp_op_unit_batch_with_qmax
clamp_op_unit_batch_with_qmin
clamp_op_zero_batch
pow_tensor_scalar_kernel
pow_tensor_scalar_optimized_kernel
pow_tensor_tensor_kernel
register_dispatch
arange_kernel
for
linspace_kernel
register_dispatch
FloatVecReturnType
IntVecReturnType
SizeType
ValueType
VecInternalMaskType
VecInternalType
VectorizedQuint8Alt0
VectorizedQuint8Alt1
dequantize
dump
float_num_vecs
int_num_vecs
loadu
maximum
minimum
of
quantize
relu
relu6
requantize_from_int
size
store
vec0
vec1
widening_subtract
SparseCsrMKLInterface
drop
is_mkl_int32_index
sparse_mm
sparse_mm_mkl
sparse_mm_mkl_template
complex_kernel
polar_kernel
register_dispatch
max_quantized_cpu
min_quantized_cpu
sort_quantized_cpu
sort_quantized_cpu_stable
adaptive_avg_pool1d
adaptive_max_pool1d
avg_pool1d
check1d
max_pool1d_with_indices
max_pool2d
max_pool3d
ConvParams
check_cudnn_depthwise_workload
check_shape_forward
conv1d
conv1d_with_bias_opt
conv2d
conv2d_with_bias_opt
conv3d
conv3d_with_bias_opt
conv_transpose1d
conv_transpose2d
conv_transpose3d
convolution_a
convolution_b
convolution_backward_overrideable
convolution_c
convolution_double_backward
convolution_mode
convolution_nogroup
convolution_overrideable
convolution_same
define_dispatch
fmt
is_depthwise
is_dilated
is_padded
is_padding_neg
is_stride_nonpos
is_strided
needs_64bit_indexing_no_split
subtensor
subvariable
use_cpu_depthwise3x3_winograd
use_cudnn
use_cudnn_depthwise
use_miopen
use_mkldnn
use_nnpack
use_xnnpack
view1d_as_2d
view3d
view4d
lazy_static
Cache
Descriptor
Flags
Handle
Pipeline
PipelineBarrier
PipelineBarrierStage
PipelineCache
PipelineDescriptor
PipelineFactory
PipelineFactoryHasher
PipelineLayout
PipelineLayoutDescriptor
PipelineLayoutFactory
PipelineLayoutFactoryHasher
PipelineObject
PipelineStageType
bitflags
create_pipeline_cache
defines
eq
invoke
operator_bool
purge
retrieve
lazy_static
cufft_clear_plan_cache_impl
cufft_get_plan_cache
cufft_get_plan_cache_max_size_impl
cufft_get_plan_cache_size_impl
cufft_set_plan_cache_max_size_impl
exec_cufft_plan
exec_fft
fft_apply_normalization
fft_apply_normalization_out
fft_c2c_cufft
fft_c2c_cufft_out
fft_c2r_cufft
fft_c2r_cufft_out
fft_normalization_scale
fft_r2c_cufft
fft_r2c_cufft_out
lazy_static
run_cufft
QUANTILE_INTERPOLATION_MODE
SortFn
TopkFn
declare_dispatch
define_dispatch
get_quantile_interpolation_mode
kthvalue_a
kthvalue_b
kthvalue_out
kthvalue_out_cpu
kthvalue_out_impl_cpu
lazy_static
median_a
median_b
median_cpu
median_impl
median_out
median_out_cpu
median_with_indices_impl
msort
msort_out
nanmedian_a
nanmedian_b
nanmedian_cpu
nanmedian_out
nanmedian_out_cpu
nanquantile_a
nanquantile_b
nanquantile_c
nanquantile_d
nanquantile_out_a
nanquantile_out_b
nanquantile_out_c
nanquantile_out_d
quantile_a
quantile_b
quantile_c
quantile_d
quantile_impl
quantile_out_a
quantile_out_b
quantile_out_c
quantile_out_d
quick_select_template
sort_cpu
sort_cpu_stable
sort_out_cpu
sort_out_cpu_stable
LinearOpContext
LinearOpContextPacked
LinearOpContextUnpacked
State
addmm
available
create
lazy_static
linear_prepack
linear_run
mm
pack_biases
pack_weights
run
unpack
usable
lazy_static
register_dispatch
renorm_scale_factor_impl
sobol_engine_draw
sobol_engine_ff
sobol_engine_initialize_state
sobol_engine_scramble
backward
data
fw_primal
is_leaf
requires_grad
retain_grad
retains_grad
set_data
version
declare_pytorch_x8lut32norm_ukernel_function
contiguous_if_zero_in_strides
bit_length
bitsubseq
cdot_pow2
lazy_static
rightmost_zero
MetalGuardImpl
block
destroy_event
device_count
exchange_device
exchange_stream
get_device
get_stream
lazy_static
query_event
record
set_device
ty
unchecked_set_device
QEmbeddingUnpackWeights
lazy_static
qembeddingbag_2bit_unpack
qembeddingbag_4bit_unpack
qembeddingbag_byte_unpack
qembeddingbag_nbit_unpack_helper
register_embedding_params
run
unpack
addcdiv_a
addcdiv_b
addcdiv_out
addcmul_a
addcmul_b
addcmul_out
declare_dispatch
define_dispatch
lazy_static
pytorch_qnnp_requantize_q31_ssse3
copy_kernel
register_dispatch
add_out
add_scalar_out
define_dispatch
lazy_static
qadd
qadd_out
qadd_scalar
qadd_scalar2
qadd_scalar_out
qadd_scalar_tensor
qadd_scalar_tensor_out
qnnpack_add
declare_pytorch_sgemm_ukernel_function
pytorch_qnnp_fp32_clamping_params
caffe_2to_pytorch_external_data
caffe_2to_pytorch_mutual_resizes
caffe_2to_pytorch_non_pod
caffe_2to_pytorch_nullptr
caffe_2to_pytorch_op
caffe_2to_pytorch_partially_initialized
caffe_2to_pytorch_simple
caffe_2to_pytorch_simple_legacy
pytorch_to_caffe2_inplace_strided
pytorch_to_caffe2_mutual_resizes
pytorch_to_caffe2_non_regular_tensor
pytorch_to_caffe2_nullptr
pytorch_to_caffe2_op
pytorch_to_caffe2_shared_storage_read
pytorch_to_caffe2_shared_storage_write
pytorch_to_caffe2_strided
Capsule
ClassTypePtr
CompAliasedIValues
ComplexHolder
Ctx
CustomClassHolder
DeviceDescriptor
HashAliasedIValue
HashAliasedIValueMap
HashAliasedIValues
IValue
IValueComparator
IValueFormatter
IValueTag
OptionalArray
Payload
StrongTypePtr
TypePtr
WeakIValue
and
assign_from
caster
check_custom_class_type
check_object_sort_schema
clear_to_none
collect_all
collect_any
copy_
create
deepcopy
default
destroy
drop
dump
eq
equals
erased
extract_data_ptrs
fast_equals_for_container
fmt
format_set_of_devices
get_attr
get_class_converter
get_custom_class_type
get_custom_class_type_impl
get_custom_class_type_map
get_greater_than_comparator
get_less_than_comparator
get_sub_values
hash
hence
in
internal_to_pointer
invoke
is
is_alias_of
is_blob
is_bool
is_bool_list
is_capsule
is_complex_double
is_complex_double_list
is_custom_class
is_device
is_double
is_double_list
is_enum
is_future
is_generator
is_generic_dict
is_int
is_int_list
is_list
is_none
is_object
is_ptr_type
is_py_object
is_quantizer
is_rref
is_same_identity
is_scalar
is_storage
is_stream
is_string
is_tensor
is_tensor_list
is_tuple
is_undefined_tensor
lazy_static
lock
make_capsule
map
move_from
move_to_intrusive_ptr
name
null_to_undefined_tensor
of
operator_optional_array_ref_t
overlaps
print_complex
print_dict
print_list
print_maybe_annotated_dict
print_maybe_annotated_list
ptr_equal
qualified_class_name
report_to_tensor_type_error
repr
resize_object
set_attr
swap
tag_kind
to_blob
to_bool
to_bool_list
to_capsule
to_complex_double
to_complex_double_list
to_complex_double_vector
to_custom_class
to_device
to_dimname
to_double
to_double_list
to_double_vector
to_future
to_int
to_int_list
to_int_vector
to_intrusive_ptr
to_ivalue
to_layout
to_list
to_list_ref
to_memory_format
to_none
to_optional
to_optional_string_ref
to_qscheme
to_quantizer
to_rref
to_scalar
to_scalar_type
to_storage
to_string
to_string_ref
to_string_view
to_tensor
to_tensor_list
to_tensor_vector
to_tuple
ty
uninitialized
unqualified_class_name
unsafe_remove_attr
unsafe_to_tensor_impl
use_count
visit
weak_use_count
you
pytorch_q8gemm_ukernel_8x8_neon
lazy_static
arange_cpu_out
declare_dispatch
define_dispatch
linspace_cpu_out
logspace_cpu_out
range_cpu_out
declare_pytorch_supdwconv_ukernel_function
pytorch_qnnp_fp32_clamping_params
LinearPackedParamsBase
LinearPackedSerializationType
apply
apply_dynamic
apply_dynamic_relu
apply_relu
bias
set_bias
unpack
pytorch_qnnp_requantize_precise_sse4
PackedConvWeight
PackedEmbeddingBagWeight
PackedLinearWeight
PackedLinearWeightFp16
apply
apply_dynamic
apply_dynamic_impl
apply_dynamic_relu
apply_impl
apply_relu
bias
bit_rate
convert_conv_weights_to_channel_last_tensor
convert_conv_weights_to_channel_last_tensor2
convert_conv_weights_to_channel_last_tensor3
convert_int8_uint8
convert_to_channels_last3d_tensor
convert_uint8_int8
copy_icf_irst3d_tensor_to_channels_last3d_tensor
copy_to_channels_last3d_tensor
dilation
embeddingbag_4bit
embeddingbag_byte
for
get_bias_data
get_quantization_params
groups
is_channels_last3d
lazy_static
make_empty_affine_quantized_channels_last3d_tensor
make_empty_per_channel_affine_quantized_channels_last3d_tensor
make_fbgemm_conv_param
make_fbgemm_conv_param1
make_fbgemm_conv_param2
make_fbgemm_conv_param3
make_strided_qtensor_cpu
padding
prepack
register_conv_params
register_embedding_params
register_linear_params
set_bias
stride
transpose
transpose_conv_tensor_unpack_conversion
transpose_conv_tensor_unpack_conversion2
transpose_conv_tensor_unpack_conversion3
unpack
version
pytorch_q8vadd_ukernel_neon
normalize
CumFn
ReduceFn
ReduceFnFlag
ReduceNormFn
ReduceStdVarFunction
all_a
all_b
all_c
all_d
all_out_a
all_out_b
amax
amax_out
amin
amin_out
any_a
any_b
any_c
any_d
any_out_a
any_out_b
casting
cpu_equal
cummax_a
cummax_b
cummax_cummin_helper
cummax_helper_cpu
cummax_out_a
cummax_out_b
cummaxmin_backward
cummin_a
cummin_b
cummin_helper_cpu
cummin_out_a
cummin_out_b
cumprod_a
cumprod_b
cumprod_backward
cumprod_c
cumprod_cpu
cumprod_d
cumprod_out_a
cumprod_out_b
cumprod_out_cpu
cumsum_a
cumsum_b
cumsum_c
cumsum_cpu
cumsum_mut
cumsum_out_a
cumsum_out_b
cumsum_out_cpu
declare_dispatch
define_dispatch
diff
diff_check
diff_check_compatible_shape
diff_helper
diff_out
diff_out_helper
dist
get_dtype_from_result
get_dtype_from_self
gradient_a
gradient_b
gradient_c
gradient_d
gradient_dim_preprocess
gradient_e
gradient_f
gradient_g
gradient_helper
gradient_helper_float
i
is
isnan
logcumsumexp
logcumsumexp_cpu
logcumsumexp_out_a
logcumsumexp_out_b
logcumsumexp_out_cpu
logcumsumexp_with_dim
logsumexp_a
logsumexp_b
logsumexp_out
logsumexp_out_impl
logsumexp_ouw_With_dims
mean
mean_cpu_gpu_a
mean_cpu_gpu_b
mean_out
mean_out_cpu_gpu
nansum_a
nansum_b
nansum_out
norm_a
norm_b
norm_c
norm_d
norm_e
norm_f
norm_g
norm_h
norm_out_a
norm_out_b
norm_out_c
norm_out_d
norm_out_e
options_to_value_type
pre_check_gradient
prepend_append_on_dim
prod_a
prod_b
prod_c
prod_out_a
prod_out_b
prod_out_impl
reversed_cumsum
squeeze_multiple
std_a
std_b
std_c
std_d
std_e
std_mean_a
std_mean_b
std_mean_c
std_mean_d
std_mean_e
std_out_a
std_out_b
std_out_c
std_out_d
std_var_all_cpu
std_var_mean_out
std_var_out
sum_a
sum_b
sum_c
sum_out_a
sum_out_b
tensor
trace_cpu
value_selecting_reduction_backward
var_a
var_b
var_c
var_d
var_e
var_mean_a
var_mean_b
var_mean_c
var_mean_d
var_mean_e
var_mean_out
var_out_a
var_out_b
var_out_c
var_out_d
batch_size
channels_size
height_size
width_size
pytorch_qnnp_create_add_nc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_add_nc_q8
pytorch_qnnp_status
compute_strides
fp16_t
fp_16to_fp32
fp_32to_fp16
get_command_buffer_from_tensor
get_tensor_impl_storage
make_mtl_buffer
make_mtl_buffer_bytes
make_tensor
nc4tonchw
nchw_tonc4
max_to_with_float
max_to_with_int
max_val_with_float
max_val_with_int
min_from_with_float
min_from_with_int
min_val_with_float
min_val_with_int
test_random
test_random_from_to
pytorch_q8avgpool_ukernel_up8x9_neon
compute_linear_combination_cpu_kernel
register_dispatch
pytorch_qnnp_requantize_gemmlowp_sse2
C10_REGISTER_CREATOR
create_c10_threadpool
get_num_interop_threads
get_pool
launch
launch_no_thread_state
lazy_static
set_num_interop_threads
u8clamp_neon_inplace
u8clamp_neon_n_div_8
u8clamp_neon_n_eq_8
u8clamp_neon_n_gt_8
u8clamp_neon_n_lt_8
u8clamp_neon_qmax
u8clamp_neon_qmin
u8clamp_sse2_inplace
u8clamp_sse2_n_div_8
u8clamp_sse2_n_eq_8
u8clamp_sse2_n_gt_8
u8clamp_sse2_n_lt_8
u8clamp_sse2_qmax
u8clamp_sse2_qmin
mkldnn_sigmoid
mkldnn_sigmoid_mut
mkldnn_tanh
mkldnn_tanh_mut
pytorch_u8maxpool_ukernel_16x9p8q_neon
lazy_static
adaptive_avg_pool2d
adaptive_avg_pool2d_backward_cpu
adaptive_avg_pool2d_backward_out_cpu
adaptive_avg_pool2d_backward_out_cpu_template
adaptive_avg_pool2d_cpu
adaptive_avg_pool2d_out_cpu
adaptive_avg_pool2d_out_cpu_template
define_dispatch
ID
SparseCsrTensorImpl
and
col_indices
crow_indices
int32
nnz
resize_as_sparse_csr_tensor
set_member_tensors
sparse_csr_tensor_set_to_device_type
values
UnfoldBackwardFn
declare_dispatch
define_dispatch
make_unfold_backward_iter_over_grad_in
make_unfold_backward_iter_over_grad_out
unfold_backward
allocate_workspace
deallocate_workspace
init_nnpack
lazy_static
nnp_padding
nnp_size
nnpack_available
nnpack_spatial_convolution
nnpack_spatial_convolution_backward
nnpack_spatial_convolution_backward_weight
nnpack_threadpool
HIPStreamMasqueradingAsCUDA
Unchecked
device
device_index
eq
fmt
get_current_hip_stream_masquerading_ascuda
get_default_hip_stream_masquerading_ascuda
get_stream_from_external_masquerading_ascuda
get_stream_from_pool_masquerading_ascuda
hip_stream
id
operator_hip_stream_t
operator_stream
pack
priority
priority_range
query
set_current_hip_stream_masquerading_ascuda
stream
synchronize
unpack
unwrap
debug_has_internal_overlap
is_pinned
pin_memory
QLinearInt8
apply
apply_impl
apply_out
apply_relu
apply_relu_out
lazy_static
register_linear_params
run
lazy_static
unsupported_random_op
unsupported_random_op_mut
get_env_num_threads
get_env_var
get_parallel_info
intraop_default_num_threads
cudnn_batch_norm
cudnn_batch_norm_backward
expand_scale
BCSRMatrix
generate_block_csr_matrix
print
BinaryClampFnAlpha
BinaryFn
BinaryFnAlpha
BinaryFnDouble
Output
StructuredBinaryFn
StructuredBinaryFnAlpha
add
add_mut_tensor_scalar_scalar
add_relu
add_relu_impl
add_relu_mut_tensor_tensor_with_alpha
add_relu_out
alpha_check
and_tensor_scalar
and_tensor_tensor
bitwise_and
bitwise_and_mut_tensor_scalar
bitwise_and_mut_tensor_tensor
bitwise_and_out
bitwise_and_out_tensor_scalar
bitwise_and_tensor_scalar
bitwise_or
bitwise_or_mut_tensor_scalar
bitwise_or_out
bitwise_or_out_tensor_scalar
bitwise_or_tensor_mut
bitwise_or_tensor_scalar
bitwise_xor
bitwise_xor_mut
bitwise_xor_mut_tensor_scalar
bitwise_xor_out_tensor_scalar
bitwise_xor_out_tensor_tensor
bitwise_xor_tensor_scalar
check_convert
comparison_op_mut
comparison_op_mut_tensor_scalar
comparison_op_out
comparison_op_out_mut_with_stub
comparison_op_tensor_scalar
comparison_op_tensor_tensor
copysign
copysign_mut_tensor_scalar
copysign_out
create_binary_meta_func
create_binary_torch_impl_func
declare_dispatch
define_dispatch
div
div_mut_tensor_scalar
div_mut_tensor_scalar_with_rounding_mode
div_tensor_scalar_with_rounding_mode
divide
divide_mut_tensor_scalar
divide_mut_tensor_scalar_with_rounding_mode
divide_mut_tensor_tensor
divide_mut_tensor_tensor_with_rounding_mode
divide_out
divide_out_with_rounding_mode
divide_tensor_scalar
divide_tensor_scalar_with_rounding_mode
divide_tensor_tensor_with_rounding_mode
eq_mut_tensor_scalar
eq_mut_tensor_tensor
eq_out
eq_out_tensor_scalar
eq_tensor_scalar
eq_tensor_tensor
floor_divide
floor_divide_inplace
floor_divide_mut
floor_divide_out
fmin_out
fmin_tensor_tensor
fmod_mut_tensor_scalar
fmod_mut_tensor_tensor
fmod_out
fmod_out_tensor_scalar
fmod_tensor_scalar
fmod_tensor_tensor
ge_mut_tensor_scalar
ge_mut_tensor_tensor
ge_out_tensor_scalar
ge_out_tensor_tensor
ge_tensor_scalar
ge_tensor_tensor
greater_equal_mut_tensor_scalar
greater_equal_mut_tensor_tensor
greater_equal_out_tensor_scalar
greater_equal_out_tensor_tensor
greater_equal_tensor_scalar
greater_equal_tensor_tensor
greater_mut_tensor_scalar
greater_mut_tensor_tensor
greater_out_tensor_scalar
greater_out_tensor_tensor
greater_tensor_scalar
greater_tensor_tensor
gt_mut_tensor_scalar
gt_mut_tensor_tensor
gt_out_tensor_scalar
gt_out_tensor_tensor
gt_tensor_scalar
gt_tensor_tensor
iand_mut_tensor_scalar
iand_mut_tensor_tensor
ilshift_mut_tensor_scalar
ilshift_mut_tensor_tensor
ior_mut_tensor_scalar
ior_mut_tensor_tensor
irshift_mut_tensor_scalar
irshift_mut_tensor_tensor
ixor_mut_tensor_scalar
ixor_mut_tensor_tensor
lazy_static
ldexp_mut_tensor_tensor
ldexp_out_tensor_tensor
ldexp_tensor_tensor
le_mut_tensor_scalar
le_mut_tensor_tensor
le_out_tensor_scalar
le_out_tensor_tensor
le_tensor_scalar
le_tensor_tensor
less_equal_mut_tensor_scalar
less_equal_mut_tensor_tensor
less_equal_out_tensor_scalar
less_equal_out_tensor_tensor
less_equal_tensor_scalar
less_equal_tensor_tensor
less_mut_tensor_scalar
less_mut_tensor_tensor
less_out
less_out_tensor_scalar
less_tensor_scalar
less_tensor_tensor
logical_and_mut_tensor_scalar
logical_and_mut_tensor_tensor
logical_and_out
logical_and_out_tensor_tensor
logical_and_tensor_scalar
logical_and_tensor_tensor
logical_or_mut_tensor_scalar
logical_or_mut_tensor_tensor
logical_or_out
logical_or_out_mut_tensor_tensor
logical_or_tensor_scalar
logical_or_tensor_tensor
logical_xor_mut_tensor_scalar
logical_xor_mut_tensor_tensor
logical_xor_out_mut_tensor_tensor
logical_xor_out_tensor_tensor
logical_xor_tensor_scalar
logical_xor_tensor_tensor
logit_backward
logit_backward_out
lshift
lshift_tensor_scalar
lt
lt_mut_tensor_scalar
lt_mut_tensor_tensor
lt_out
lt_out_tensor_scalar
lt_tensor_scalar
max_out
max_tensor_tensor
min_out
min_tensor_tensor
mul
mul_mut
multiply
multiply_mut
multiply_mut_scalar
multiply_out
multiply_tensor_scalar
ne_mut_tensor_scalar
ne_mut_tensor_tensor
ne_out
ne_out_tensor_scalar
ne_tensor_scalar
ne_tensor_tensor
not_equal_mut_tensor_scalar
not_equal_mut_tensor_tensor
not_equal_out_tensor_scalar
not_equal_out_tensor_tensor
not_equal_tensor_scalar
not_equal_tensor_tensor
or_tensor_scalar
or_tensor_tensor
promotion
remainder_mut
remainder_out
remainder_scalar_tensor
remainder_tensor_scalar
rshift_tensor_scalar
rshift_tensor_tensor
rsub_tensor_scalar_scalar
rsub_tensor_tensor_scalar
sigmoid_backward
sigmoid_backward_out
special_xlog1py_out_scalar_tensor
special_xlog1py_out_tensor_scalar
special_xlog1py_scalar_tensor
special_xlog1py_tensor_scalar
sub
sub_check
sub_check_with_scalar
sub_mut
subtract
subtract_mut
subtract_mut_tensor_scalar_scalar
subtract_out
subtract_tensor_scalar_scalar
tanh_backward
tanh_backward_out
test_serialization_subcmul
true_divide
true_divide_mut
true_divide_mut_tensor_scalar
true_divide_out
true_divide_tensor_scalar
wrapped_scalar_tensor_and_check_convert
xlogy_mut_tensor_scalar
xlogy_mut_tensor_tensor
xlogy_out
xlogy_out_scalar_tensor
xlogy_out_tensor_scalar
xlogy_scalar_tensor
xlogy_tensor_scalar
xlogy_tensor_tensor
xor
xor_tensor_scalar
lazy_static
cdist
cdist_backward
cdist_forward
cdist_impl
cosine_similarity
declare_dispatch
define_dispatch
euclidean_dist
lazy_static
pairwise_distance
pdist
pdist_backward
pdist_forward
Block
lazy_static
reflection_pad2d
Output
SizeType
ValueType
VectorizedComplexDouble
abs
abs_2
acos
add
angle
arange
asin
atan
atan2
bitand
bitor
bitxor
blend
blendv
ceil
conj
cos
cosh
div
eq
erf
erfc
exp
expm1
floor
hypot
igamma
igammac
imag
loadu
log
log10
log1p
log2
map
maximum
minimum
mul
ne
neg
nextafter
operator_m_256d
pow
real
reciprocal
round
rsqrt
set
sgn
sin
sinh
size
sqrt
store
sub
tan
tanh
trunc
DispatchKeyExtractor
MultiDispatchKeySet
check_invariants
compute_dispatch_key_set
deregister_schema
dump_state
get_dispatch_key_set_boxed
get_dispatch_key_set_unboxed
invoke
make
make_uninitialized
multi_dispatch_key_set
of
register_schema
set_operator_has_fallthrough_for_key
pytorch_u8maxpool_ukernel_sub16_sse2
define_dispatch
hardsigmoid_quantized_cpu
qnnpack_hardsigmoid
add_override
backend_extension_test_register_op
empty_override
empty_strided_override
lazy_static
lazy_static
QConv1dUnpackWeightsInt8
QConvDilation
QConvGroups
QConvOutputPadding
QConvPadding
QConvStride
QConvTranspose
QConvUnpackWeightsInt8
lazy_static
run
unpack
gemmlowp_sse_mul_s32
gemmlowp_sse_rdivbypo2_s32
gemmlowp_sse_vqrdmulh_s32
mkldnn_bf16_device_check
VGG
lazy_static
max_pooling_u8
shuffle_net
squeeze_netv10
squeeze_netv11
pytorch_qnnp_create_global_average_pooling_nwc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_global_average_pooling_nwc_q8
pytorch_qnnp_status
CPUCapability
DispatchStub
DispatchStubImpl
RegisterCUDADispatch
RegisterHIPDispatch
choose_cpu_impl
compute_cpu_capability
get_call_ptr
get_cpu_capability
lazy_static
name
to
deserialize_conv
lazy_static
parse_conv_serialized_state
serialize_conv
CastImpl
Vectorized
abs
acos
angle
arange
asin
atan
atan2
binary_pred
binary_pred_bool
bitwise_binary_op
blend
blendv
ceil
clamp
clamp_max
clamp_min
conj
conversion
copysign
cos
cosh
default
eq
erf
erfc
erfinv
exp
expm1
floor
fmadd
fmod
frac
ge
gt
here
hypot
i0
i0e
igamma
igammac
imag
is
isnan
lazy_static
le
lgamma
loadu
log
log10
log1p
log2
lt
map
maximum
maximum_complex
minimum
minimum_complex
ne
neg
nextafter
operator_const_tptr
operator_tptr
pow
real
reciprocal
round
rsqrt
set
sgn
sin
sinh
size
so
sqrt
store
tan
tanh
trunc
without
zero_mask
pytorch_q8gemm_dq_sparse_1x4_ukernel_8x4_packeda_sse2
csr_matmult
csr_matmult_maxnnz
csr_to_coo
sparse_matmul_kernel
sparse_sparse_matmul_cpu
AddrFn
KronImpl
LinalgVectorNormFn
TupleTensorRefs3
UnpackPivotsFn
addbmm_a
addbmm_b
addbmm_impl
addbmm_out
addmm_impl_cpu
addr_a
addr_b
addr_out
allocate_buffer
backward_analytic_function_of_a_matrix
baddbmm_cpu_a
baddbmm_cpu_b
baddbmm_cpu_kernel
baddbmm_out_cpu
blob_to_tensor
bmm_cpu
bmm_out_cpu
bmm_out_or_baddbmm
build_addr_iter
chain_matmul
chain_matmul_out
check_1d
check_addr_scalar
check_str_ord_valid
compute_t1
compute_t12
compute_t18
compute_t18_scale_square
compute_t2
compute_t4
compute_t8
declare_dispatch
define_dispatch
det
dot_out
fill_matrix_powers
frobenius_norm_a
frobenius_norm_b
frobenius_norm_out
ger
ger_out
inner
inner_out
is
kron
kron_out
lazy_static
linalg_cond_check_ord
linalg_cond_empty_matrix
linalg_cond_helper
linalg_cond_out_a
linalg_cond_out_b
linalg_cond_with_frobenius_or_nuclear_norms
linalg_cond_with_numerical_or_none_norms
linalg_det
linalg_det_out
linalg_matrix_norm_a
linalg_matrix_norm_b
linalg_matrix_norm_out_a
linalg_matrix_norm_out_b
linalg_matrix_power
linalg_matrix_power_impl
linalg_matrix_power_out
linalg_matrix_rank_a
linalg_matrix_rank_b
linalg_matrix_rank_out_a
linalg_matrix_rank_out_b
linalg_matrix_rank_out_helper
linalg_multi_dot
linalg_multi_dot_out
linalg_norm_matrix_out
linalg_norm_out_impl
linalg_norm_out_with_frobenius_and_nuclear_norms
linalg_norm_out_with_numerical_or_none_norms
linalg_norm_with_frobenius_and_nuclear_norms
linalg_norm_with_numerical_or_none_norms
linalg_pinv_a
linalg_pinv_b
linalg_pinv_out_a
linalg_pinv_out_b
linalg_slogdet
linalg_slogdet_out
linalg_tensorinv
linalg_tensorinv_out
linalg_tensorsolve
linalg_tensorsolve_out
linalg_vector_norm
linalg_vector_norm_impl
linalg_vector_norm_out
logdet
lu_det_p_diag_u
lu_unpack
lu_unpack_out
make_dim_list
math_addr
math_addr_out
matmul_a
matmul_b
matmul_out
matrix_chain_multiplication
matrix_chain_order
matrix_exp
matrix_exp_backward
matrix_power
matrix_power_out
matrix_rank_a
matrix_rank_b
mexp
mexp_impl
multi_dot_impl
norm_min_max
nuclear_norm_a
nuclear_norm_b
nuclear_norm_out_a
nuclear_norm_out_b
operator_1_norm
outer
outer_out
pinverse
slogdet
vdot_out
legacy_cat_wrap_dim
legacy_cat_wrap_dim_with_tensors
maybe_wrap_dim_with_ptensor
maybe_wrap_dim_with_tensor_list
maybe_wrap_dim_with_tensor_sizes
maybe_wrap_dim_with_wrap_scalar
maybe_wrap_dims
maybe_wrap_dims_n
wrap_all_dims
declare_pytorch_u8maxpool_ukernel_function
THBFloat16Storage
THBoolStorage
THByteStorage
THCharStorage
THComplexDoubleStorage
THComplexFloatStorage
THDoubleStorage
THFloatStorage
THHalfStorage
THIntStorage
THLongStorage
THQInt32Storage
THQInt8Storage
THQUInt8Storage
THShortStorage
lazy_static
basic_test_cpu
basic_test_factory_methods
basic_test_half_cpu
basic_testcuda
send_context
test
test_abs_value
test_add
test_adding_avalue_with_scalar
test_copy
test_copy_broadcasting
test_dispatch
test_indexing_by_scalar
test_indexing_by_zerodim_tensor
test_indexing_mixed_device
test_int_array_ref_expansion
test_is_contiguous
test_load_of_adds_with_copy
test_loads_of_adds
test_mm
test_negative_dim
test_ones_and_dot
test_permute
test_randperm
test_resize
test_select
test_sort
test_squeeze
test_to_cfloat
test_to_string
test_view
test_zero_dim
test_zeros
lazy_static
vld1_f16_x2
vld1_f32_x2
vld1_f64_x2
vld1_p16_x2
vld1_p64_x2
vld1_p8_x2
vld1_s16_x2
vld1_s32_x2
vld1_s64_x2
vld1_s8_x2
vld1_u16_x2
vld1_u32_x2
vld1_u64_x2
vld1_u8_x2
vld1q_f16_x2
vld1q_f32_x2
vld1q_f64_x2
vld1q_p16_x2
vld1q_p64_x2
vld1q_p8_x2
vld1q_s16_x2
vld1q_s32_x2
vld1q_s64_x2
vld1q_s8_x2
vld1q_u16_x2
vld1q_u32_x2
vld1q_u64_x2
vld1q_u8_x2
vst1_f16_x2
vst1_f32_x2
vst1_f64_x2
vst1_p16_x2
vst1_p64_x2
vst1_p8_x2
vst1_s16_x2
vst1_s32_x2
vst1_s64_x2
vst1_s8_x2
vst1_u16_x2
vst1_u32_x2
vst1_u64_x2
vst1_u8_x2
vst1q_f16_x2
vst1q_f32_x2
vst1q_f64_x2
vst1q_p16_x2
vst1q_p64_x2
vst1q_p8_x2
vst1q_s16_x2
vst1q_s32_x2
vst1q_s64_x2
vst1q_s8_x2
vst1q_u16_x2
vst1q_u32_x2
vst1q_u64_x2
vst1q_u8_x2
MetalTensorImplStorage
copy_data_to_host
defined
dim
impl_
numel
set_data_from_host
sizes
strides
texture
Addmm
BaseOp
Conv2d
Hardtanh_
Mean
MobileNetV2
OpType
OpsList
almost_equal
call_op_by_handle
call_op_by_name
check_rtol
default
exactly_equal
make_stack
run
run_dual
to_string
vulkan_test_adaptive_avg_pool2d
vulkan_test_add
vulkan_test_add_cpu
vulkan_test_add_not4dim
vulkan_test_add_scalar
vulkan_test_addmm
vulkan_test_avg_pool2d
vulkan_test_cat
vulkan_test_clamp
vulkan_test_conv2d
vulkan_test_conv2d_dww_eights_on_cpu
vulkan_test_conv2d_prepack
vulkan_test_disabled_adaptive_avg_pool2d_2
vulkan_test_disabled_max_pool2d
vulkan_test_disabled_mobilenetv2
vulkan_test_hardtanh
vulkan_test_mean
vulkan_test_mm
vulkan_test_mul_scalar
vulkan_test_ops_list
vulkan_test_relu
vulkan_test_reshape
vulkan_test_reshape2
vulkan_test_select
vulkan_test_slice
vulkan_test_tensor5d
vulkan_test_tensor5d_transpose
vulkan_test_to_cpu
vulkan_test_unsqueeze
vulkan_test_upsample_nearest2d
vulkan_test_view
capture_epilogue
capture_prologue
checking
clone
clone_impl
create_cuda_generator
current_seed
device_type
get_default_cuda_generator
get_state
init_cuda_gen_vector
lazy_static
philox_cuda_state
philox_offset_per_thread
seed
set_current_seed
set_philox_offset_per_thread
set_state
was
expand_param_if_needed
Block
activation
activation_mut
clamp
clamp_mut
hardsigmoid
hardsigmoid_mut
hardswish
hardswish_mut
hardtanh
hardtanh_mut
lazy_static
relu
relu_mut
sigmoid
sigmoid_mut
CheckedFrom
common_device_check_failure
get_device_from_ptr
pytorch_qnnp_requantize_q31_sse4
RNNDescriptorParams
RNNDescriptors
RNNParams
TensorDescriptorListParams
copy_params
copy_params_and_permute
descriptor
descriptors
get_descs
get_num_weights
get_parameters
get_x_descs
get_y_descs
hidden_size
lstm_miopen
lstm_packed_miopen
miopen_impl
miopen_rnn
miopen_rnn_backward
miopen_rnn_backward_weight
num_directions
num_linear_layers
pack_hidden
permute_wei_for_miopen
rnn_descriptor
rnn_descriptor_sequence
set
set_algo
set_bidirectional
set_mode
unpack_hidden
view_or_copy_params
view_params
pytorch_qnnp_operator
pytorch_qnnp_status
Block
add_scalar
add_scalar_mut
add_tensor
add_tensor_mut
arithmetic_scalar
arithmetic_scalar_mut
arithmetic_tensor
arithmetic_tensor_mut
div_scalar
div_scalar_mut
div_tensor
div_tensor_mut
lazy_static
mul_scalar
mul_scalar_mut
mul_tensor
mul_tensor_mut
sub_scalar
sub_scalar_mut
sub_tensor
sub_tensor_mut
bincount_cpu
bincount_cpu_template
lazy_static
clog_define_log_debug
clog_define_log_error
clog_define_log_fatal
clog_define_log_info
clog_define_log_warning
lazy_static
sigmoid_op_small_batch
sigmoid_op_small_batch_with_qmax
sigmoid_op_small_batch_with_qmin
sigmoid_op_strided_batch
sigmoid_op_strided_batch_with_qmax
sigmoid_op_strided_batch_with_qmin
sigmoid_op_unit_batch
sigmoid_op_unit_batch_with_qmax
sigmoid_op_unit_batch_with_qmin
sigmoid_op_zero_batch
lazy_static
pytorch_q8dwconv_ukernel_mp8x25_per_channel_neon
align_down
align_up
div_up
is_signed_to_unsigned
safe_downcast
lazy_static
RequantizationTester
iterations
qmax
qmin
requantize_approximate
s
scale
shift_left
test_divide_by_po2with_rounding_away
test_divide_by_po2with_rounding_down
test_divide_by_po2with_rounding_up
test_exact_divide_bypo2
test_random_cases_against_reference
test_random_cases_approximate
test_random_cases_precise
test_special_cases
zero_point
apply_utils_test_10d
apply_utils_test_2d
apply_utils_test_3d
apply_utils_test_contiguous2d
apply_utils_test_medium3d
apply_utils_test_small2d
as
fill_tensor
test
InputMeta
cat_serial_kernel
cat_serial_kernel_impl
declare_dispatch
lazy_static
register_dispatch
MinValuesOps
and_kernel_impl
cpu_cum_base_kernel
cumprod_cpu_kernel
cumsum_cpu_kernel
lazy_static
logcumsumexp_cpu_kernel
max_values_kernel_impl
mean_kernel_impl
min_values_kernel_impl
nansum_kernel_impl
norm_kernel_tensor_iterator_impl
or_kernel_impl
prod_kernel_impl
register_dispatch
std_var_kernel_impl
DispatchKeySet
KernelRegistrationConfig
RegisterOperators
RegisterOperatorsOptions
alias_analysis
catch_all_kernel
check_no_duplicate_kernels
check_schema_and_register_op
const_assert
default
infer_function_schema_from_functor
infer_schema_from_kernels
kernel
op
options
register_op
schema
we
_pi32_const256
_pi32avx_const
_ps256_const
_ps256_const_type
cos256_ps
exp256_ps
log256_ps
sin256_ps
sincos256_ps
v4si
v8sf
v8si
lazy_static
batch_size
channels_size
height_size
width_size
define_dispatch
lazy_static
upsample_linear1d
upsample_linear1d_backward
AveragePoolingOperatorTester
batch_size
channels
iterations
next_batch_size
padding
padding_bottom
padding_height
padding_left
padding_right
padding_top
padding_width
pooling_height
pooling_size
pooling_width
qmax
qmin
stride
stride_height
stride_width
test_setupq8
testq8
Q8GemmDqContext
compute_q8gemm_dq
pytorch_qnnp_conv_dynamic_quantization_params
q8gemm_dq_context
qnnpack_linear_dynamic
CuDNNError
cublas_get_error_enum
cusolver_get_error_message
cusparse_get_error_string
SparseTensor
SparseType
alias_into_sparse
coo_to_csr
copy_into_sparse
flatten_indices
flatten_indices_by_dims
get_sparse_impl
is_same_density
is_same_tensor
Descriptor
Handle
Hasher
VulkanCache
purge
retrieve
MetaBase
TypeMeta
lazy_static
pytorch_qnnp_fp16_clamping_params
cudnn_version
get_cudnn_data_type
get_cudnn_data_type_from_scalar_type
lazy_static
pytorch_q8dwconv_ukernel_mp8x25_sse2
declare_dispatch
lazy_static
lazy_static
aminmax_all_kernel_impl
max_all_kernel_impl
min_all_kernel_impl
reduce_all_impl
reduce_all_impl_vec
register_dispatch
ParallelRegionGuard
calc_num_tasks_and_chunk_size
drop
get_intraop_pool
get_num_threads
get_thread_num
in_parallel_region
init_num_threads
intraop_launch
intraop_launch_future
num_pool_threads
parallel_for
parallel_reduce
parallel_run
run_with_pool
set_in_parallel_region
set_num_threads
set_thread_num
unset_thread_num
all_nonnegative
all_positive
slow_conv_dilated_shape_check
Blob
T
assign_from
default
drop
fmt
for
free
get
get_mutable
get_mutable_or_null
get_raw
get_raw_mut
is_type
meta
reset
reset_to_allocated
share_external
share_external_with_meta
swap
type_name
convert_double_to_int_of_same_size
convert_float_to_int_of_same_size
convert_i32
convert_i64
define_clamp_funcs
define_reinterpret_cast_to_all_funcs
deinterleave2_double
deinterleave2_float
fmadd_f64
fmadd_i16
fmadd_i32
fmadd_i64
interleave2_double
interleave2_float
apply_triu_tril
apply_triu_tril_single
trace_backward
tril
tril_cpu
tril_cpu_out
triu
triu_cpu
triu_cpu_out
lazy_static
BernoulliDistribution
CauchyDistribution
DiscreteDistributionType
ExponentialDistribution
GeometricDistribution
LognormalDistribution
NormalDistribution
T
UniformIntDistribution
UniformIntFromToDistribution
UniformIntFullRangeDistribution
UniformRealDistribution
distribution_helper_generate_has_member
distribution_helper_generate_next_normal_methods
has_member_
invoke
lazy_static
DistAccumType
bernoulli
cauchy
exponential
geometric
lazy_static
log_normal
normal
uniform_int
uniform_real
ComputeUnit
ComputeUnitFactory
ImageSize
ImageSizes
VBuffer
VBufferMapMemory
VContext
VImage
VulkanTensor
VulkanTensorImpl
WorkGroupSize
add_buffer_memory_barrier
add_image_memory_barrier
add_image_memory_barrier_to_general
add_image_memory_barrier_to_shader_read
add_memory_barrier
allocate_command_buffer
allocate_descriptor_set
allocate_storage
begin_command_buffer
bind
bind_shader_read
bind_storage_image
buffer
buffer_size_for_sizes
can_be_image
capacity_bytes
command_buffer
command_pool
compute_unit_factory
context
copy_buffer_to_buffer
copy_buffer_to_image
copy_data_to_host
copy_from_device_to_host
copy_from_host_to_device
copy_image_to_buffer
create_command_buffer
create_compute_pipeline
create_compute_pipeline_compile
create_descriptor_pool
create_descriptor_set_layout
create_descriptor_set_layout_single_pool
create_device
create_instance
d
debug_report_callback_fn
defined
descriptor_set_layout_binding
device
dim
dispatch_command_buffer
drop
end_command_buffer
find_memory_type
find_physical_device
flush_write_to_device
flush_write_to_host
fmt
get
get_cache_key
get_compute_queue_family_index
h
has_buffer
has_image
has_storage
image
image_sizes_w_h_nc4
impl_
init_vulkan_context_once
is_available
lazy_static
limits
macro_rules
make_descriptor_buffer_info
make_descriptor_image_info
make_image_view_create_info
make_sampler_create_info
make_uniform_buffer
make_uniform_const_buffer
make_write_descriptor_set
map
numel
physical_device
ptr
queue
set_data_from_host
set_zeros
size_bytes
sizes
strides
submit_and_wait_command_buffer
sync_image_to_buffer
vkbuffer
w
fill_kernel
fill_non_native_type
fill_non_native_type_complex_half
has
register_dispatch
pytorch_hgemm_ukernel_8x8_neonfp_16arith
pytorch_qnnp_x8zip_x2_sse2
cudnn_testcuda
ApplyGridSample
ComputeLocation
ComputeLocationBase
ComputeLocationBaseWithAlignedCorners
ComputeLocationBaseWithUnalignedCorners
ComputeLocationBorderPaddingWithAlignedCorners
ComputeLocationReflectionPaddingWithAlignedCorners
ComputeLocationZeroesPaddingWithAlignedCorners
InterpParams
aligned_corners_clip_coordinates
aligned_corners_clip_coordinates_get_grad
aligned_corners_reflect_coordinates
aligned_corners_reflect_coordinates_get_grad
aligned_corners_unnormalize
apply
apply_get_grad
bicubic2_add_value_bounded
bicubic2_backward
bicubic2_forward
bicubic2_get_cubic_coefficients
bicubic2_get_cubic_coefficients_grad
bicubic2_get_value_bounded
bilinear2_backward
bilinear2_compute_interp_params
bilinear2_forward
compute_coordinates
declare_dispatch
equality
grid_sample_2d_grid_slice_iterator
grid_sampler_2d_backward_cpu_kernel_impl
grid_sampler_2d_cpu_kernel_impl
lazy_static
mask_scatter_add
nearest2_backward
nearest2_forward
register_dispatch
that
unaligned_corners_clip_coordinates
unaligned_corners_clip_coordinates_get_grad
unaligned_corners_reflect_coordinates
unaligned_corners_reflect_coordinates_get_grad
unaligned_corners_unnormalize
Descriptor
DescriptorPool
DescriptorPoolSet
DescriptorPoolSetLayout
DescriptorSet
DescriptorSetBindings
DescriptorSetItem
allocate
allocate_descriptor_sets
assign_from
bind
create_descriptor_pool
defines
drop
handle
invalidate
purge
update
lazy_static
add_dense_sparse_worker_cpu
add_out_dense_sparse_cpu
add_out_sparse_contiguous
add_out_sparse_cpu
add_out_sparse_non_contiguous
add_sparse_a
add_sparse_b
addmm_out_sparse_dense_cpu
addmm_sparse_dense_cpu
any_sparse
asin_out_sparse
asin_sparse
binary_search_strided_rightmost
bmm_out_sparse_cpu
bmm_sparse_cpu
cannot
coalesce
conj_physical_out_sparse
div_out_sparse_scalar_a
div_out_sparse_scalar_b
div_out_sparse_zerodim_a
div_out_sparse_zerodim_b
div_sparse_a
div_sparse_b
div_sparse_c
div_sparse_d
floor_divide_out_sparse_scalar
floor_divide_out_sparse_zerodim
floor_divide_sparse_a
floor_divide_sparse_b
get_result_tensor_for_unary_op
hspmm_out_sparse_cpu
hspmm_sparse_cpu
isnan_sparse
log1p_out_sparse
log1p_sparse
log1p_sparse_mut
mul_out_sparse_cpu
mul_out_sparse_scalar
mul_out_sparse_zerodim
mul_sparse_a
mul_sparse_b
mv_sparse
neg_out_sparse
neg_sparse_a
neg_sparse_b
norm_sparse_a
norm_sparse_b
pow_out_sparse_scalar
pow_sparse_scalar
s_addmm_out_sparse_dense_cpu
s_addmm_out_sparse_dense_worker
s_addmm_sparse_dense_cpu_a
s_addmm_sparse_dense_cpu_b
smm
sparse_addmm
sparse_mm
sparse_mm_out
sparse_sum_a
sparse_sum_b
sparse_sum_backward_cpu
sparse_sum_c
sparse_sum_d
sqrt_out_sparse
sqrt_sparse
sspaddmm
sspaddmm_out_cpu
sspaddmm_out_only_sparse
sub_out_sparse
sub_sparse_a
sub_sparse_b
zero_sparse
check_grid_size
cudnn_grid_sampler_backward
cudnn_grid_sampler_forward
set_sampler_descriptor
clog_vlog_debug
clog_vlog_error
clog_vlog_fatal
clog_vlog_info
clog_vlog_warning
lazy_static
VdotOp
dot_fast_path
dot_impl_complex_f32
dot_impl_complex_f64
dot_impl_f32
dot_impl_f64
dot_impl_floating
dot_impl_scalar
dot_naive
gemv
gemv_fast_path
gemv_fast_path_double
gemv_fast_path_float
gemv_use_fast_path
gemv_use_fast_path_double
gemv_use_fast_path_float
instantiate
instantiate_dot_impl
instantiate_vdot_impl
invoke
lazy_static
scal
scal_fast_path
scal_fast_path_double
scal_fast_path_float
scal_use_fast_path
scal_use_fast_path_double
scal_use_fast_path_float
vdot_fast_path
vdot_impl
last_pow2
ActivationDescriptor
CTCLossDescriptor
ConvolutionDescriptor
Descriptor
DescriptorDeleter
DropoutDescriptor
FilterDescriptor
RNNDescriptor
SpatialTransformerDescriptor
TensorDescriptor
cudnn_memory_format_to_string
cudnn_type_to_string
data_size
desc
fix_size_one_dim_stride
fmt
get_data_type
init
initialize_rng
invoke
mut_desc
print
set
set_ex
set_no_dropout
the
ATenDLMTensor
deleter
from_dlp_ack
get_aten_device
get_dl_device
get_dld_ata_type
to_dlp_ack
to_scalar_type
LUTNormMicrokernelTester
inplace
iterations
n
test
DeprecatedTypeProperties
and
backend
copy_
cpu
cuda
device_type
eq
hip
id
is_cuda
is_sparse
is_sparse_csr
layout
of
operator_tensor_options
options
scalar_type
to_backend
to_scalar_type
to_string
type_meta
unsafe_storage_fromth
unsafe_tensor_fromth
device_of
device_of_maybe_tensor
device_of_tensor_list
set_flush_denormal
float
lazy_static
ParamsEqual
ParamsHash
invoke
ExclusivelyOwnedTest
ExclusivelyOwnedTypes
MyString
assert_is_sample_object
assert_is_sample_object_intrusive_ptr_my_string
assert_is_sample_object_my_string
assert_is_sample_object_tensor
exclusively_owned_test_default_constructor
exclusively_owned_test_move_assignment
exclusively_owned_test_move_assignment_from_contained_type
exclusively_owned_test_move_constructor
exclusively_owned_test_take
get_sample_value_a
get_sample_value_b
get_sample_value_c
inspect_exclusively_owned_intrusive_ptr
inspect_exclusively_owned_tensor
inspect_intrusive_ptr
inspect_tensor
inspect_unique_ptr
set_up
tear_down
typed_test_case
pytorch_q8gavgpool_ukernel_mp8x7p7q_sse2
AliasAnalysisKind
to_string
abs_kernel
acosh_kernel
and
angle_kernel
asinh_kernel
atanh_kernel
bernoulli_scalar_kernel
bernoulli_scalar_kernel_default
bernoulli_tensor_kernel
bitwise_not_kernel
cauchy_kernel
conj_kernel
cosh_kernel
digamma_kernel
does
entr_kernel
exp2_kernel
exponential_kernel
frac_kernel
frexp_kernel
geometric_kernel
i0e_kernel
i1_kernel
i1e_kernel
imag_kernel
implement_complex_kernel
implement_float_kernel
kaiser_window_kernel
log_normal_kernel
logical_not_kernel
logit_kernel
logit_mkl_kernel
nan_to_num_kernel
neg_kernel
normal_kernel
polygamma_kernel
random_from_to_kernel
random_full_64_bits_range_kernel
random_kernel
real_kernel
reciprocal_kernel
register_dispatch
rsqrt_kernel
sgn_kernel
sigmoid_kernel
sign_kernel
signbit_kernel
sinc_kernel
sinh_kernel
trigamma_kernel
uniform_kernel
vml_log
vml_log_double
vml_log_float
contiguous_a
contiguous_b
cudnn_is_acceptable
detach
is_same_size
is_set_to
size_a
size_b
stride_a
stride_b
Output
SizeType
ValueType
VectorizedFloat
abs
acos
add
angle
arange
asin
atan
atan2
bitand
bitor
bitxor
blend
blendv
ceil
clamp
clamp_max
clamp_min
conj
convert
copysign
cos
cosh
div
eq
erf
erfc
erfinv
exp
expm1
floor
fmadd
fmod
frac
ge
gt
hypot
i0
i0e
igamma
igammac
imag
isnan
le
lgamma
loadu
log
log10
log1p
log2
lt
map
maximum
minimum
mul
ne
neg
nextafter
operator_m256
partial_cmp
pow
real
reciprocal
round
rsqrt
set
sin
sinh
size
sqrt
store
sub
tan
tanh
trunc
zero_mask
pytorch_qnnp_create_fully_connected_sparse_dq_nc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_fully_connected_sparse_dq_nc_q8
pytorch_qnnp_status
pytorch_q8dwconv_ukernel_up8x9_neon
HIPAllocatorMasqueradingAsCUDA
allocate
raw_deleter
test_empty_tensor
test_expression_specification
test_scalar_vs_1dim_1size
test_simple_case
test_wrapdim
compute_sum
fxdiv_divisor_u32
pytorch_u8lut_32norm_ukernel_scalar
test_dlconvertor
test_dlconvertor_no_strides
x8lut_scalar_n_eq_1
x8lut_scalar_n_eq_1_inplace
x8lut_scalar_small_n
x8lut_scalar_small_n_inplace
define_dispatch
lazy_static
quantized_celu
quantized_elu
pytorch_q8dwconv_ukernel_mp8x25_per_channel_sse2
pytorch_qnnp_create_deconvolution2d_nhwc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_deconvolution2d_nhwc_q8
pytorch_qnnp_status
AxpyFn
CopyFn
GemmFn
TransposeType
axpy
axpy_complex_f32
axpy_complex_f64
axpy_f32
axpy_f64
copy_
copy_complex_f32
copy_complex_f64
copy_f32
copy_f64
declare_dispatch
define_dispatch
gemm_complex_f32
gemm_complex_f64
gemm_f32
gemm_f64
gemm_i64
gemm_scalar
lazy_static
normalize_last_dims
to_blas
to_fbgemm
use_blas_gemm
Command
CommandBuffer
CommandBufferBarrier
CommandBufferBarrierStage
CommandBufferBound
CommandPool
CommandPoolBuffer
CommandPoolStream
allocate
allocate_command_buffers
assign_from
barrier
begin
bind
copy_
create_command_pool
dispatch
drop
end
handle
invalidate
operator_bool
purge
reset
stream
submit
HIP_CHECK
MIOPEN_CHECK
MiOpenException
Vectorized
lazy_static
of
OpNameEquals
OpNameHash
invoke
is_custom_op
th_tensor_free
th_tensor_get_size_ptr
th_tensor_get_storage_ptr
th_tensor_get_stride_ptr
th_tensor_n_dimension
th_tensor_n_dimension_legacy_all
th_tensor_n_dimension_legacy_no_scalars
th_tensor_resize
th_tensor_resize_nd
th_tensor_set_storage
th_tensor_size_legacy_no_scalars
th_tensor_sizes_legacy_no_scalars
th_tensor_steal_and_set_storage_ptr
th_tensor_stride_legacy_no_scalars
th_tensor_strides_legacy_no_scalars
th_tensor_wrap
q8vadd_neon_a_scale
q8vadd_neon_a_zero_point
q8vadd_neon_inplace_a
q8vadd_neon_inplace_a_and_b
q8vadd_neon_inplace_b
q8vadd_neon_n_div_8
q8vadd_neon_n_eq_8
q8vadd_neon_n_gt_8
q8vadd_neon_n_lt_8
q8vadd_neon_qmax
q8vadd_neon_qmin
q8vadd_neon_scale
q8vadd_neon_y_scale
q8vadd_neon_y_zero_point
q8vadd_neon_zero_point
q8vadd_sse2_a_scale
q8vadd_sse2_a_zero_point
q8vadd_sse2_inplace_a
q8vadd_sse2_inplace_a_and_b
q8vadd_sse2_inplace_b
q8vadd_sse2_n_div_8
q8vadd_sse2_n_eq_8
q8vadd_sse2_n_gt_8
q8vadd_sse2_n_lt_8
q8vadd_sse2_qmax
q8vadd_sse2_qmin
q8vadd_sse2_scale
q8vadd_sse2_y_scale
q8vadd_sse2_y_zero_point
q8vadd_sse2_zero_point
get_num_threads
get_thread_num
in_parallel_region
init_num_threads
internal_set_num_threads
intraop_invoke
intraop_launch
intraop_launch_future
lazy_static
parallel_for
parallel_reduce
set_num_threads
BorrowType
ConstPointerType
ExclusivelyOwnedTraitsTensor
MaybeOwnedTraitsTensor
OwnedType
PointerType
ReprType
Stream
Tensor
&[Tensor]
UnsafeBorrow
accessor
and
assign_borrow
assign_from
backward
base
bool
borrow_from_optional_tensor
conj
contiguous
cpu
create_borrow
create_in_place
cuda
data
data_ptr
debug_borrow_is_valid
defined
destroy_borrow
destroy_owned
device
dim
dtype
element_size
enforce_invariants
expect_contiguous
fw_grad
generic_packed_accessor
get_device
get_impl
get_intrusive_ptr
get_named_tensor_meta
grad
grad_fn
has_names
has_storage
hip
index
index_put
is_alias_of
is_complex
is_conj
is_contiguous
is_cpu
is_cuda
is_floating_point
is_hip
is_inference
is_leaf
is_meta
is_metal
is_mkldnn
is_mlc
is_non_overlapping_and_dense
is_quantized
is_same
is_signed
is_sparse
is_sparse_csr
is_variable
is_view
is_vulkan
is_xla
is_xpu
item
itemsize
key_set
layout
lazy_static
legacy_extract_dispatch_key
m
make_tensor
metal
move_to_repr
mutable_grad
name
names
nbytes
ndimension
null_repr
numel
opt_names
options
packed_accessor
packed_accessor32
packed_accessor64
pointer_from_borrow
print
quantizer
reference_from_borrow
register_hook
remove_hook
requires_grad
reset
retain_grad
retains_grad
scalar_type
set_conj
set_data
set_fw_grad
set_requires_grad
size
sizes
std
storage
storage_offset
stride
strides
suggest_memory_format
take
tensor_data
to
to_backend
to_string
to_type
ty
unsafe_get_tensor_impl
unsafe_release_intrusive_ptr
unsafe_release_tensor_impl
use_count
var
variable_data
variable_excluded_from_dispatch
version
void
vulkan
weak_use_count
wrap_tensor_impl
BoxedKernelWrapper
FT
PopResult
has_ivalue_to
is
is_mutable_tensor_ref
is_tuple_of_mutable_tensor_refs
lazy_static
AliasInfo
Symbol
Unknown
add_after_set
add_before_set
add_contained_type
after_sets
before_set
before_sets
contained_types
e
eq
fmt
is_wildcard_after
is_wildcard_before
is_write
set_is_write
wildcard_set
RegistrationHandleRAII
assign_from
drop
check_names_valid_for
check_unique_names
default_names
get_named_tensor_meta
get_named_tensor_meta_mut
get_names
get_opt_names
has_names
internal_set_names_inplace
internal_set_names_inplace_with_dimname_list
internal_set_names_inplace_with_maybe_dimname_list
is_enabled
lazy_static
set_enabled
BernoulliKernel
CauchyKernel
ExponentialKernel
GeometricKernel
LogNormalKernel
NormalKernel
RandomFromToKernel
RandomKernel
UniformKernel
bernoulli_kernel
bernoulli_kernel_tensor
cauchy_kernel
exponential_kernel
geometric_kernel
invoke
log_normal_kernel
normal_fill
normal_fill_16
normal_fill_16_avx2
normal_fill_avx2
normal_kernel
random_from_to_kernel
random_full_64_bits_range_kernel
random_kernel
uniform_kernel
batch_count_tril_triu
check_tril_triu_batch_contiguous
Access
Barrier
Block
Buffer
Component
Fence
Flags
Image
ImageLayout
Memory
Payload
Stage
Transition
VulkanTensor
VulkanTensorFuture
VulkanTensorImpl
VulkanTensorView
VulkanTensorViewCmd
VulkanTensorViewState
VulkanTensorViewStateBundle
VulkanTensorViewStateBundleBuffer
VulkanTensorViewStateBundleImage
VulkanTensorViewStateComponent
VulkanTensorViewStateComponentType
access
allocate_buffer
allocate_fence
allocate_image
allocate_staging
assign_from
barrier
bitflags
buffer
buffer_bytes
categorize_a
categorize_b
convert
convert_vulkan_tensor
copy_buffer_to_image
copy_buffer_to_staging
copy_image_to_buffer
copy_staging_to_buffer
default
drop
extents
fence
fmt
has_image
host
image
image_extents
is_available
is_clean
is_dirty
is_discrete
is_uma
nbytes
of
operator_bool
options
requires_image
requires_staging
set_clean
set_dirty
sizes
staging
strides
submit
to
transition
verify
vk_access
vk_extent
vk_format
vk_layout
vk_stage
wait
ActivationND
ContextConv2D
ContextLinear
Deleter
Operator
available
batch
channel
invoke
APIVitals
Output
TorchVital
TorchVitalAttr
create
default
drop
lazy_static
set_vital
shl
torch_vital_enabled
QChannelShuffle
channel_shuffle_quantized_cpu
invoke
quantized_channel_shuffle_impl
define_dispatch
lazy_static
upsample_bicubic2d
upsample_bicubic2d_backward
upsample_bicubic2d_backward_kernel
upsample_bicubic2d_backward_out_frame
channel_shuffle
use_channel_shuffle
TensorGeometry
default
dim
is_contiguous
numel
size
sizes
storage_offset
stride
strides
transpose
pytorch_qnnp_create_hardswish_nc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_hardswish_nc_q8
pytorch_qnnp_status
expect_calls_concat_unboxed
expect_calls_increment
lazy_static
operator_registration_test_legacy_lambda_based_kernel_given_mismatched_with_different_num_returns_when_registering_then_fails
operator_registration_test_legacy_lambda_based_kernel_given_mismatched_with_different_return_types_when_registering_then_fails
operator_registration_test_legacy_lambda_based_kernel_given_multiple_operators_and_kernels_when_registered_in_one_registrar_then_calls_right
operator_registration_test_legacy_lambda_based_kernel_given_multiple_operators_and_kernels_when_registered_in_registrars_then_calls_right
operator_registration_test_legacy_lambda_based_kernel_given_when_registered_in_constructor_then_can_be_called
operator_registration_test_legacy_lambda_based_kernel_given_when_registered_then_can_be_called
operator_registration_test_legacy_lambda_based_kernel_given_when_registered_then_can_be_called_unboxed
operator_registration_test_legacy_lambda_based_kernel_given_when_registered_without_specifying_schema_then_infers
operator_registration_test_legacy_lambda_based_kernel_given_when_runs_out_of_scope_then_cannot_be_called_anymore
operator_registration_test_legacy_lambda_based_kernel_given_with_list_of_map_int_when_registered_then_can_be_called
operator_registration_test_legacy_lambda_based_kernel_given_with_list_of_map_when_registered_then_can_be_called
operator_registration_test_legacy_lambda_based_kernel_given_with_map_of_list_when_registered_then_can_be_called
mkldnn_convolution
mkldnn_convolution_backward
mkldnn_convolution_backward_weights
AveragePoolingContext
ChannelShuffleContext
ClampContiguousContext
ClampStridedContext
GlobalAveragePoolingContext
LutContiguousContext
LutStridedContext
MaxPoolingContext
Q8AddContiguousContext
Q8AddStridedContext
Q8ConvContext
Q8DwConvContext
Q8GemmContext
Q8GemmPrepackASparseDqContext
Q8GemmSparseDqContext
Q8GemmXzpContext
Q8SumRowsContext
U8SoftArgmaxContext
average_pooling_context
channel_shuffle_context
clamp_contiguous_context
clamp_strided_context
compute_average_pooling_multipass
compute_average_pooling_unipass
compute_channel_shuffle_fixed
compute_channel_shuffle_variable
compute_clamp_contiguous
compute_clamp_strided
compute_dwconv_multiipass
compute_dwconv_unipass
compute_global_average_pooling_multipass
compute_global_average_pooling_unipass
compute_lut_contiguous
compute_lut_strided
compute_max_pooling
compute_q8add_contiguous
compute_q8add_strided
compute_q8conv
compute_q8gemm
compute_q8gemm_prepack_a_sparse
compute_q8gemm_prepacked_sparse_dq
compute_q8gemm_sparse_dq
compute_q8gemm_xzp
compute_sum_rows
global_average_pooling_context
lut_contiguous_context
lut_strided_context
max_pooling_context
pytorch_qnnp_run_operator
q8add_contiguous_context
q8add_strided_context
q8conv_context
q8dwconv_context
q8gemm_context
q8gemm_prepackA_sparse_dq_context
q8gemm_sparse_dq_context
q8gemm_xzp_context
q8sum_rows_context
declare_pytorch_q8vadd_ukernel_function
THCIndex
THCIndexTensor
cudnn_rnn_cast_reflatten
QLinearInt8
apply
apply_impl
apply_relu
lazy_static
register_linear_params
run
test_undefined
lazy_static
Algo
AlgoIterator
AlgorithmSearch
AlgorithmSearchCuDnnConvolutionBwdDataAlgoPerf
AlgorithmSearchCuDnnConvolutionBwdFilterAlgoPerf
AlgorithmSearchCudnnConvolutionFwdAlgoPerf_t
BenchmarkCache
ConvolutionArgs
Perf
Search
Workspace
allocate_workspace
cache
drop
find
find_algorithms
fmt
for
get_max_workspace_size
get_valid_algorithms
get_workspace_size
insert
lazy_static
only_default_algorithm
operator_tib_raw_string_suffix
raw_cudnn_convolution_add_relu_out
raw_cudnn_convolution_backward_weight_out
raw_cudnn_convolution_backward_weight_out_32bit
raw_cudnn_convolution_forward_out
raw_cudnn_convolution_forward_out_32bit
split_batch_dim_to_32bit_out
try_all
broadcastOp
test_thread_pool_guard
test_thread_pool_guard_run_with
Mode
ObservedOperators
get_unobserved_operator_list
is_observed
thc_storage_get_device
thc_storage_resize_bytes
RecursiveMethodCallError
Stack
TaskLauncher
clear_execution_info
doc_string
ensure_defined
get_executor
get_schema
graph
invoke
is_graph_function
name
optimized_graph
preoptimize_graph
pretty_print_schema
qualname
run
run_async
run_mut
set_schema
VariableHooksRegisterer
backward
base
data
get_variable_hooks
grad_fn
is_leaf
is_view
lazy_static
name
register_hook
remove_hook
requires_grad
retain_grad
retains_grad
set_data
set_variable_hooks
tensor_data
variable_data
version
align
align_as
align_tensors
align_tensors_to
align_to_a
align_to_b
aligned_size
count_unset
gather
gather_out
index_add_a
index_add_b
index_copy_a
index_copy_b
index_fill_a
index_fill_b
index_fill_c
index_fill_d
index_select
index_select_out
refine_names
rename
rename_mut
report_moving_unnamed_dim_error
report_not_a_subsequence_error
scatter_a
scatter_add_a
scatter_add_b
scatter_b
scatter_c
scatter_d
sort_a
sort_b
sort_out_a
sort_out_b
squeeze_a
squeeze_b
dot
dot_check
dot_impl
gemv
lazy_static
lda_cond
mv
mv_out
vdot
vdot_impl
hgemm_8x8_aarch32_neonfp16arith_k_div_4
hgemm_8x8_aarch32_neonfp16arith_k_div_4_strided_a
hgemm_8x8_aarch32_neonfp16arith_k_div_4_strided_c
hgemm_8x8_aarch32_neonfp16arith_k_div_4_subtile
hgemm_8x8_aarch32_neonfp16arith_k_eq_4
hgemm_8x8_aarch32_neonfp16arith_k_eq_4_qmax128
hgemm_8x8_aarch32_neonfp16arith_k_eq_4_qmin128
hgemm_8x8_aarch32_neonfp16arith_k_eq_4_strided_a
hgemm_8x8_aarch32_neonfp16arith_k_eq_4_strided_c
hgemm_8x8_aarch32_neonfp16arith_k_gt_4
hgemm_8x8_aarch32_neonfp16arith_k_gt_4_strided_a
hgemm_8x8_aarch32_neonfp16arith_k_gt_4_strided_c
hgemm_8x8_aarch32_neonfp16arith_k_gt_4_subtile
CuSparsePoolType
create_cusparse_handle
destroy_cusparse_handle
get_current_cuda_sparse_handle
pytorch_qnnp_requantize_gemmlowp_ssse3
adaptive_max_pool3d_backward_out_frame
adaptive_max_pool3d_backward_single_out_frame
adaptive_max_pool3d_out_frame
adaptive_max_pool3d_single_out_frame
end_index
lazy_static
start_index
FullyConnectedOperatorTester
Mode
batch_size
iterations
per_channel
qmax
qmin
testq8
available
deinitialize
initialize
lazy_static
check_batch_dims_equal
check_broadcasting_vmap_transform
check_multi_batch_vmap_transform
max_batch_dims_at_front
vmap_test_batched_tensor
vmap_test_batched_tensor_actual_dim
vmap_test_batched_tensor_expand
vmap_test_batched_tensor_max_level
vmap_test_batched_tensor_mul
vmap_test_batched_tensor_permute
vmap_test_batched_tensor_size
vmap_test_batched_tensor_squeeze
vmap_test_batched_tensor_sum
vmap_test_batched_tensor_transpose
vmap_test_batched_tensor_unsqueeze
vmap_test_broadcasting_transform_batched
vmap_test_broadcasting_transform_batched_unbatched
vmap_test_broadcasting_transform_max_levels
vmap_test_multi_batch_transform
vmap_test_multi_batch_transform_batched
vmap_test_multi_batch_transform_batched_unbatched
vmap_test_multi_batch_transform_max_levels
vmap_test_multi_batch_transform_multiple_tensors
vmap_test_physical_view_get_dim
vmap_test_physical_view_get_dims
vmap_test_physical_view_get_shape
BuiltinOpFunction
clear_execution_info
doc_string
ensure_defined
get_executor
get_schema
graph
invoke
is_graph_function
name
optimized_graph
pretty_print_schema
qualname
run
run_async
set_schema
SegmentReduceBackwardFn
SegmentReduceFn
SegmentReductionType
declare_dispatch
define_dispatch
get_reduction_enum
register_arch_dispatch
register_avx2_dispatch
register_avx_dispatch
register_vsx_dispatch
segment_reduce_backward_kernel
segment_reduce_cpu_backward_kernel
segment_reduce_cpu_kernel
segment_reduce_kernel
AdvancedIndex
can_use_32bit_index_math
check_index_tensor_types
expand_tensors
has_contiguous_subspace
invalid_mask
to_list_of_optional_tensors_a
to_list_of_optional_tensors_b
transpose_to_front
transpose_to_front_and_inv_perm
Output
append
assign_from
begin
clear
copy_
element_type
emplace
emplace_back
empty
end
eq
erase
extract
get
get_type_ptr
index
index_mut
insert
is
list_element_to
list_element_to_const_ref
list_element_to_const_ref_optional_string
operator_t
pop_back
push_back
reserve
resize
set
size
swap
to_list
to_string
to_typed_list
unsafe_set_element_type
use_count
vec
test_stream_copy_and_move
test_stream_cross_device
test_stream_cuda_event_sync
test_stream_cuda_guard
test_stream_generic_inline_cuda_event
test_stream_generic_virtual_cuda_event
test_stream_get_and_set
test_stream_multi_gpu
test_stream_multithread_get_and_set
test_stream_pool
thread_fun
declare_pytorch_q8gemm_dynamic_quantization_sparse_packeda_ukernel_function
declare_pytorch_q8gemm_dynamic_quantization_sparse_ukernel_function
declare_pytorch_q8gemm_parse_packa_ukernel_function
declare_pytorch_q8gemm_sparse_ukernel_function
pytorch_qnnp_conv_dynamic_quantization_params
QLinearUnpackWeightFp16
QLinearUnpackWeightFp16Legacy
QLinearUnpackWeightInt8
QLinearUnpackWeightInt8Legacy
lazy_static
register_linear_params
run
unpack
AbsMaxOps
AbsMinOps
AbsSwitch
Acc
ArgMaxOps
ArgMinOps
ArgReductionOps
GreaterOrNan
LessOrNan
MaxOps
MeanOps
MinMaxOps
MinMaxReductionOps
MinOps
NanSumOps
NormOneOps
NormOps
NormTwoOps
NormZeroOps
WelfordData
WelfordOps
abs_if_complex
combine
default
invoke
lazy_static
max_propagate_nan
min_propagate_nan
of
project
reduce
translate_idx
warp_shfl_down
pytorch_q8dwconv_ukernel_up8x9_per_channel_neon
avg_pool3d_backward_cpu
avg_pool3d_backward_out_cpu
avg_pool3d_backward_out_cpu_template
avg_pool3d_backward_out_frame
avg_pool3d_out_frame
lazy_static
BufferHandler
Cache
Descriptor
Flags
Handle
ImageHandler
Linear
LinearBlock
LinearEntry
LinearEntryDeleter
Resource
ResourceBuffer
ResourceBufferBarrier
ResourceBufferDescriptor
ResourceBufferDescriptorUsage
ResourceBufferObject
ResourceFence
ResourceImage
ResourceImageBarrier
ResourceImageBarrierLayour
ResourceImageDescriptor
ResourceImageDescriptorUsage
ResourceImageDescriptorView
ResourceImageObject
ResourceImageSampler
ResourceImageSamplerDescriptor
ResourceImageSamplerFactory
ResourceImageSamplerFactoryHasher
ResourceMemory
ResourceMemoryAccess
ResourceMemoryAccessType
ResourceMemoryBarrier
ResourceMemoryDescriptor
ResourceMemoryScope
ResourcePool
ResourcePoolBuffer
ResourcePoolFence
ResourcePoolImage
ResourcePoolMemory
ResourcePoolPolicy
assign_from
bitflags
buffer
create_allocation_create_info
create_allocator
drop
enact
eq
fence
handle
image
invalidate
invoke
linear
map
operator_bool
purge
release_buffer
release_image
this
uniform
wait
clone_sparse
coalesce
coalesce_sparse_cpu
coalesced_sparse
copy_sparse
dense_dim_sparse
dense_to_sparse_a
dense_to_sparse_b
empty_sparse
expand_values_if_needed
indices_sparse
is_coalesced_sparse
is_same_size_as_sparse
nnz_sparse
resize_as_sparse
sparse_coo_tensor_a
sparse_coo_tensor_b
sparse_coo_tensor_c
sparse_coo_tensor_unsafe
sparse_dim_sparse
sparse_mask_cpu
sparse_mask_helper_cpu
sparse_mask_out_cpu
sparse_mask_out_cpu_kernel
sparse_resize
sparse_resize_and_clear
sparse_to_dense
values_sparse
Conv2dOpContext
LinearOpContext
SerializationTypeConv2dPrePack
SerializationTypeLinearPrePack
get_bias
get_opaque_op_ptr
get_release_callback
get_weight
pack
release_resources
set_opaque_op_ptr
set_release_callback
x8zip_x2_neon_n_div_16
x8zip_x2_neon_n_eq_8
x8zip_x2_neon_n_gt_8
x8zip_x2_neon_n_lt_8
x8zip_x2_sse2_n_div_16
x8zip_x2_sse2_n_eq_16
x8zip_x2_sse2_n_gt_16
x8zip_x2_sse2_n_lt_16
x8zip_x3_neon_n_div_8
x8zip_x3_neon_n_eq_8
x8zip_x3_neon_n_gt_8
x8zip_x3_neon_n_lt_8
x8zip_x3_sse2_n_div_16
x8zip_x3_sse2_n_eq_16
x8zip_x3_sse2_n_gt_16
x8zip_x3_sse2_n_lt_16
x8zip_x4_neon_n_div_8
x8zip_x4_neon_n_eq_8
x8zip_x4_neon_n_gt_8
x8zip_x4_neon_n_lt_16
x8zip_x4_sse2_n_div_16
x8zip_x4_sse2_n_eq_16
x8zip_x4_sse2_n_gt_16
x8zip_x4_sse2_n_lt_16
x8zip_xm_neon_n_div_8_m_4
x8zip_xm_neon_n_div_8_m_eq_4
x8zip_xm_neon_n_div_8_m_gt_4
x8zip_xm_neon_n_eq_8_m_4
x8zip_xm_neon_n_eq_8_m_div_4
x8zip_xm_neon_n_eq_8_m_gt_4
x8zip_xm_neon_n_gt_8_m_4
x8zip_xm_neon_n_gt_8_m_div_4
x8zip_xm_neon_n_gt_8_m_eq_4
x8zip_xm_neon_n_lt_8
x8zip_xm_sse2_n_div_16_m_4
x8zip_xm_sse2_n_div_16_m_eq_4
x8zip_xm_sse2_n_div_16_m_gt_4
x8zip_xm_sse2_n_eq_16_m_4
x8zip_xm_sse2_n_eq_16_m_div_4
x8zip_xm_sse2_n_eq_16_m_gt_4
x8zip_xm_sse2_n_eq_8_m_4
x8zip_xm_sse2_n_eq_8_m_div_4
x8zip_xm_sse2_n_eq_8_m_gt_4
x8zip_xm_sse2_n_gt_16_m_4
x8zip_xm_sse2_n_gt_16_m_div_4
x8zip_xm_sse2_n_gt_16_m_eq_4
x8zip_xm_sse2_n_lt_16
MaybeOwnedTest
MaybeOwnedTypes
MyString
assert_borrow_a
assert_borrow_b
assert_own_a
assert_own_b
equal_a
equal_b
get_sample_value2_a
get_sample_value2_b
get_sample_value_a
get_sample_value_b
maybe_owned_test_copy_assignment_into
maybe_owned_test_copy_assignment_into_borrowed
maybe_owned_test_copy_constructor
maybe_owned_test_default_ctor
maybe_owned_test_move_assignment_into
maybe_owned_test_move_assignment_into_borrowed
maybe_owned_test_move_constructor
maybe_owned_test_move_dereferencing
maybe_owned_test_self_assignment
maybe_owned_test_simple_dereferencing_string
set_up
tear_down
typed_test_case
register_dispatch
unfold_backward_cpu_kernel
unfold_backward_internal_kernel
col_indices_sparse_csr
crow_indices_sparse_csr
is_same_size_as_sparse_csr
nnz_sparse_csr
of
resize_as_sparse_csr
sparse_csr_tensor_a
sparse_csr_tensor_b
sparse_csr_tensor_unsafe
values_sparse_csr
pytorch_u8rmax_ukernel_neon
caffe_known_type
calc_col_offsets_transpose
check_and_saturate
fbgemm_is_cpu_supported
fbgemm_linear_fp16_weight
fbgemm_linear_fp16_weight_fp32_activation
fbgemm_linear_int8_weight
fbgemm_linear_int8_weight_fp32_activation
fbgemm_linear_quantize_weight
fbgemm_pack_gemm_matrix_fp16
fbgemm_pack_quantized_matrix
handle_weights_saturation
raw_uint_16to_fp16
cpu_atomic_add_float
default_error_handler_function
lazy_static
th_alloc
th_assertion_failed
th_error
th_free
th_realloc
th_set_default_error_handler
th_set_error_handler
th_set_gch_andler
th_size_desc
pytorch_q8gemm_dq_ukernel_4x8_neon
available
conv2d_clamp_run
conv2d_transpose_clamp_run
convolution2d
create
create_and_run
create_conv2d_clamp_pre_pack_op_context
create_conv2d_transpose_clamp_pre_pack_op_context
reorder_weights_for_transpose_conv
run
usable
use_convolution2d
Q8GemmContext
compute_q8gemm
q8gemm_context
qnnpack_linear
thc_sleep
MatrixRef
MatrixRefSizeType
Output
data
default
empty
equals
index
numel
size
avg_pool2d_out_frame
avg_pool2d_quantized_cpu
define_dispatch
get_kernel
get_padding
get_stride
q_avg_pool2d
qnnpack_avg_pool2d
AlignedAllocator
ConstPointer
ConstReference
DifferenceType
Other
Pointer
Refernce
SizeType
ValueType
address
allocate
construct
deallocate
destroy
lazy_static
max_size
propagate_on_container_move_assignment
rebind
CUDAGeneratorImpl
capture_epilogue
capture_prologue
clone
clone_impl
create_cuda_generator
current_seed
device_type
get_default_cuda_generator
get_state
philox_cuda_state
philox_offset_per_thread
seed
set_current_seed
set_philox_offset_per_thread
set_state
define_dispatch
lazy_static
qnnpack_hardswish
quantized_hardswish
lazy_static
pytorch_q8avgpool_ukernel_up8xm_neon
CPUGeneratorImpl
CPUGeneratorImplState
CPUGeneratorImplStateLegacy
checking
clone
clone_impl
create_cpu_generator
current_seed
device_type
engine
get_default_cpu_generator
get_state
make_64bits_from_32bits
next_double_normal_sample
next_float_normal_sample
random
random64
seed
set_current_seed
set_engine
set_next_double_normal_sample
set_next_float_normal_sample
set_state
Dict
DictElementTypes
DictEntryRef
DictImpl
DictIterator
DictKeyEqualTo
DictKeyHash
DictMapType
GenericDict
Iterator
KeyType
MappedType
Output
SizeType
TypePtr
ValidDictKeyTypes
assign_from
at
begin
clear
cmp
contains
copy_
deref
empty
end
eq
erase
find
for
get_iterator
insert
insert_or_assign
invoke
is
key
key_type
lazy_static
of
out_of_range
partial_cmp
prefix_increment
reserve
set_value
size
sub
the
unsafe_set_key_type
unsafe_set_value_type
value
value_type
will
boxed_func_for_inplace_op
boxed_func_for_outofplace_multi_op
boxed_func_for_outofplace_op
boxed_func_with_multi_return
boxed_func_with_return
boxed_func_without_return
expect_boxed_calling_fails_with
expect_boxed_calling_with_multi_return_works
expect_boxed_calling_with_return_works
expect_boxed_calling_without_return_works
expect_in_place_boxed_calling_works
expect_in_place_unboxed_calling_works
expect_out_of_place_boxed_calling_works
expect_out_of_place_multi_boxed_calling_works
expect_out_of_place_multi_unboxed_calling_works
expect_out_of_place_unboxed_calling_works
expect_unboxed_calling_with_multi_return_works
expect_unboxed_calling_with_return_works
expect_unboxed_calling_without_return_works
kernel_function_test_given_boxed_with_in_place_signature_when_calling_then_works
kernel_function_test_given_boxed_with_in_place_signature_when_calling_unboxed_then_works
kernel_function_test_given_boxed_with_multi_return_when_calling_then_works
kernel_function_test_given_boxed_with_multi_return_when_calling_unboxed_then_works
kernel_function_test_given_boxed_with_out_of_place_multi_signature_when_calling_then_works
kernel_function_test_given_boxed_with_out_of_place_multi_signature_when_calling_unboxed_then_works
kernel_function_test_given_boxed_with_out_of_place_signature_when_calling_then_works
kernel_function_test_given_boxed_with_out_of_place_signature_when_calling_unboxed_then_works
kernel_function_test_given_boxed_with_return_when_calling_then_works
kernel_function_test_given_boxed_with_return_when_calling_unboxed_then_works
kernel_function_test_given_boxed_without_return_when_calling_then_works
kernel_function_test_given_boxed_without_return_when_calling_unboxed_then_works
kernel_function_test_given_unboxed_functor_with_return_when_calling_boxed_then_works
kernel_function_test_given_unboxed_functor_with_return_when_calling_then_works
kernel_function_test_given_unboxed_functor_without_return_when_calling_boxed_then_works
kernel_function_test_given_unboxed_functor_without_return_when_calling_then_works
kernel_function_test_given_unboxed_lambda_with_return_when_calling_boxed_then_works
kernel_function_test_given_unboxed_lambda_with_return_when_calling_then_works
kernel_function_test_given_unboxed_lambda_without_return_when_calling_boxed_then_works
kernel_function_test_given_unboxed_lambda_without_return_when_calling_then_works
kernel_function_test_given_unboxed_runtime_with_return_when_calling_boxed_then_works
kernel_function_test_given_unboxed_runtime_with_return_when_calling_then_works
kernel_function_test_given_unboxed_runtime_without_return_when_calling_boxed_then_works
kernel_function_test_given_unboxed_runtime_without_return_when_calling_then_works
kernel_function_test_given_unboxed_with_return_when_calling_boxed_then_works
kernel_function_test_given_unboxed_with_return_when_calling_then_works
kernel_function_test_given_unboxed_without_return_when_calling_boxed_then_works
kernel_function_test_given_unboxed_without_return_when_calling_then_works
make_dummy_operator_handle
unboxed_function_with_return
unboxed_function_without_return
unboxed_functor_with_return
unboxed_functor_with_return_factory
unboxed_functor_without_return
unboxed_functor_without_return_factory
MetalDeviceInfo
create_device_info
DimMask
FastSetupType
Loop2d
LoopSubiter
OperandInfo
PtrVector
SplitUntil32Bit
SplitUntil32BitIterator
StrideVector
TensorIterator
TensorIteratorBase
TensorIteratorConfig
allow_cpu_scalars
apply_perm_and_mul
begin
binary_float_op
binary_op
borrowing_binary_op
borrowing_nullary_op
build
build_binary_float_op
build_binary_op
build_borrowing_binary_float_op
build_borrowing_binary_op
build_unary_float_op
build_unary_op
can_use_32bit_indexing
check_all_same_device
check_all_same_dtype
coalesce_dimensions
common_dtype
comparison_op
compatible_stride
compute_common_dtype
compute_fast_setup_type
compute_mem_overlaps
compute_names
compute_shape
compute_strides
compute_types
conversion
conversions
data_ptr
declare_static_dtype_and_device
declare_static_shape
default
device
device_type
dtype
element_size
end
enforce_linear_iteration
eq
fast_set_up
for
for_each
foreach_reduced_elt
get_base_ptrs
get_dim_strides
get_dim_to_split
get_inner_strides
get_strides
has_contiguous_first_dim
in
increment
information
invert_perm
is
is_contiguous
is_cpu_scalar
is_dim_reduced
is_done
is_reduction
is_scalar
is_trivial_1d
is_type_defined
loop_2d_from_1d
max_2d_step
narrow
ndim
ntensors
nullary_op
num_reduce_dims
numel
operator_star
options
original_options
parallel_reduce
permute_dimensions
populate_operands
prefix_increment
promotion
reduce_op
remove_operand
reorder_dimensions
scalar_value
select_all_keeping_dim
serial_for_each
set_check_mem_overlap
shape
should_accumulate
split
strides
tensor
that
to
unary_float_op
unary_op
undefined
unsafe_replace_operand
validate
view_offsets
with_32bit_indexing
assign_from
at
begin
clear
contains
copy_
empty
end
eq
erase
find
for
get_type_ptr
insert
insert_or_assign
invoke
is
key_type
reserve
size
to_generic_dict
to_string
to_typed_dict
unsafe_set_key_type
unsafe_set_value_type
value_type
mkldnn_softmax
global_average_pooling_op_small_batch_few_channels
global_average_pooling_op_small_batch_many_channels_width
global_average_pooling_op_unit_batch_few_channels
global_average_pooling_op_unit_batch_many_channels_small_width
global_average_pooling_op_zero_batch
cpu
cuda
hip
lazy_static
metal
options
to_backend
to_type
vulkan
define_dispatch
lazy_static
Q8GEMM
Q8GEMMSparse
Q8GEMMSparse_Op
Q8GEMM_Op
a
b
c
col_block_size
divide_round_up
fill_block_sparse_weights
k
kc
kc_stride
kr
lazy_static
mc
mr
nc
nc_stride
np
nr
quantization_params
round_up
row_block_size
set_up
sparsity
tear_down
w
convolution_op_grouped_xzp_1x1
convolution_op_grouped_xzp_1x1_per_channel
convolution_op_grouped_xzp_1x1_runtime_quant
convolution_op_grouped_xzp_1x1_runtime_quant_per_channel
convolution_op_xzp_1x1
convolution_op_xzp_1x1_per_channel
convolution_op_xzp_1x1_with_batch
convolution_op_xzp_1x1_with_batch_per_channel
convolution_op_xzp_1x1_with_qmax
convolution_op_xzp_1x1_with_qmax_per_channel
convolution_op_xzp_1x1_with_qmin
convolution_op_xzp_1x1_with_qmin_per_channel
lazy_static
OperatorName
OperatorNameView
eq
fmt
get_namespace
hash
lazy_static
parse
set_namespace_if_not_set
to_string
PackedLinearWeight
apply
apply_dynamic
apply_dynamic_relu
apply_impl
apply_relu
bias
lazy_static
prepack
register_linear_params
unpack
TensorQuantizationParams
check_and_saturate
choose_quantization_params
handle_weights_saturation
raw_uint_16to_fp16
infer_size
infer_size_dv
infer_size_impl
pytorch_qnnp_requantize_precise_scalar_signed64
pytorch_qnnp_requantize_precise_scalar_unsigned32
pytorch_qnnp_requantize_precise_scalar_unsigned64
pytorch_sgemm_ukernel_6x8_psimd
conv2d_prepack
lazy_static
linear_prepack
unpack_a
unpack_b
pytorch_qnnp_requantize_precise_sse2
define_dispatch
quantized_topk_out_cpu
topk_quantized_cpu
pytorch_q8avgpool_ukernel_mp8x9p8q_sse2
col2im
im2col
pytorch_sconv_ukernel_6x8_psimd
test_dlconvertor_dlconvertorcuda
test_dlconvertor_dlconvertorcudahip
test_dlconvertor_no_stridescuda
list_test_can_access_optional_string_by_reference
list_test_can_access_string_by_reference
list_test_can_access_tensor_by_reference
list_test_i_value_based_copy_has_separate_storage
list_test_i_value_based_given_different_iterators_then_are_not_equal
list_test_i_value_based_given_different_lists_then_is_not_equal
list_test_i_value_based_given_empty_when_calling_resize_then_resizes_and_sets
list_test_i_value_based_given_empty_when_calling_resize_with_then_resizes_and_sets
list_test_i_value_based_given_empty_when_calling_size_then_returns_zero
list_test_i_value_based_given_empty_when_calling_then_returns_true
list_test_i_value_based_given_empty_when_iterating_then_begin_is_end
list_test_i_value_based_given_equal_iterators_then_are
list_test_i_value_based_given_equal_lists_then_is
list_test_i_value_based_given_iterator_when_calculating_difference_then_returns_correct_number
list_test_i_value_based_given_iterator_when_dereferencing_then_points_to_correct_element
list_test_i_value_based_given_iterator_when_postfix_decrementing_then_moves_to_next_and_returns_old_position
list_test_i_value_based_given_iterator_when_postfix_incrementing_then_moves_to_next_and_returns_old_position
list_test_i_value_based_given_iterator_when_swapping_values_from_then_changes
list_test_i_value_based_given_nonempty_when_calling_clear_then_is_empty
list_test_i_value_based_given_nonempty_when_calling_empty_then_returns_false
list_test_i_value_based_given_nonempty_when_calling_size_then_returns_number_of_elements
list_test_i_value_based_given_one_element_when_calling_pop_back_then_is_empty
list_test_i_value_based_given_one_element_when_erasing_then_is_empty
list_test_i_value_based_given_when_erasing_full_range_then_is_empty
list_test_i_value_based_given_when_erasing_then_returns_iterator
list_test_i_value_based_is_reference_type
list_test_i_value_based_when_assigning_to_access_operator_from_then_sets_element
list_test_i_value_based_when_assigning_to_access_operator_with_existing_position_then_sets_element
list_test_i_value_based_when_calling_access_operator_with_existing_position_then_returns_element
list_test_i_value_based_when_calling_access_operator_with_non_existing_position_then_throws_exception
list_test_i_value_based_when_calling_copying_set_with_existing_position_then_changes_element
list_test_i_value_based_when_calling_copying_set_with_non_existing_position_then_throws_exception
list_test_i_value_based_when_calling_emplace_back_with_lvalue_then_inserts_element
list_test_i_value_based_when_calling_emplace_back_with_rvalue_then_inserts_element
list_test_i_value_based_when_calling_emplace_with_lvalue_then_inserts_element
list_test_i_value_based_when_calling_emplace_with_rvalue_then_inserts_element
list_test_i_value_based_when_calling_extract_with_existing_position_then_element_becomes_invalid
list_test_i_value_based_when_calling_extract_with_existing_position_then_returns_element
list_test_i_value_based_when_calling_extract_with_non_existing_position_then_throws_exception
list_test_i_value_based_when_calling_get_with_existing_position_then_returns_element
list_test_i_value_based_when_calling_get_with_non_existing_position_then_throws_exception
list_test_i_value_based_when_calling_insert_on_iterator_with_lvalue_then_inserts_element
list_test_i_value_based_when_calling_insert_on_iterator_with_rvalue_then_inserts_element
list_test_i_value_based_when_calling_moving_set_with_existing_position_then_changes_element
list_test_i_value_based_when_calling_moving_set_with_non_existing_position_then_throws_exception
list_test_i_value_based_when_calling_push_back_with_lvalue_then_inserts_element
list_test_i_value_based_when_calling_push_back_with_rvalue_then_inserts_element
list_test_i_value_based_when_calling_reserve_then_doesnt_crash
list_test_i_value_based_when_copy_assigning_then_are_equal
list_test_i_value_based_when_copy_constructing_then_are_equal
list_test_i_value_based_when_copying_then_are_equal
list_test_i_value_based_when_iterating_then_finds_elements
list_test_i_value_based_when_iterating_with_foreach_then_finds_elements
list_test_i_value_based_when_move_assigning_then_old_is_empty
list_test_i_value_based_when_move_constructing_then_old_is_empty
list_test_i_value_based_when_swapping_from_access_operator_then_swaps_elements
list_test_non_ivalue_based_copy_has_separate_storage
list_test_non_ivalue_based_given_different_iterators_then_are_not_equal
list_test_non_ivalue_based_given_different_lists_then_is_not_equal
list_test_non_ivalue_based_given_empty_when_calling_resize_then_resizes_and_sets_value
list_test_non_ivalue_based_given_empty_when_calling_resize_with_value_then_resizes_and_sets
list_test_non_ivalue_based_given_empty_when_calling_size_then_returns_zero
list_test_non_ivalue_based_given_empty_when_calling_then_returns_true
list_test_non_ivalue_based_given_empty_when_iterating_then_begin_is_end
list_test_non_ivalue_based_given_equal_iterators_then_are
list_test_non_ivalue_based_given_equal_lists_then_is
list_test_non_ivalue_based_given_iterator_when_calculating_difference_then_returns_correct_number
list_test_non_ivalue_based_given_iterator_when_dereferencing_then_points_to_correct_element
list_test_non_ivalue_based_given_iterator_when_postfix_decrementing_then_moves_to_next_and_returns_old_position
list_test_non_ivalue_based_given_iterator_when_postfix_incrementing_then_moves_to_next_and_returns_old_position
list_test_non_ivalue_based_given_iterator_when_swapping_values_from_then_changes_value
list_test_non_ivalue_based_given_nonempty_when_calling_clear_then_is_empty
list_test_non_ivalue_based_given_nonempty_when_calling_empty_then_returns_false
list_test_non_ivalue_based_given_nonempty_when_calling_size_then_returns_number_of_elements
list_test_non_ivalue_based_given_one_element_when_calling_pop_back_then_is_empty
list_test_non_ivalue_based_given_one_element_when_erasing_then_is_empty
list_test_non_ivalue_based_given_when_erasing_full_range_then_is_empty
list_test_non_ivalue_based_given_when_erasing_then_returns_iterator
list_test_non_ivalue_based_is_checks_identity
list_test_non_ivalue_based_is_reference_type
list_test_non_ivalue_based_same_value_different_storage_then_is_returns_false
list_test_non_ivalue_based_when_assigning_to_access_operator_from_then_sets_element
list_test_non_ivalue_based_when_assigning_to_access_operator_with_existing_position_then_sets_element
list_test_non_ivalue_based_when_calling_access_operator_with_existing_position_then_returns_element
list_test_non_ivalue_based_when_calling_access_operator_with_existing_position_then_throws_exception
list_test_non_ivalue_based_when_calling_copying_set_with_existing_position_then_changes_element
list_test_non_ivalue_based_when_calling_copying_set_with_existing_position_then_throws_exception
list_test_non_ivalue_based_when_calling_emplace_back_with_lvalue_then_inserts_element
list_test_non_ivalue_based_when_calling_emplace_back_with_rvalue_then_inserts_element
list_test_non_ivalue_based_when_calling_emplace_with_lvalue_then_inserts_element
list_test_non_ivalue_based_when_calling_emplace_with_rvalue_then_inserts_element
list_test_non_ivalue_based_when_calling_extract_with_existing_position_then_returns_element
list_test_non_ivalue_based_when_calling_extract_with_existing_position_then_throws_exception
list_test_non_ivalue_based_when_calling_get_with_existing_position_then_returns_element
list_test_non_ivalue_based_when_calling_get_with_existing_position_then_throws_exception
list_test_non_ivalue_based_when_calling_insert_on_iterator_with_lvalue_then_inserts_element
list_test_non_ivalue_based_when_calling_insert_on_iterator_with_rvalue_then_inserts_element
list_test_non_ivalue_based_when_calling_moving_set_with_existing_position_then_changes_element
list_test_non_ivalue_based_when_calling_moving_set_with_existing_position_then_throws_exception
list_test_non_ivalue_based_when_calling_push_back_with_lvalue_then_inserts_element
list_test_non_ivalue_based_when_calling_push_back_with_rvalue_then_inserts_element
list_test_non_ivalue_based_when_calling_reserve_then_doesnt_crash
list_test_non_ivalue_based_when_copy_assigning_then_are_equal
list_test_non_ivalue_based_when_copy_constructing_then_are_equal
list_test_non_ivalue_based_when_copying_then_are_equal
list_test_non_ivalue_based_when_iterating_then_finds_elements
list_test_non_ivalue_based_when_iterating_with_foreach_then_finds_elements
list_test_non_ivalue_based_when_move_assigning_then_old_is_empty
list_test_non_ivalue_based_when_move_constructing_then_old_is_empty
list_test_non_ivalue_based_when_swapping_from_access_operator_then_swaps_elements
adaptive_max_pool2d_backward_out_frame
adaptive_max_pool2d_backward_single_out_frame
adaptive_max_pool2d_out_frame
adaptive_max_pool2d_single_out_frame
end_index
lazy_static
start_index
make_per_tensor_quantized_tensor_cpu
global_average_pool
use_global_average_pool
Block
BlockSize
Comparison
HostAllocator
THCCachingHostAllocator
allocate
block_comparator
default
empty_cache
free
get_thc_caching_host_allocator
insert_events
lazy_static
malloc
process_events
raw_deleter
record_event
thc_caching_host_allocator_empty_cache
thc_caching_host_allocator_record_event
thc_caching_host_deleter
compute_stride_for_view_as_complex
compute_stride_for_view_as_real
view_as_complex
view_as_real
view_as_real_physical
view_tensor
define_dispatch
lazy_static
upsample_bilinear2d
upsample_bilinear2d_backward
compute_fused_params
define_dispatch
lazy_static
q_batch_norm1d_impl
q_batch_norm2d_impl
q_batch_norm3d_impl
q_batch_norm_impl
quantized_batch_norm
conv_weight_size
cudnn_conv_use_channels_last
reshape_bias
pytorch_qnnp_requantize_gemmlowp_sse4
mkldnn_adaptive_avg_pool2d
mkldnn_adaptive_avg_pool2d_backward
mkldnn_adaptive_avg_pool2d_backward_out
mkldnn_adaptive_avg_pool2d_out
mkldnn_avg_pool2d
mkldnn_avg_pool2d_backward
mkldnn_avg_pool2d_backward_out
mkldnn_avg_pool2d_out
mkldnn_avg_pool3d
mkldnn_avg_pool3d_backward
mkldnn_avg_pool3d_backward_out
mkldnn_avg_pool3d_out
mkldnn_max_pool2d
mkldnn_max_pool2d_backward
mkldnn_max_pool3d
mkldnn_max_pool3d_backward
mkldnn_pooling
mkldnn_pooling_backward
expect_calls_concat_unboxed
expect_calls_decrement
expect_calls_increment
lazy_static
operator_registration_test_lambda_based_kernel_given_mismatched_with_different_num_returns_when_registering_then_fails
operator_registration_test_lambda_based_kernel_given_mismatched_with_different_return_types_when_registering_then_fails
operator_registration_test_lambda_based_kernel_given_multiple_operators_and_kernels_when_registered_in_one_registrar_then_calls_right
operator_registration_test_lambda_based_kernel_given_multiple_operators_and_kernels_when_registered_in_registrars_then_calls_right
operator_registration_test_lambda_based_kernel_given_out_of_line_when_registered_then_can_be_called
operator_registration_test_lambda_based_kernel_given_when_registered_then_can_be_called
operator_registration_test_lambda_based_kernel_given_when_registered_then_can_be_called_unboxed
operator_registration_test_lambda_based_kernel_given_when_registered_without_specifying_schema_then_infers
operator_registration_test_lambda_based_kernel_given_when_runs_out_of_scope_then_cannot_be_called_anymore
lazy_static
reflection_pad1d_backward_out_frame
reflection_pad1d_backward_out_loop
reflection_pad1d_cpu
reflection_pad1d_out_cpu
reflection_pad1d_out_frame
reflection_pad1d_out_loop
reflection_pad1d_out_template
reflection_pad2d_backward_cpu
reflection_pad2d_backward_out_cpu
reflection_pad2d_backward_out_frame
reflection_pad2d_backward_out_loop
reflection_pad2d_backward_out_template
reflection_pad2d_cpu
reflection_pad2d_out_cpu
reflection_pad2d_out_frame
reflection_pad2d_out_loop
reflection_pad2d_out_template
clear_computation_cache
get_num_threads
get_thread_num
in_parallel_region
init_num_threads
intraop_launch
intraop_launch_future
lazy_static
parallel_for
parallel_reduce
set_num_threads
q8gemm_2x4c8_sse2_k_div_8
q8gemm_2x4c8_sse2_k_div_8_strided_a
q8gemm_2x4c8_sse2_k_div_8_strided_c
q8gemm_2x4c8_sse2_k_div_8_subtile
q8gemm_2x4c8_sse2_k_eq_8
q8gemm_2x4c8_sse2_k_eq_8_azp0
q8gemm_2x4c8_sse2_k_eq_8_bzp0
q8gemm_2x4c8_sse2_k_eq_8_nozp
q8gemm_2x4c8_sse2_k_eq_8_qmax128
q8gemm_2x4c8_sse2_k_eq_8_qmin128
q8gemm_2x4c8_sse2_k_eq_8_strided_a
q8gemm_2x4c8_sse2_k_eq_8_strided_c
q8gemm_2x4c8_sse2_k_gt_8
q8gemm_2x4c8_sse2_k_gt_8_azp0
q8gemm_2x4c8_sse2_k_gt_8_bzp0
q8gemm_2x4c8_sse2_k_gt_8_nozp
q8gemm_2x4c8_sse2_k_gt_8_strided_a
q8gemm_2x4c8_sse2_k_gt_8_strided_c
q8gemm_2x4c8_sse2_k_gt_8_subtile
q8gemm_4x4c2_sse2_k_div_8
q8gemm_4x4c2_sse2_k_div_8_strided_a
q8gemm_4x4c2_sse2_k_div_8_strided_c
q8gemm_4x4c2_sse2_k_div_8_subtile
q8gemm_4x4c2_sse2_k_eq_1
q8gemm_4x4c2_sse2_k_eq_1_azp0
q8gemm_4x4c2_sse2_k_eq_1_bzp0
q8gemm_4x4c2_sse2_k_eq_1_nozp
q8gemm_4x4c2_sse2_k_eq_1_qmax128
q8gemm_4x4c2_sse2_k_eq_1_qmin128
q8gemm_4x4c2_sse2_k_eq_1_strided_a
q8gemm_4x4c2_sse2_k_eq_1_strided_c
q8gemm_4x4c2_sse2_k_eq_8
q8gemm_4x4c2_sse2_k_eq_8_azp0
q8gemm_4x4c2_sse2_k_eq_8_bzp0
q8gemm_4x4c2_sse2_k_eq_8_nozp
q8gemm_4x4c2_sse2_k_eq_8_qmax128
q8gemm_4x4c2_sse2_k_eq_8_qmin128
q8gemm_4x4c2_sse2_k_eq_8_strided_a
q8gemm_4x4c2_sse2_k_eq_8_strided_c
q8gemm_4x4c2_sse2_k_gt_8
q8gemm_4x4c2_sse2_k_gt_8_azp0
q8gemm_4x4c2_sse2_k_gt_8_bzp0
q8gemm_4x4c2_sse2_k_gt_8_nozp
q8gemm_4x4c2_sse2_k_gt_8_strided_a
q8gemm_4x4c2_sse2_k_gt_8_strided_c
q8gemm_4x4c2_sse2_k_gt_8_subtile
q8gemm_4x4c2_sse2_k_lt_4
q8gemm_4x4c2_sse2_k_lt_4_azp0
q8gemm_4x4c2_sse2_k_lt_4_bzp0
q8gemm_4x4c2_sse2_k_lt_4_nozp
q8gemm_4x4c2_sse2_k_lt_4_qmax128
q8gemm_4x4c2_sse2_k_lt_4_qmin128
q8gemm_4x4c2_sse2_k_lt_4_strided_a
q8gemm_4x4c2_sse2_k_lt_4_strided_c
q8gemm_4x4c2_sse2_k_lt_8
q8gemm_4x4c2_sse2_k_lt_8_azp0
q8gemm_4x4c2_sse2_k_lt_8_bzp0
q8gemm_4x4c2_sse2_k_lt_8_nozp
q8gemm_4x4c2_sse2_k_lt_8_qmax128
q8gemm_4x4c2_sse2_k_lt_8_qmin128
q8gemm_4x4c2_sse2_k_lt_8_strided_a
q8gemm_4x4c2_sse2_k_lt_8_strided_c
q8gemm_4x8_aarch32_neon_k_div_8
q8gemm_4x8_aarch32_neon_k_div_8_strided_a
q8gemm_4x8_aarch32_neon_k_div_8_strided_c
q8gemm_4x8_aarch32_neon_k_div_8_subtile
q8gemm_4x8_aarch32_neon_k_eq_8
q8gemm_4x8_aarch32_neon_k_eq_8_azp0
q8gemm_4x8_aarch32_neon_k_eq_8_bzp0
q8gemm_4x8_aarch32_neon_k_eq_8_nozp
q8gemm_4x8_aarch32_neon_k_eq_8_qmax128
q8gemm_4x8_aarch32_neon_k_eq_8_qmin128
q8gemm_4x8_aarch32_neon_k_eq_8_strided_a
q8gemm_4x8_aarch32_neon_k_eq_8_strided_c
q8gemm_4x8_aarch32_neon_k_gt_8
q8gemm_4x8_aarch32_neon_k_gt_8_azp0
q8gemm_4x8_aarch32_neon_k_gt_8_bzp0
q8gemm_4x8_aarch32_neon_k_gt_8_nozp
q8gemm_4x8_aarch32_neon_k_gt_8_strided_a
q8gemm_4x8_aarch32_neon_k_gt_8_strided_c
q8gemm_4x8_aarch32_neon_k_gt_8_subtile
q8gemm_4x8_neon_k_div_8
q8gemm_4x8_neon_k_div_8_strided_a
q8gemm_4x8_neon_k_div_8_strided_c
q8gemm_4x8_neon_k_div_8_subtile
q8gemm_4x8_neon_k_eq_8
q8gemm_4x8_neon_k_eq_8_azp0
q8gemm_4x8_neon_k_eq_8_bzp0
q8gemm_4x8_neon_k_eq_8_nozp
q8gemm_4x8_neon_k_eq_8_qmax128
q8gemm_4x8_neon_k_eq_8_qmin128
q8gemm_4x8_neon_k_eq_8_strided_a
q8gemm_4x8_neon_k_eq_8_strided_c
q8gemm_4x8_neon_k_gt_8
q8gemm_4x8_neon_k_gt_8_azp0
q8gemm_4x8_neon_k_gt_8_bzp0
q8gemm_4x8_neon_k_gt_8_nozp
q8gemm_4x8_neon_k_gt_8_strided_a
q8gemm_4x8_neon_k_gt_8_strided_c
q8gemm_4x8_neon_k_gt_8_subtile
q8gemm_4x8c2_xzp_aarch32_neon_k_div_8
q8gemm_4x8c2_xzp_aarch32_neon_k_div_8_strided_a
q8gemm_4x8c2_xzp_aarch32_neon_k_div_8_strided_c
q8gemm_4x8c2_xzp_aarch32_neon_k_div_8_subtile
q8gemm_4x8c2_xzp_aarch32_neon_k_eq_8
q8gemm_4x8c2_xzp_aarch32_neon_k_eq_8_azp0
q8gemm_4x8c2_xzp_aarch32_neon_k_eq_8_bzp0
q8gemm_4x8c2_xzp_aarch32_neon_k_eq_8_nozp
q8gemm_4x8c2_xzp_aarch32_neon_k_eq_8_qmax128
q8gemm_4x8c2_xzp_aarch32_neon_k_eq_8_qmin128
q8gemm_4x8c2_xzp_aarch32_neon_k_eq_8_strided_a
q8gemm_4x8c2_xzp_aarch32_neon_k_eq_8_strided_c
q8gemm_4x8c2_xzp_aarch32_neon_k_gt_8
q8gemm_4x8c2_xzp_aarch32_neon_k_gt_8_azp0
q8gemm_4x8c2_xzp_aarch32_neon_k_gt_8_bzp0
q8gemm_4x8c2_xzp_aarch32_neon_k_gt_8_nozp
q8gemm_4x8c2_xzp_aarch32_neon_k_gt_8_strided_a
q8gemm_4x8c2_xzp_aarch32_neon_k_gt_8_strided_c
q8gemm_4x8c2_xzp_aarch32_neon_k_gt_8_subtile
q8gemm_4x8c2_xzp_neon_k_div_8
q8gemm_4x8c2_xzp_neon_k_div_8_strided_a
q8gemm_4x8c2_xzp_neon_k_div_8_strided_c
q8gemm_4x8c2_xzp_neon_k_div_8_subtile
q8gemm_4x8c2_xzp_neon_k_eq_8
q8gemm_4x8c2_xzp_neon_k_eq_8_azp0
q8gemm_4x8c2_xzp_neon_k_eq_8_bzp0
q8gemm_4x8c2_xzp_neon_k_eq_8_nozp
q8gemm_4x8c2_xzp_neon_k_eq_8_qmax128
q8gemm_4x8c2_xzp_neon_k_eq_8_qmin128
q8gemm_4x8c2_xzp_neon_k_eq_8_strided_a
q8gemm_4x8c2_xzp_neon_k_eq_8_strided_c
q8gemm_4x8c2_xzp_neon_k_gt_8
q8gemm_4x8c2_xzp_neon_k_gt_8_azp0
q8gemm_4x8c2_xzp_neon_k_gt_8_bzp0
q8gemm_4x8c2_xzp_neon_k_gt_8_nozp
q8gemm_4x8c2_xzp_neon_k_gt_8_strided_a
q8gemm_4x8c2_xzp_neon_k_gt_8_strided_c
q8gemm_4x8c2_xzp_neon_k_gt_8_subtile
q8gemm_6x4_neon_k_div_8
q8gemm_6x4_neon_k_div_8_strided_a
q8gemm_6x4_neon_k_div_8_strided_c
q8gemm_6x4_neon_k_div_8_subtile
q8gemm_6x4_neon_k_eq_8
q8gemm_6x4_neon_k_eq_8_azp0
q8gemm_6x4_neon_k_eq_8_bzp0
q8gemm_6x4_neon_k_eq_8_nozp
q8gemm_6x4_neon_k_eq_8_qmax128
q8gemm_6x4_neon_k_eq_8_qmin128
q8gemm_6x4_neon_k_eq_8_strided_a
q8gemm_6x4_neon_k_eq_8_strided_c
q8gemm_6x4_neon_k_gt_8
q8gemm_6x4_neon_k_gt_8_azp0
q8gemm_6x4_neon_k_gt_8_bzp0
q8gemm_6x4_neon_k_gt_8_nozp
q8gemm_6x4_neon_k_gt_8_strided_a
q8gemm_6x4_neon_k_gt_8_strided_c
q8gemm_6x4_neon_k_gt_8_subtile
q8gemm_8x8_aarch64_neon_k_div_8
q8gemm_8x8_aarch64_neon_k_div_8_strided_a
q8gemm_8x8_aarch64_neon_k_div_8_strided_c
q8gemm_8x8_aarch64_neon_k_div_8_subtile
q8gemm_8x8_aarch64_neon_k_eq_8
q8gemm_8x8_aarch64_neon_k_eq_8_azp0
q8gemm_8x8_aarch64_neon_k_eq_8_bzp0
q8gemm_8x8_aarch64_neon_k_eq_8_nozp
q8gemm_8x8_aarch64_neon_k_eq_8_qmax128
q8gemm_8x8_aarch64_neon_k_eq_8_qmin128
q8gemm_8x8_aarch64_neon_k_eq_8_strided_a
q8gemm_8x8_aarch64_neon_k_eq_8_strided_c
q8gemm_8x8_aarch64_neon_k_gt_8
q8gemm_8x8_aarch64_neon_k_gt_8_azp0
q8gemm_8x8_aarch64_neon_k_gt_8_bzp0
q8gemm_8x8_aarch64_neon_k_gt_8_nozp
q8gemm_8x8_aarch64_neon_k_gt_8_strided_a
q8gemm_8x8_aarch64_neon_k_gt_8_strided_c
q8gemm_8x8_aarch64_neon_k_gt_8_subtile
q8gemm_8x8_neon_k_div_8
q8gemm_8x8_neon_k_div_8_strided_a
q8gemm_8x8_neon_k_div_8_strided_c
q8gemm_8x8_neon_k_div_8_subtile
q8gemm_8x8_neon_k_eq_8
q8gemm_8x8_neon_k_eq_8_azp0
q8gemm_8x8_neon_k_eq_8_bzp0
q8gemm_8x8_neon_k_eq_8_nozp
q8gemm_8x8_neon_k_eq_8_qmax128
q8gemm_8x8_neon_k_eq_8_qmin128
q8gemm_8x8_neon_k_eq_8_strided_a
q8gemm_8x8_neon_k_eq_8_strided_c
q8gemm_8x8_neon_k_gt_8
q8gemm_8x8_neon_k_gt_8_azp0
q8gemm_8x8_neon_k_gt_8_bzp0
q8gemm_8x8_neon_k_gt_8_nozp
q8gemm_8x8_neon_k_gt_8_strided_a
q8gemm_8x8_neon_k_gt_8_strided_c
q8gemm_8x8_neon_k_gt_8_subtile
q8gemm_dq_4x4c2_sse2_k_div_8
q8gemm_dq_4x4c2_sse2_k_div_8_strided_a
q8gemm_dq_4x4c2_sse2_k_div_8_strided_c
q8gemm_dq_4x4c2_sse2_k_div_8_subtile
q8gemm_dq_4x4c2_sse2_k_eq_8
q8gemm_dq_4x4c2_sse2_k_eq_8_azp0
q8gemm_dq_4x4c2_sse2_k_eq_8_bzp0
q8gemm_dq_4x4c2_sse2_k_eq_8_nozp
q8gemm_dq_4x4c2_sse2_k_eq_8_qmax128
q8gemm_dq_4x4c2_sse2_k_eq_8_qmin128
q8gemm_dq_4x4c2_sse2_k_eq_8_strided_a
q8gemm_dq_4x4c2_sse2_k_eq_8_strided_c
q8gemm_dq_4x4c2_sse2_k_gt_8
q8gemm_dq_4x4c2_sse2_k_gt_8_azp0
q8gemm_dq_4x4c2_sse2_k_gt_8_bzp0
q8gemm_dq_4x4c2_sse2_k_gt_8_nozp
q8gemm_dq_4x4c2_sse2_k_gt_8_strided_a
q8gemm_dq_4x4c2_sse2_k_gt_8_strided_c
q8gemm_dq_4x4c2_sse2_k_gt_8_subtile
q8gemm_dq_4x4c2_sse2_k_lt_4
q8gemm_dq_4x4c2_sse2_k_lt_4_azp0
q8gemm_dq_4x4c2_sse2_k_lt_4_bzp0
q8gemm_dq_4x4c2_sse2_k_lt_4_nozp
q8gemm_dq_4x4c2_sse2_k_lt_4_qmax128
q8gemm_dq_4x4c2_sse2_k_lt_4_qmin128
q8gemm_dq_4x4c2_sse2_k_lt_4_strided_a
q8gemm_dq_4x4c2_sse2_k_lt_4_strided_c
q8gemm_dq_4x4c2_sse2_k_lt_8
q8gemm_dq_4x4c2_sse2_k_lt_8_azp0
q8gemm_dq_4x4c2_sse2_k_lt_8_bzp0
q8gemm_dq_4x4c2_sse2_k_lt_8_nozp
q8gemm_dq_4x4c2_sse2_k_lt_8_qmax128
q8gemm_dq_4x4c2_sse2_k_lt_8_qmin128
q8gemm_dq_4x4c2_sse2_k_lt_8_strided_a
q8gemm_dq_4x4c2_sse2_k_lt_8_strided_c
q8gemm_dq_4x8_aarch32_neon_k_div_8
q8gemm_dq_4x8_aarch32_neon_k_div_8_strided_a
q8gemm_dq_4x8_aarch32_neon_k_div_8_strided_c
q8gemm_dq_4x8_aarch32_neon_k_div_8_subtile
q8gemm_dq_4x8_aarch32_neon_k_eq_8
q8gemm_dq_4x8_aarch32_neon_k_eq_8_azp0
q8gemm_dq_4x8_aarch32_neon_k_eq_8_bzp0
q8gemm_dq_4x8_aarch32_neon_k_eq_8_nozp
q8gemm_dq_4x8_aarch32_neon_k_eq_8_qmax128
q8gemm_dq_4x8_aarch32_neon_k_eq_8_qmin128
q8gemm_dq_4x8_aarch32_neon_k_eq_8_strided_a
q8gemm_dq_4x8_aarch32_neon_k_eq_8_strided_c
q8gemm_dq_4x8_aarch32_neon_k_gt_8
q8gemm_dq_4x8_aarch32_neon_k_gt_8_azp0
q8gemm_dq_4x8_aarch32_neon_k_gt_8_bzp0
q8gemm_dq_4x8_aarch32_neon_k_gt_8_nozp
q8gemm_dq_4x8_aarch32_neon_k_gt_8_strided_a
q8gemm_dq_4x8_aarch32_neon_k_gt_8_strided_c
q8gemm_dq_4x8_aarch32_neon_k_gt_8_subtile
q8gemm_dq_4x8_neon_k_div_8
q8gemm_dq_4x8_neon_k_div_8_strided_a
q8gemm_dq_4x8_neon_k_div_8_strided_c
q8gemm_dq_4x8_neon_k_div_8_subtile
q8gemm_dq_4x8_neon_k_eq_8
q8gemm_dq_4x8_neon_k_eq_8_azp0
q8gemm_dq_4x8_neon_k_eq_8_bzp0
q8gemm_dq_4x8_neon_k_eq_8_nozp
q8gemm_dq_4x8_neon_k_eq_8_qmax128
q8gemm_dq_4x8_neon_k_eq_8_qmin128
q8gemm_dq_4x8_neon_k_eq_8_strided_a
q8gemm_dq_4x8_neon_k_eq_8_strided_c
q8gemm_dq_4x8_neon_k_gt_8
q8gemm_dq_4x8_neon_k_gt_8_azp0
q8gemm_dq_4x8_neon_k_gt_8_bzp0
q8gemm_dq_4x8_neon_k_gt_8_nozp
q8gemm_dq_4x8_neon_k_gt_8_strided_a
q8gemm_dq_4x8_neon_k_gt_8_strided_c
q8gemm_dq_4x8_neon_k_gt_8_subtile
q8gemm_dq_8x8_aarch64_neon_k_div_8
q8gemm_dq_8x8_aarch64_neon_k_div_8_strided_a
q8gemm_dq_8x8_aarch64_neon_k_div_8_strided_c
q8gemm_dq_8x8_aarch64_neon_k_div_8_subtile
q8gemm_dq_8x8_aarch64_neon_k_eq_8
q8gemm_dq_8x8_aarch64_neon_k_eq_8_azp0
q8gemm_dq_8x8_aarch64_neon_k_eq_8_bzp0
q8gemm_dq_8x8_aarch64_neon_k_eq_8_nozp
q8gemm_dq_8x8_aarch64_neon_k_eq_8_qmax128
q8gemm_dq_8x8_aarch64_neon_k_eq_8_qmin128
q8gemm_dq_8x8_aarch64_neon_k_eq_8_strided_a
q8gemm_dq_8x8_aarch64_neon_k_eq_8_strided_c
q8gemm_dq_8x8_aarch64_neon_k_gt_8
q8gemm_dq_8x8_aarch64_neon_k_gt_8_azp0
q8gemm_dq_8x8_aarch64_neon_k_gt_8_bzp0
q8gemm_dq_8x8_aarch64_neon_k_gt_8_nozp
q8gemm_dq_8x8_aarch64_neon_k_gt_8_strided_a
q8gemm_dq_8x8_aarch64_neon_k_gt_8_strided_c
q8gemm_dq_8x8_aarch64_neon_k_gt_8_subtile
ActivationBackwardFn
ActivationFn
EluBackwardFn
EluFn
HardsigmoidBackwardFn
HardsigmoidFn
HardswishBackwardFn
HardswishFn
HardtanhBackwardFn
LeakyReluBackwardFn
LeakyReluFn
LogSigmoidCpuFn
ShrinkBackwardFn
ShrinkFn
SoftplusBackwardFn
SoftplusFn
SoftshrinkFn
StructuredActivationBackwardFn
StructuredActivationFn
ThresholdFn
celu
celu_mut
declare_dispatch
define_dispatch
hardswish
hardswish_backward
hardswish_mut
hardswish_out
hardtanh
hardtanh_backward
hardtanh_backward_out
hardtanh_mut
hardtanh_out
infinitely_differentiable_gelu_backward
lazy_static
log_sigmoid
log_sigmoid_backward_cpu
log_sigmoid_backward_out_cpu
log_sigmoid_forward_cpu
log_sigmoid_forward_out_cpu
log_sigmoid_out
math_mish_backward
math_silu_backward
mish_backward
prelu_backward_cpu
prelu_cpu
prelu_cpu_backward_kernel_multi_weights
prelu_cpu_backward_kernel_share_weights
prelu_cpu_kernel_multi_weights
prelu_cpu_kernel_share_weights
relu
relu6
relu6_mut
relu_mut
rrelu
rrelu_mut
rrelu_with_noise_backward
rrelu_with_noise_cpu
rrelu_with_noise_cpu_mut
rrelu_with_noise_out_cpu
rrelu_with_noise_train
selu
selu_mut
silu_backward
softshrink_check
HIPGuardImplMasqueradingAsCUDA
HIPGuardMasqueradingAsCUDA
HIPMultiStreamGuardMasqueradingAsCUDA
HIPStreamGuardMasqueradingAsCUDA
OptionalHIPGuardMasqueradingAsCUDA
OptionalHIPStreamGuardMasqueradingAsCUDA
block
create_event
current_device
current_stream
destroy_event
device_count
exchange_device
exchange_stream
get_default_stream
get_device
get_stream
get_stream_from_global_pool
lazy_static
original_device
original_stream
query_event
query_stream
record
record_data_ptr_on_stream
reset
reset_device
reset_stream
set_device
set_index
synchronize_stream
ty
unchecked_set_device
unwrap_streams
TupleInfoCPU
lazy_static
swap
tie
pytorch_qnnp_requantize_fp32_sse2
DeprecatedTypePropertiesDeleter
DeprecatedTypePropertiesRegistry
get_deprecated_type_properties
global_deprecated_type_properties_registry
invoke
AnnotatedKernel
AnnotatedSchema
CppSignatureWithDebug
OperatorEntry
assert_signature_is_correct
check_invariants
check_schema
compute_dispatch_table_entry
compute_dispatch_table_entry_with_debug
debug
deregister_kernel
deregister_schema
dispatch_key_extractor
dump_computed_table
dump_state
get_kernel_for_dispatch_key
has_kernel_for_any_dispatch_key
has_schema
is_observed
lazy_static
list_all_dispatch_keys
lookup
of
operator_name
register_kernel
register_schema
report_error
report_signature_error
schema
to_string
update_dispatch_table
update_dispatch_table_entry
update_dispatch_table_full
update_fallback
update_schema_alias_analysis
CallbackHandle
CallbackHandles
CallbackManager
CoinflipTLS
DisableRecordFunctionGuard
EndCallback
GlobalRecordFunctionCallbacks
GlobalRecordFunctionCallbacksEntry
ObserverContext
ObserverContextList
RecordFunction
RecordFunctionCallback
RecordFunctionGuard
RecordFunctionHandle
RecordFunctionState
RecordFunctionTLS
RecordScope
StartCallback
StringView
ThreadLocalRecordFunctionCallbacks
ThreadLocalRecordFunctionCallbacksEntry
ToggledCallbackResult
add_global_callback
add_thread_local_callback
assign_from
before
bump_record_all_functions
callback_should_run
check_record_all_functions
check_scope
clear_callbacks
clear_global_callbacks
clear_thread_local_callbacks
coinflip_tls
current_thread_id
default
disable
disable_callback
drop
enable
enable_record_function
end
eq
find_and_remove_callback
find_and_toggle_callback
fmt
forward_thread_id
get_default_node_id
get_record_function_tls
handle
has_callbacks
has_global_callbacks
has_thread_local_callbacks
hash
init
is_active
is_async
is_enabled
is_record_function_enabled
lazy_static
manager
merge_run_callbacks
name
needs_ids
next_unique_callback_handle
next_unique_record_function_handle
operator_name
reenable_callback
release_record_all_functions
remove_callback
rf_tls
run_end_callbacks
run_start_callbacks
sample_geometric
sample_zero_one
sampling_prob
scope
scopes
seq_nr
set_async
set_default_node_id
set_forward_thread_id
set_handle
set_record_function_tls
should_run_record_function
start
str_
thread_id
try_run_callback
DispatchKeySet
OperatorKernel
Stack
SupportedPrimitiveArgTypes
TypeCheckHelper
decay_if_not_tensor
instead
is
make_boxed_from_unboxed_functor
mismatch
of
return_to_ivalue
supplied
that
to
wrap_kernel_functor_unboxed_
pytorch_q8gemm_xzp_ukernel_4x8c2_neon
q8avgpool_mp8x9p8q_neon_kc_div_8_multipass_fulltile
q8avgpool_mp8x9p8q_neon_kc_div_8_multipass_fulltile_with_x_stride
q8avgpool_mp8x9p8q_neon_kc_div_8_multipass_subtile
q8avgpool_mp8x9p8q_neon_kc_div_8_twopass_fulltile
q8avgpool_mp8x9p8q_neon_kc_div_8_twopass_fulltile_with_x_stride
q8avgpool_mp8x9p8q_neon_kc_div_8_twopass_subtile
q8avgpool_mp8x9p8q_neon_kc_div_8_with_x_scale
q8avgpool_mp8x9p8q_neon_kc_div_8_with_x_zero_point
q8avgpool_mp8x9p8q_neon_kc_div_8_with_y_max
q8avgpool_mp8x9p8q_neon_kc_div_8_with_y_min
q8avgpool_mp8x9p8q_neon_kc_div_8_with_y_scale
q8avgpool_mp8x9p8q_neon_kc_div_8_with_y_zero_point
q8avgpool_mp8x9p8q_neon_kc_eq_8_multipass_fulltile
q8avgpool_mp8x9p8q_neon_kc_eq_8_multipass_subtile
q8avgpool_mp8x9p8q_neon_kc_eq_8_twopass_fulltile
q8avgpool_mp8x9p8q_neon_kc_eq_8_twopass_subtile
q8avgpool_mp8x9p8q_neon_kc_gt_8_multipass_fulltile
q8avgpool_mp8x9p8q_neon_kc_gt_8_multipass_fulltile_with_x_stride
q8avgpool_mp8x9p8q_neon_kc_gt_8_multipass_subtile
q8avgpool_mp8x9p8q_neon_kc_gt_8_twopass_fulltile
q8avgpool_mp8x9p8q_neon_kc_gt_8_twopass_fulltile_with_x_stride
q8avgpool_mp8x9p8q_neon_kc_gt_8_twopass_subtile
q8avgpool_mp8x9p8q_neon_small_n
q8avgpool_mp8x9p8q_neon_small_n_with_s
q8avgpool_mp8x9p8q_neon_small_n_with_x_stride
q8avgpool_mp8x9p8q_neon_small_n_with_y_stride
q8avgpool_mp8x9p8q_sse2_kc_div_8_multipass_fulltile
q8avgpool_mp8x9p8q_sse2_kc_div_8_multipass_fulltile_with_x_stride
q8avgpool_mp8x9p8q_sse2_kc_div_8_multipass_subtile
q8avgpool_mp8x9p8q_sse2_kc_div_8_twopass_fulltile
q8avgpool_mp8x9p8q_sse2_kc_div_8_twopass_fulltile_with_x_stride
q8avgpool_mp8x9p8q_sse2_kc_div_8_twopass_subtile
q8avgpool_mp8x9p8q_sse2_kc_div_8_with_x_scale
q8avgpool_mp8x9p8q_sse2_kc_div_8_with_x_zero_point
q8avgpool_mp8x9p8q_sse2_kc_div_8_with_y_max
q8avgpool_mp8x9p8q_sse2_kc_div_8_with_y_min
q8avgpool_mp8x9p8q_sse2_kc_div_8_with_y_scale
q8avgpool_mp8x9p8q_sse2_kc_div_8_with_y_zero_point
q8avgpool_mp8x9p8q_sse2_kc_eq_8_multipass_fulltile
q8avgpool_mp8x9p8q_sse2_kc_eq_8_multipass_subtile
q8avgpool_mp8x9p8q_sse2_kc_eq_8_twopass_fulltile
q8avgpool_mp8x9p8q_sse2_kc_eq_8_twopass_subtile
q8avgpool_mp8x9p8q_sse2_kc_gt_8_multipass_fulltile
q8avgpool_mp8x9p8q_sse2_kc_gt_8_multipass_fulltile_with_x_stride
q8avgpool_mp8x9p8q_sse2_kc_gt_8_multipass_subtile
q8avgpool_mp8x9p8q_sse2_kc_gt_8_twopass_fulltile
q8avgpool_mp8x9p8q_sse2_kc_gt_8_twopass_fulltile_with_x_stride
q8avgpool_mp8x9p8q_sse2_kc_gt_8_twopass_subtile
q8avgpool_mp8x9p8q_sse2_small_n
q8avgpool_mp8x9p8q_sse2_small_n_with_s
q8avgpool_mp8x9p8q_sse2_small_n_with_x_stride
q8avgpool_mp8x9p8q_sse2_small_n_with_y_stride
q8avgpool_up8x9_neon_kc_div_8_fulltile
q8avgpool_up8x9_neon_kc_div_8_fulltile_with_x_stride
q8avgpool_up8x9_neon_kc_div_8_subtile
q8avgpool_up8x9_neon_kc_div_8_with_x_scale
q8avgpool_up8x9_neon_kc_div_8_with_x_zero_point
q8avgpool_up8x9_neon_kc_div_8_with_y_max
q8avgpool_up8x9_neon_kc_div_8_with_y_min
q8avgpool_up8x9_neon_kc_div_8_with_y_scale
q8avgpool_up8x9_neon_kc_div_8_with_y_zero_point
q8avgpool_up8x9_neon_kc_eq_8_fulltile
q8avgpool_up8x9_neon_kc_eq_8_subtile
q8avgpool_up8x9_neon_kc_gt_8_fulltile
q8avgpool_up8x9_neon_kc_gt_8_fulltile_with_x_stride
q8avgpool_up8x9_neon_kc_gt_8_subtile
q8avgpool_up8x9_neon_small_n
q8avgpool_up8x9_neon_small_n_with_s
q8avgpool_up8x9_neon_small_n_with_x_stride
q8avgpool_up8x9_neon_small_n_with_y_stride
q8avgpool_up8x9_sse2_kc_div_8_fulltile
q8avgpool_up8x9_sse2_kc_div_8_fulltile_with_x_stride
q8avgpool_up8x9_sse2_kc_div_8_subtile
q8avgpool_up8x9_sse2_kc_div_8_with_x_scale
q8avgpool_up8x9_sse2_kc_div_8_with_x_zero_point
q8avgpool_up8x9_sse2_kc_div_8_with_y_max
q8avgpool_up8x9_sse2_kc_div_8_with_y_min
q8avgpool_up8x9_sse2_kc_div_8_with_y_scale
q8avgpool_up8x9_sse2_kc_div_8_with_y_zero_point
q8avgpool_up8x9_sse2_kc_eq_8_fulltile
q8avgpool_up8x9_sse2_kc_eq_8_subtile
q8avgpool_up8x9_sse2_kc_gt_8_fulltile
q8avgpool_up8x9_sse2_kc_gt_8_fulltile_with_x_stride
q8avgpool_up8x9_sse2_kc_gt_8_subtile
q8avgpool_up8x9_sse2_small_n
q8avgpool_up8x9_sse2_small_n_with_s
q8avgpool_up8x9_sse2_small_n_with_x_stride
q8avgpool_up8x9_sse2_small_n_with_y_stride
q8avgpool_up8xm_neon_kc_lt_8_small_ks
q8avgpool_up8xm_neon_kc_lt_8_with_x_scale
q8avgpool_up8xm_neon_kc_lt_8_with_x_zero_point
q8avgpool_up8xm_neon_kc_lt_8_with_y_max
q8avgpool_up8xm_neon_kc_lt_8_with_y_min
q8avgpool_up8xm_neon_kc_lt_8_with_y_scale
q8avgpool_up8xm_neon_kc_lt_8_with_y_zero_point
q8avgpool_up8xm_neon_small_n
q8avgpool_up8xm_neon_small_n_with_s
q8avgpool_up8xm_neon_small_n_with_x_stride
q8avgpool_up8xm_neon_small_n_with_y_stride
q8avgpool_up8xm_sse2_kc_lt_8_small_ks
q8avgpool_up8xm_sse2_kc_lt_8_with_x_scale
q8avgpool_up8xm_sse2_kc_lt_8_with_x_zero_point
q8avgpool_up8xm_sse2_kc_lt_8_with_y_max
q8avgpool_up8xm_sse2_kc_lt_8_with_y_min
q8avgpool_up8xm_sse2_kc_lt_8_with_y_scale
q8avgpool_up8xm_sse2_kc_lt_8_with_y_zero_point
q8avgpool_up8xm_sse2_small_n
q8avgpool_up8xm_sse2_small_n_with_s
q8avgpool_up8xm_sse2_small_n_with_x_stride
q8avgpool_up8xm_sse2_small_n_with_y_stride
sconv_6x8_psimd_k_eq_1
sconv_6x8_psimd_k_eq_1_qmax128
sconv_6x8_psimd_k_eq_1_qmin128
sconv_6x8_psimd_k_eq_1_strided_c
sconv_6x8_psimd_k_gt_1
sconv_6x8_psimd_k_gt_1_strided_c
sconv_6x8_psimd_k_gt_1_subtile
ANeuralNetworksCompilation
ANeuralNetworksDevice
ANeuralNetworksEvent
ANeuralNetworksExecution
ANeuralNetworksMemory
ANeuralNetworksModel
ANeuralNetworksOperandType
ANeuralNetworksOperationType
OperandCode
PreferenceCode
ResultCode
leaky_relu_op_small_batch
leaky_relu_op_unit_batch
leaky_relu_op_unit_batch_with_negative_slope
leaky_relu_op_unit_batch_with_qmax
leaky_relu_op_unit_batch_with_qmin
leaky_relu_op_zero_batch
FullyConnectedSparseOperatorTester
Mode
batch_size
col_block_size
fill_block_sparse_weights
iterations
print_matrix_f32
print_matrix_u8
qmax
qmin
row_block_size
sparsity
testq8
testq8_prepacked
divup
get_num_interop_threads
get_num_threads
get_parallel_info
get_thread_num
in_parallel_region
init_num_threads
intraop_default_num_threads
intraop_launch
intraop_launch_future
launch
launch_no_thread_state
lazy_init_num_threads
parallel_for
parallel_reduce
set_num_interop_threads
set_num_threads
CpuScatterGatherBaseKernel
CpuScatterGatherDimLoop
ReduceAdd
ReduceMultiply
TensorAssign
gather_cpu_kernel
invoke
lazy_static
register_dispatch
scatter_add_cpu_kernel
scatter_cpu_kernel
scatter_fill_cpu_kernel
scatter_reduce_cpu_kernel
scatter_scalar_reduce_cpu_kernel
contiguous_if_zero_in_strides
QuantizeValCommand
check_zero_point
dequantize_val
dequantize_vec
quantize
quantize_val_arm
quantize_val_fbgemm
quantize_val_float_qparams
quantize_vec
requantize_from_int
requantize_val
round
declare_pytorch_q8gemm_dynamic_quantization_ukernel_function
declare_pytorch_q8gemm_ukernel_function
declare_pytorch_q8gemm_xzp_ukernel_function
pytorch_q8sumrows_ukernel_4x_neon
pytorch_qnnp_conv_dynamic_quantization_params
pytorch_qnnp_x8zip_x3_sse2
ConstBlock
adaptive_avg_pool2d
add_a
add_b
addmm
avg_pool2d
buffer_from_optional_host_data
buffer_zeros
cat
clamp
conv2d_a
conv2d_b
conv2d_c
conv2d_d
conv2d_depthwise_a
conv2d_depthwise_b
conv2d_depthwise_c
conv2d_e
conv2d_prepack_weights
conv2d_prepack_weights_image
conv2d_prepack_weights_image_sizes
conv2d_prepack_weights_to_image
kernelnchw_ochw_repack_o4c4h_wi4o4
max_pool2d
mean
mul
reshape_copy
slice
transpose
upsample_nearest2d
adaptive_avg_pool3d
adaptive_avg_pool3d_backward_cpu
adaptive_avg_pool3d_backward_out_cpu
adaptive_avg_pool3d_backward_out_cpu_template
adaptive_avg_pool3d_backward_out_frame
adaptive_avg_pool3d_cpu
adaptive_avg_pool3d_out_cpu
adaptive_avg_pool3d_out_cpu_template
adaptive_avg_pool3d_out_frame
end_index
start_index
DWConvMicrokernelTester
channels
cr
iterations
kernel_height
kernel_size
kernel_width
kernel_zero_point
packed_channels
qmax
qmin
subsampling
test
width
RNGTest
TestCPUGenerator
bernoulli_float
bernoulli_out
bernoulli_tensor
cauchy
clone_impl
current_seed
device_type
exponential
geometric
get_state
log_normal
next_double_normal_sample
next_float_normal_sample
normal
normal_float_tensor
normal_float_tensor_out
normal_tensor_float
normal_tensor_float_out
normal_tensor_tensor
normal_tensor_tensor_out
random
random64
random_from_to
random_to
rng_test_bernoulli
rng_test_bernoulli_2
rng_test_bernoulli_out
rng_test_bernoulli_p
rng_test_bernoulli_p_2
rng_test_bernoulli_scalar
rng_test_bernoulli_tensor
rng_test_cauchy
rng_test_exponential
rng_test_geometric
rng_test_log_normal
rng_test_normal
rng_test_normal_float_tensor
rng_test_normal_float_tensor_out
rng_test_normal_tensor
rng_test_normal_tensor_float
rng_test_normal_tensor_float_out
rng_test_normal_tensor_out
rng_test_random
rng_test_random_64bits
rng_test_random_from_to
rng_test_uniform
seed
set_current_seed
set_next_double_normal_sample
set_next_float_normal_sample
set_state
uniform
A
ArgumentDef
CreateSingleReturn
bool_t
call
createArguments
createReturns
create_function_schema_from_traits_flattened_returns
create_function_schema_from_traits_single_return
find_schema_differences
for
infer_function_schema_flattened_returns
infer_function_schema_single_return
lazy_static
make_function_schema_a
make_function_schema_b
copy_to_host
bucketize_cpu_scalar_tensor
bucketize_cpu_tensor_tensor
bucketize_out_cpu
conversion
cus_lower_bound
dispatch
if
searchsorted_cpu_contiguous
searchsorted_cpu_tensor_scalar
searchsorted_cpu_tensor_tensor
searchsorted_out_cpu
lazy_static
Output
SizeType
ValueType
VectorizedDouble
abs
acos
add
angle
arange
asin
atan
atan2
bitand
bitor
bitxor
blend
blendv
ceil
clamp
clamp_max
clamp_min
conj
convert
copysign
cos
cosh
div
eq
erf
erfc
erfinv
exp
expm1
floor
fmadd
fmod
frac
ge
gt
hypot
i0
i0e
igamma
igammac
imag
isnan
le
lgamma
loadu
log
log10
log1p
log2
lt
map
maximum
minimum
mul
ne
neg
nextafter
operator_m_256d
partial_cmp
pow
real
reciprocal
round
rsqrt
set
sin
sinh
size
sqrt
store
sub
tan
tanh
trunc
zero_mask
Functor
cpp_signature_test_given_different_functor_and_function_then_are_different
cpp_signature_test_given_different_signature_then_are_different
cpp_signature_test_given_equal_functor_and_function_then_are_equal
cpp_signature_test_given_equal_signature_then_are_equal
cpp_signature_test_given_then_can_query_name_without_crashing
ctc_loss
ctc_loss_backward_cpu
ctc_loss_backward_cpu_template
ctc_loss_cpu
ctc_loss_cpu_template
ctc_loss_tensor
KeyValueCompAsc
KeyValueCompDesc
dim_apply
fill_indices
invoke
register_dispatch
sort_kernel
topk_kernel
generate_sizes2d
generate_sizes4d
lazy_static
quantize_per_channel_2d
quantize_per_channel_4d_channels_last
quantize_per_channel_4d_contiguous
pytorch_qnnp_requantize_precise_ssse3
foreach_binary_op_list
foreach_binary_op_list_alpha
foreach_binary_op_scalar
foreach_binary_op_scalarlist
foreach_maximum_minimum_op
foreach_pointwise_op_scalar
foreach_pointwise_op_scalarlist
foreach_tensor_zero_slow
foreach_unary_op
apply_loss_reduction
binary_cross_entropy_backward_cpu
binary_cross_entropy_backward_out_cpu
binary_cross_entropy_cpu
binary_cross_entropy_out_cpu
binary_cross_entropy_with_logits
binary_cross_entropy_with_logits_backward
cosine_embedding_loss
define_dispatch
hinge_embedding_loss
huber_loss
huber_loss_backward
huber_loss_backward_out
huber_loss_out
kl_div
kl_div_backward_cpu
l1_loss
l1_loss_backward
l1_loss_backward_out
l1_loss_out
mse_loss
mse_loss_backward
mse_loss_backward_out
mse_loss_out
poisson_nll_loss
smooth_l1_loss
smooth_l1_loss_backward
smooth_l1_loss_backward_out
smooth_l1_loss_out
pytorch_qnnp_requantize_q31_sse2
check_unify_and_match
dimname_test_create_normal_name
dimname_test_is_valid_identifier
dimname_test_unify_and_match
dimname_test_wildcard_name
CONTIGUOUS
T
add_attribute
add_constant
add_forward_hook
add_forward_pre_hook
add_method
add_property
add_static_method
annotation_str_impl
as
check_forward_hook_schema
check_forward_pre_hook_schema
check_no_any
check_not_exist
compilation_unit
compute_stride_props
containers
contains_any
create
create_contiguous
create_named
dump
element_type_can_be_inferred_from_members
eq
find_constant
find_forward_hook
find_forward_pre_hook
find_hook
find_method
find_static_method
fmt
for
from
get
get_constant
get_forward_hook_error_message
get_forward_hooks
get_forward_pre_hook_error_message
get_forward_pre_hooks
get_hook
get_method
get_property
has_method
if
is
is_module
is_null_or_equal
is_sub_type_impl
is_subtype_of_ext
is_unresolved_class_attribute
lazy_static
list
match_tensor
match_type_variables
matches
merge
methods
of
of_bools
of_complex_doubles
of_floats
of_ints
of_strings
of_tensor
of_tensors
refine
sizes
str_
strides
symbolic_sizes
try_eval_type_variables
type_kind_to_string
type_verbosity
unify_type_list
unify_types
unify_types_impl
unsafe_change_attribute_type
unsafe_remove_attribute
unsafe_remove_constant
unsafe_remove_method
apply_bag_size
apply_bag_size_backward
compute_counts
compute_counts_uniq
embedding_bag_a
embedding_bag_b
embedding_bag_backward
embedding_bag_cpu
embedding_bag_cpu_impl
embedding_bag_cpu_impl_out_b
embedding_bag_cpu_max_out
embedding_bag_dense_backward_cpu
embedding_bag_dense_backward_cpu_max
embedding_bag_dense_backward_cpu_sum_mean
embedding_bag_forward_only_cpu
embedding_bag_per_sample_weights_backward_cpu
embedding_bag_per_sample_weights_backward_cpu_template
embedding_bag_sparse_backward
index_select_add
index_select_add_float
index_select_scale_add
index_select_scale_add_float
is_fast_path
is_fast_path_index_select
is_fast_path_index_select_scale
make_bag_size
make_bag_size_out
make_max_indices
make_max_indices_out
make_offset2bag_a
make_offset2bag_b
make_offset2bag_out
promote_indices_and_offsets
pytorch_qnnp_requantize_fp32_neon
tensor_impl_test_caffe_2constructor
PoolingParams1D
declare_dispatch
define_dispatch
index
lazy_static
max_pool1d
max_pool1d_impl
conjugate_fallback
lazy_static
define_dispatch
qnnpack_tanh
tanh_quantized_cpu
col2vol
vol2col
PyTorchQnnpOperator
PyTorchQnnpStatus
pytorch_qnnp_create_add_nc_q8
pytorch_qnnp_create_average_pooling2d_nhwc_q8
pytorch_qnnp_create_channel_shuffle_nc_x8
pytorch_qnnp_create_clamp_nc_u8
pytorch_qnnp_create_convolution2d_nhwc_q8
pytorch_qnnp_create_deconvolution2d_nhwc_q8
pytorch_qnnp_create_fully_connected_nc_q8
pytorch_qnnp_create_fully_connected_sparse_dq_nc_q8
pytorch_qnnp_create_global_average_pooling_nwc_q8
pytorch_qnnp_create_hardsigmoid_nc_q8
pytorch_qnnp_create_hardswish_nc_q8
pytorch_qnnp_create_leaky_relu_nc_q8
pytorch_qnnp_create_max_pooling2d_nhwc_u8
pytorch_qnnp_create_sigmoid_nc_q8
pytorch_qnnp_create_tanh_nc_q8
pytorch_qnnp_deinitialize
pytorch_qnnp_delete_operator
pytorch_qnnp_initialize
pytorch_qnnp_run_operator
pytorch_qnnp_setup_add_nc_q8
pytorch_qnnp_setup_average_pooling2d_nhwc_q8
pytorch_qnnp_setup_channel_shuffle_nc_x8
pytorch_qnnp_setup_clamp_nc_u8
pytorch_qnnp_setup_convolution2d_nhwc_q8
pytorch_qnnp_setup_deconvolution2d_nhwc_q8
pytorch_qnnp_setup_fully_connected_nc_q8
pytorch_qnnp_setup_fully_connected_sparse_dq_nc_q8
pytorch_qnnp_setup_global_average_pooling_nwc_q8
pytorch_qnnp_setup_hardsigmoid_nc_q8
pytorch_qnnp_setup_hardswish_nc_q8
pytorch_qnnp_setup_leaky_relu_nc_q8
pytorch_qnnp_setup_max_pooling2d_nhwc_u8
pytorch_qnnp_setup_sigmoid_nc_q8
pytorch_qnnp_setup_tanh_nc_q8
th_copy_ignoring_overlaps
th_cross_kernel
th_cross_kernel_out
th_gels
th_gels_out
th_masked_fill
th_masked_fill_bool
th_potri
th_potri_out
thnn_conv2d_backward
thnn_conv2d_backward_out
thnn_conv2d_forward
thnn_conv2d_forward_out
thnn_conv_depthwise2d_backward
thnn_conv_depthwise2d_backward_out
thnn_conv_depthwise2d_forward
thnn_conv_depthwise2d_forward_out
thnn_glu_backward
thnn_glu_backward_out
thnn_glu_forward
thnn_glu_forward_out
thnn_log_sigmoid_backward
thnn_log_sigmoid_backward_out
thnn_log_sigmoid_forward
thnn_log_sigmoid_forward_out
thnn_nll_loss2d_backward
thnn_nll_loss2d_backward_out
thnn_nll_loss2d_forward
thnn_nll_loss2d_forward_out
thnn_nll_loss_backward
thnn_nll_loss_backward_out
thnn_nll_loss_forward
thnn_nll_loss_forward_out
thnn_rrelu_with_noise_backward
Ctype
alias_specialization
alpha_dropout_a
alpha_dropout_b
dropout_a
dropout_b
dropout_impl
feature_alpha_dropout_a
feature_alpha_dropout_b
feature_dropout_a
feature_dropout_b
is_fused_kernel_acceptable
make_feature_noise
multiply_a
multiply_b
THCCudaResourcesPerDevice
THCState
is
th_cublas_check
th_cuda_check
th_cuda_check_warn
th_cuda_free
th_cuda_host_alloc
th_cuda_host_record
th_cuda_init
th_cuda_malloc
th_cuda_shutdown
th_cusparse_check
thc_state_alloc
thc_state_free
thc_state_get_cuda_host_allocator
thc_state_get_current_device_scratch_space_size
thc_state_get_device_resource_ptr
thc_state_get_peer_to_peer_access
do_trapz_a
do_trapz_b
trapz_a
trapz_b
zeros_like_except
gemmlowp_scalar_rdivbypo2_s32
gemmlowp_scalar_vqrdmulh_s32
pytorch_qnnp_requantize_gemmlowp_scalar
Addmm
Conv2d
Hardtanh_
Mean
MobileNetV2
OpType
OpsList
almost_equal
check_rtol
default
exactly_equal
run
show_rtol
to_string
vulkan_api_test_adaptive_avg_pool2d
vulkan_api_test_add
vulkan_api_test_add_broadcast0
vulkan_api_test_add_broadcast1
vulkan_api_test_add_broadcast2
vulkan_api_test_add_scalar
vulkan_api_test_addmm
vulkan_api_test_addmm_expand
vulkan_api_test_avg_pool2d
vulkan_api_test_clamp
vulkan_api_test_conv2d
vulkan_api_test_conv2d_dw
vulkan_api_test_conv2d_pw
vulkan_api_test_conv2d_winograd
vulkan_api_test_copy_
vulkan_api_test_div
vulkan_api_test_div_broadcast0
vulkan_api_test_div_broadcast1
vulkan_api_test_div_broadcast2
vulkan_api_test_div_scalar
vulkan_api_test_empty
vulkan_api_test_hardsigmoid
vulkan_api_test_hardswish
vulkan_api_test_max_pool2d
vulkan_api_test_mean
vulkan_api_test_mean2d
vulkan_api_test_mm
vulkan_api_test_mobilenetv2
vulkan_api_test_mul
vulkan_api_test_mul_broadcast0
vulkan_api_test_mul_broadcast1
vulkan_api_test_mul_broadcast2
vulkan_api_test_mul_scalar
vulkan_api_test_reflection_pad2d
vulkan_api_test_reshape
vulkan_api_test_sigmoid
vulkan_api_test_sub
vulkan_api_test_sub_broadcast0
vulkan_api_test_sub_broadcast1
vulkan_api_test_sub_broadcast2
vulkan_api_test_upsample_nearest2d
quantized_copy_from_float_cpu
QLinearDynamicInt8
apply_dynamic
apply_dynamic_impl_false
apply_dynamic_impl_true
apply_dynamic_relu
lazy_static
register_linear_params
run
is_custom_op
average_pooling_q8
lazy_static
shuffle_netv1g1
shuffle_netv1g2
shuffle_netv1g3
shuffle_netv1g4
shuffle_netv1g8
Persistent
persistent
pytorch_sgemm_ukernel_5x8_neon
test_parallel
test_parallel_exceptions
test_parallel_intra_op_launch_future
test_parallel_nested
cpu_generator_impl_test_cloning
cpu_generator_impl_test_default
cpu_generator_impl_test_dynamic_cast
cpu_generator_impl_test_get_set_current_seed
cpu_generator_impl_test_mt19937engine_reproducibility
cpu_generator_impl_test_multithreading_get_engine_operator
cpu_generator_impl_test_multithreading_get_set_current_seed
cpu_generator_impl_test_philox_engine_index
cpu_generator_impl_test_philox_engine_offset1
cpu_generator_impl_test_philox_engine_offset2
cpu_generator_impl_test_philox_engine_offset3
cpu_generator_impl_test_philox_engine_reproducibility
cpu_generator_impl_test_rng_forking
thread_func_get_engine_op
thread_func_get_set_current_seed
GatherFn
IndexCopyFn
IndexFillFn
IndexFn
IndexPutFn
IndexPutWithSortFn
Long
MaskedFillFn
MaskedScatterFn
MaskedSelectFn
PutFn
ScatterAddFn
ScatterFillFn
ScatterFn
ScatterGatherOp
ScatterReduceFn
ScatterScalarReduceFn
TakeFn
all_strides_match
check_device_a
check_device_b
check_indexarray_range
count_nonzero
count_nonzero_cpu
count_nonzero_cuda
count_nonzero_impl
declare_dispatch
define_dispatch
gather
gather_backward
gather_out_cpu_cuda
gather_sparse_backward
get_operator_enum
index
index_add_a
index_add_b
index_add_c
index_add_cpu
index_copy_a
index_copy_b
index_copy_impl
index_fill_a
index_fill_b
index_fill_c
index_fill_d
index_out
index_put_a
index_put_b
index_put_impl
index_select_backward
index_select_cpu
index_select_out_cpu
index_select_out_cpu_dim1
is
lazy_static
make_index_iterator
make_index_out_iterator
make_index_put_iterator
make_info
masked_fill_a
masked_fill_b
masked_fill_cpu_a
masked_fill_cpu_b
masked_fill_impl_cpu
masked_scatter
masked_scatter_cpu
masked_select_backward
masked_select_cpu
masked_select_out_cpu
masked_select_out_impl_cpu
nonzero_cpu
nonzero_numpy
nonzero_out_cpu
of
put_a
put_b
quantized_index
register_no_cpu_dispatch
reshape_indexer
restride_src
scatter_impl
scatter_meta_impl
shapes_as_str
take
take_along_dim
take_along_dim_helper
take_along_dim_out
take_out
lazy_static
declare_pytorch_x8lut_ukernel_function
channel_shuffle
test_ambiguous_defaults_a
test_ambiguous_defaults_b
test_optional_floatlist
test_optional_intlist
test_string_default
BernoulliKernel
CauchyKernel
ExponentialKernel
GeometricKernel
LogNormalKernel
NormalKernel
RandomFromToKernel
RandomKernel
UniformKernel
bernoulli_kernel
bernoulli_kernel_with_tensor
bernoulli_tensor_cuda_kernel
calc_execution_policy
cauchy_kernel
distribution_binary_elementwise_kernel
distribution_binary_kernel
distribution_elementwise_grid_stride_kernel
distribution_nullary_kernel
exponential_kernel
geometric_kernel
invoke
log_normal_kernel
normal_and_transform
normal_kernel
random_from_to_kernel
random_full_64_bits_range_kernel
random_kernel
uniform_and_transform
uniform_kernel
dict_test_copy_has_separate_storage
dict_test_equality
dict_test_given_const_when_calling_find_on_existing_key_then_finds_correct_element
dict_test_given_const_when_calling_find_on_non_existing_key_then_returns_end
dict_test_given_const_when_iterating_then_finds_elements
dict_test_given_const_when_iterating_with_foreach_then_finds_elements
dict_test_given_different_iterators_then_are_not_equal
dict_test_given_empty_when_calling_size_then_returns_zero
dict_test_given_empty_when_calling_then_returns_true
dict_test_given_empty_when_iterating_then_begin_is_end
dict_test_given_equal_iterators_then_are
dict_test_given_iterator_then_can_modify_value
dict_test_given_iterator_when_dereferencing_then_points_to_correct_element
dict_test_given_iterator_when_postfix_incrementing_then_moves_to_next_and_returns_old_position
dict_test_given_iterator_when_writing_to_value_then_changes
dict_test_given_mutable_when_calling_find_on_existing_key_then_finds_correct_element
dict_test_given_mutable_when_calling_find_on_non_existing_key_then_returns_end
dict_test_given_mutable_when_iterating_then_finds_elements
dict_test_given_mutable_when_iterating_with_foreach_then_finds_elements
dict_test_given_nonempty_when_calling_clear_then_is_empty
dict_test_given_nonempty_when_calling_empty_then_returns_false
dict_test_given_nonempty_when_calling_size_then_returns_number_of_elements
dict_test_given_one_element_when_erasing_by_iterator_then_is_empty
dict_test_given_one_element_when_erasing_by_key_then_returns_and_is_empty
dict_test_given_one_element_when_erasing_by_nonexisting_key_then_returns_zero_and_is_unchanged
dict_test_is_reference_type
dict_test_tensor_as_key
dict_test_when_calling_at_with_existing_key_then_returns_correct_element
dict_test_when_calling_at_with_non_existing_key_then_returns_correct_element
dict_test_when_calling_contains_with_existing_key_then_returns_true
dict_test_when_calling_contains_with_non_existing_key_then_returns_false
dict_test_when_calling_reserve_then_doesnt_crash
dict_test_when_copy_assigning_then_are_equal
dict_test_when_copy_constructing_then_are_equal
dict_test_when_copying_then_are_equal
dict_test_when_insert_or_assigning_existing_key_then_does_modify
dict_test_when_insert_or_assigning_existing_key_then_returns_false_and_iterator_to_changed_element
dict_test_when_inserting_existing_key_then_does_not_modify
dict_test_when_inserting_existing_key_then_returns_false_and_iterator_to_element
dict_test_when_move_assigning_then_old_is_empty
dict_test_when_move_constructing_then_old_is_empty
list_test_i_value_based_given_iterator_when_writing_to_from_then_changes
cadd
register_dispatch
unfolded2d_acc
unfolded2d_acc_kernel
unfolded2d_copy
unfolded2d_copy_kernel
VulkanGuardImpl
block
destroy_event
device_count
exchange_device
exchange_stream
get_device
get_stream
lazy_static
query_event
record
set_device
ty
unchecked_set_device
pytorch_qnnp_requantize_q31_scalar
test_adaptive_avg_pool2d
test_add
test_add_broadcast
test_add_broadcast2
test_addmm
test_cat_dim0
test_cat_dim0_nonarray
test_cat_dim1_0
test_cat_dim1_1
test_cat_dim1_nonarray_0
test_cat_dim1_nonarray_1
test_chunk
test_chunk2
test_chunk3
test_conv2d
test_copy_nchw_to_metal
test_depthwise_conv
test_div
test_div_broadcast
test_div_broadcast2
test_hardsigmoid
test_hardswish
test_hardtanh
test_log_softmax
test_max_pool2d
test_max_pool2d_ceil
test_max_pool2d_padding
test_mean_dim
test_mean_dim2
test_mean_dim3
test_mul
test_mul_broadcast
test_mul_broadcast2
test_nchw_to_nc4_cpu
test_reflection_pad2d
test_relu
test_reshape
test_sigmoid
test_softmax
test_sub
test_sub_broadcast
test_sub_broadcast2
test_synchronization
test_t
test_transpose
test_transpose2
test_transpose3
test_upsampling_nearest2d_vec
test_view
test_view2
test_view3
test_view4
allocator
infer_scalar_type
infer_scalar_type_with_list
options
th_histc
th_histc_out
copy_weights_to_flat_buf_views
pytorch_qnnp_create_hardsigmoid_nc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_hardsigmoid_nc_q8
pytorch_qnnp_status
check_tensor_memory_format
dequantize_per_channel_affine_kernel
dequantize_tensor_per_channel_affine_cpu
dequantize_tensor_per_channel_float_qparams_cpu
dequantize_tensor_per_tensor_affine_cpu
dequantize_tensor_per_tensor_affine_sub_byte_cpu
dispatch
do_avg_pool_nhwc_on_avx2
do_avg_pool_on_avx2
do_bn_compute
do_quantized_bilinear_on_avx2
fake_quant_per_channel_cachemask_cpu
fake_quantize_learnable_channel_grad_kernel_cpu
fake_quantize_learnable_tensor_grad_kernel_cpu
fake_quantize_tensor_cachemask_kernel
hsum
hsum_i32
hsum_i8
hsum_sq
hsum_sq_i32
hsum_sq_i8
hsum_sq_u8
hsum_u8
leaky_qrelu_out_kernel
q_batch_norm_kernel
qadaptive_avg_pool2d_nhwc_kernel
qadaptive_avg_pool3d_ndhwc_kernel
qadaptive_avg_pool_kernel
qadd_kernel
qadd_scalar_kernel
qavg_pool2d_nhwc_kernel
qavg_pool3d_nhwc_kernel
qavg_pool_nhwc_kernel
qcat_nhwc_kernel
qclamp_kernel
qclamp_max_kernel
qclamp_min_kernel
qelu_kernel
qhardsigmoid_kernel
qhardswish_kernel
qmaxpool_2d_nhwc_kernel
qmul_kernel
qrelu6_kernel
qrelu_kernel
qsigmoid_kernel
qtanh_kernel
qthreshold_kernel
qtopk_kernel
quantize_tensor_arm
quantize_tensor_arm_quint8
quantize_tensor_per_channel_affine_cpu
quantize_tensor_per_channel_float_qparams_cpu
quantize_tensor_per_channel_impl
quantize_tensor_per_channel_impl_quint8
quantize_tensor_per_tensor_affine_cpu
quantize_tensor_per_tensor_affine_sub_byte_cpu
quantized_normalize_kernel
qupsample_bilinear2d_nhwc_kernel
register_dispatch
divide_round_up
doz
max
min
round_up
Runtime
RuntimeDebug
RuntimeSelector
RuntimeType
acquire_physical_devices
create_debug_report_callback
create_instance
debug_report_callback_fn
instance
invoke
query_compute_queue_family_index
query_physical_device_memory_properties
query_physical_device_properties
runtime
select
CellParams
CellParamsSerializationType
FullBidirectionalLayer
FullLayer
GRUCell
LSTMCell
Layer
LayerOutput
LstmFn
LstmPackedFn
PackedBidirectionalLayer
PackedLayer
PackedSequence
QRNNCellParamsWrapper
QuantizedCellParams
QuantizedCellParamsDynamic
QuantizedCellParamsFP16
QuantizedGruCellDynamicType
QuantizedGruCellType
QuantizedLstmCellDynamicType
QuantizedLstmCellType
QuantizedLstmReturnType
QuantizedRnnReluCellDynamicType
QuantizedRnnReluCellType
QuantizedRnnTanhCellDynamicType
QuantizedRnnTanhCellType
ReluF
ReulCellType
ReversedPackedLayer
RnnFn
RnnPackedFn
SimpleCell
SimpleHxType
TanfCellType
TanhF
apply_layer_stack
b_hh
b_ih
check_attributes
check_rnn_cell_forward_hidden
declare_dispatch
define_dispatch
define_quantized_rnn_cell
define_quantized_rnn_cell_dynamic
dropout_a
dropout_b
for
gather_params
gather_quantized_params
gather_quantized_params_dynamic
gather_quantized_params_fp16
getstate
gru_cell
hidden_concat_a
hidden_concat_b
hidden_slice_a
hidden_slice_b
in
invoke
invoke_cell
lazy_static
linear_hh
linear_ih
lstm_a
lstm_b
lstm_cell
lstm_impl
make_quantized_cell_params
make_quantized_cell_params_dynamic
make_quantized_cell_params_fp16
matmul_hh
matmul_hr
matmul_ih
of
one_hidden_rnn
only
pair_vec
prepare_quantized_hx
prepare_quantized_lstm_hx
project
quantized_gru_data_legacy
quantized_lstm_data
quantized_lstm_data_legacy
register_linear_params
register_no_cpu_dispatch
rnn_impl
rnn_impl_with_concat
rnn_relu_cell
rnn_tanh_cell
setstate
thnn_differentiable_gru_cell_backward
thnn_differentiable_lstm_cell_backward
unpair_vec
use_cudnn_rnn_flatten_weight
use_miopen
clear_computation_cache
lazy_static
Operation
Stack
TuplePacker
drop_a
drop_b
execute
last_a
last_b
lazy_static
pack_a
pack_b
pack_c
peek_a
peek_b
peek_c
peek_d
peek_slice
pop
pop_n
pop_variadic
push_a
push_b
push_list_elements
push_one_a
push_one_b
hardswish
hardswish_impl
hardswish_mut
use_hardswish
pytorch_qnnp_requantize_q31_neon
fp32_neon_random_cases
fp32_psimd_random_cases
fp32_scalar_lrintf_random_cases
fp32_scalar_magic_random_cases
fp32_sse2_random_cases
gemmlowp_neon_random_cases
gemmlowp_scalar_random_cases
gemmlowp_sse2_divide_by_po2_with_rounding_away
gemmlowp_sse2_divide_by_po2_with_rounding_up
gemmlowp_sse2_exact_divide_by_po2
gemmlowp_sse2_exact_divide_by_po2_with_zero_point
gemmlowp_sse2_random_cases
gemmlowp_sse2_special_cases
gemmlowp_sse4_divide_by_po2_with_rounding_away
gemmlowp_sse4_divide_by_po2_with_rounding_up
gemmlowp_sse4_exact_divide_by_po2
gemmlowp_sse4_exact_divide_by_po2_with_zero_point
gemmlowp_sse4_random_cases
gemmlowp_sse4_special_cases
gemmlowp_ssse3_divide_by_po2_with_rounding_away
gemmlowp_ssse3_divide_by_po2_with_rounding_up
gemmlowp_ssse3_exact_divide_by_po2
gemmlowp_ssse3_exact_divide_by_po2_with_zero_point
gemmlowp_ssse3_random_cases
gemmlowp_ssse3_special_cases
precise_neon_divide_by_po2_with_rounding_away
precise_neon_divide_by_po2_with_rounding_down
precise_neon_divide_by_po2_with_rounding_up
precise_neon_exact_divide_by_po2
precise_neon_exact_divide_by_po2_with_zero_point
precise_neon_random_cases
precise_neon_special_cases
precise_psimd_divide_by_po2_with_rounding_away
precise_psimd_divide_by_po2_with_rounding_down
precise_psimd_divide_by_po2_with_rounding_up
precise_psimd_exact_divide_by_po2
precise_psimd_exact_divide_by_po2_with_zero_point
precise_psimd_random_cases
precise_psimd_special_cases
precise_scalar_signed64_divide_by_po2_with_rounding_away
precise_scalar_signed64_divide_by_po2_with_rounding_down
precise_scalar_signed64_divide_by_po2_with_rounding_up
precise_scalar_signed64_exact_divide_by_po2
precise_scalar_signed64_exact_divide_by_po2_with_zero_point
precise_scalar_signed64_random_cases
precise_scalar_signed64_special_cases
precise_scalar_unsigned32_divide_by_po2_with_rounding_away
precise_scalar_unsigned32_divide_by_po2_with_rounding_down
precise_scalar_unsigned32_divide_by_po2_with_rounding_up
precise_scalar_unsigned32_exact_divide_by_po2
precise_scalar_unsigned32_exact_divide_by_po2_with_zero_point
precise_scalar_unsigned32_random_cases
precise_scalar_unsigned32_special_cases
precise_scalar_unsigned64_divide_by_po2_with_rounding_away
precise_scalar_unsigned64_divide_by_po2_with_rounding_down
precise_scalar_unsigned64_divide_by_po2_with_rounding_up
precise_scalar_unsigned64_exact_divide_by_po2
precise_scalar_unsigned64_exact_divide_by_po2_with_zero_point
precise_scalar_unsigned64_random_cases
precise_scalar_unsigned64_special_cases
precise_sse2_divide_by_po2_with_rounding_away
precise_sse2_divide_by_po2_with_rounding_down
precise_sse2_divide_by_po2_with_rounding_up
precise_sse2_exact_divide_by_po2
precise_sse2_exact_divide_by_po2_with_zero_point
precise_sse2_random_cases
precise_sse2_special_cases
precise_sse4_divide_by_po2_with_rounding_away
precise_sse4_divide_by_po2_with_rounding_down
precise_sse4_divide_by_po2_with_rounding_up
precise_sse4_exact_divide_by_po2
precise_sse4_exact_divide_by_po2_with_zero_point
precise_sse4_random_cases
precise_sse4_special_cases
precise_ssse3_divide_by_po2_with_rounding_away
precise_ssse3_divide_by_po2_with_rounding_down
precise_ssse3_divide_by_po2_with_rounding_up
precise_ssse3_exact_divide_by_po2
precise_ssse3_exact_divide_by_po2_with_zero_point
precise_ssse3_random_cases
precise_ssse3_special_cases
q31_neon_divide_by_po2_with_rounding_away
q31_neon_divide_by_po2_with_rounding_up
q31_neon_exact_divide_by_po2
q31_neon_exact_divide_by_po2_with_zero_point
q31_neon_random_cases
q31_neon_random_match_gemmlowp
q31_neon_special_cases
q31_scalar_divide_by_po2_with_rounding_away
q31_scalar_divide_by_po2_with_rounding_up
q31_scalar_exact_divide_by_po2
q31_scalar_exact_divide_by_po2_with_zero_point
q31_scalar_random_cases
q31_scalar_random_match_gemmlowp
q31_scalar_special_cases
q31_sse2_divide_by_po2_with_rounding_away
q31_sse2_divide_by_po2_with_rounding_up
q31_sse2_exact_divide_by_po2
q31_sse2_exact_divide_by_po2_with_zero_point
q31_sse2_random_cases
q31_sse2_random_match_gemmlowp
q31_sse2_special_cases
q31_sse4_divide_by_po2_with_rounding_away
q31_sse4_divide_by_po2_with_rounding_up
q31_sse4_exact_divide_by_po2
q31_sse4_exact_divide_by_po2_with_zero_point
q31_sse4_random_cases
q31_sse4_random_match_gemmlowp
q31_sse4_special_cases
q31_ssse3_divide_by_po2_with_rounding_away
q31_ssse3_divide_by_po2_with_rounding_up
q31_ssse3_exact_divide_by_po2
q31_ssse3_exact_divide_by_po2_with_zero_point
q31_ssse3_random_cases
q31_ssse3_random_match_gemmlowp
q31_ssse3_special_cases
declare_pytorch_hgemm_ukernel_function
pytorch_qnnp_fp16_clamping_params
lazy_static
memory_format_test_set
memory_format_test_slice_first
memory_format_test_slice_step_two
memory_format_test_transpose
slice_first
slice_step_two
pytorch_sdwconv_ukernel_up4x9_psimd
InferExpandGeometryResult
are_expandable
check_defined
expand_inplace
expand_inplace2
expand_inplace2_with_api_name
expand_inplace_with_api_name
expand_outplace
expand_outplace2
expand_outplace3
expand_outplace3_with_api_name
expand_outplace_with_api_name
expand_size
expand_size_with_api_name
infer_dense_strides
infer_expand_geometry
infer_expand_geometry_dimvector
infer_expand_geometry_impl
infer_size
infer_size_dimvector
infer_size_impl
instead
is_expandable_to
sum_to
IDeepTensorWrapper
IDeepTensorWrapperPtr
MKLDNNTensor
MKLDNNTensorImpl
get_mkldnn_dtype
itensor_from_mkldnn
itensor_from_tensor
itensor_view_from_dense
VulkanOpaqueTensorImpl
is_contiguous_custom
stride
strides
tensorimpl_type_name
pytorch_qnnp_x8zip_x3_neon
in
int
lazy_static
concat_kernel
decrement_kernel
error_kernel
expect_calls_concat_unboxed
expect_calls_decrement
expect_calls_increment
increment_kernel
kernel_for_schema_inference
kernel_func
kernel_with_list_of_map
kernel_with_list_of_map_of_int_list
kernel_with_map_of_int_list
kernel_with_map_of_list_of_map
lazy_static
operator_registration_test_legacy_function_based_kernel_given_mismatched_with_different_num_returns_when_registering_then_fails
operator_registration_test_legacy_function_based_kernel_given_mismatched_with_different_return_types_when_registering_then_fails
operator_registration_test_legacy_function_based_kernel_given_multiple_operators_and_kernels_when_registered_in_one_registrar_then_calls_right
operator_registration_test_legacy_function_based_kernel_given_multiple_operators_and_kernels_when_registered_in_registrars_then_calls_right
operator_registration_test_legacy_function_based_kernel_given_when_registered_in_constructor_then_can_be_called
operator_registration_test_legacy_function_based_kernel_given_when_registered_then_can_be_called
operator_registration_test_legacy_function_based_kernel_given_when_registered_then_can_be_called_unboxed
operator_registration_test_legacy_function_based_kernel_given_when_registered_without_specifying_schema_then_infers
operator_registration_test_legacy_function_based_kernel_given_when_runs_out_of_scope_then_cannot_be_called_anymore
operator_registration_test_legacy_function_based_kernel_given_with_list_of_map_int_when_registered_then_can_be_called
operator_registration_test_legacy_function_based_kernel_given_with_list_of_map_when_registered_then_can_be_called
operator_registration_test_legacy_function_based_kernel_given_with_map_of_list_when_registered_then_can_be_called
generate_sizes
lazy_static
stateful_conv1d
DOUBLE2
FLOAT2
Philox4_32_10
PhiloxEngine
UINT2
UINT4
incr
incr_n
invoke
mulhilo32
single_round
lazy_static
tanh_q8
memory_clean_up_no_error_without_release
memory_clean_up_unpack_error
define_dispatch
upsample_bilinear2d_out_frame
upsample_bilinear2d_quantized_cpu
upsample_bilinear2d_quantized_cpu_with_scales
QMul
QMulOut
QMulScalar
QMulScalar2
QMulScalarOut
QMulScalarTensor
QMulScalarTensorOut
define_dispatch
lazy_static
mul_out
mul_scalar_out
run
BroadcastingVmapTransform
MultiBatchVmapTransform
VmapDimVector
VmapPhysicalToLogicalMap
VmapPhysicalView
VmapPhysicalViewVec
align_batch_dims_at_front
apply
apply_inplace
are_bdims_at_front_in_order
compute_front_batch_dims_from_levels
get_physical_dim
get_physical_dims
get_physical_shape
get_physical_tensor_and_levels
get_physical_to_logical_map
logical_to_physical
num_batch_dims
num_logical_dims
of
permute_batch_dims_to_front
tensor
used
HIPHooksArgs
c10_declare_registry
c10_define_registry
current_device
get_hip_hooks
get_num_gpu_s
get_pinned_memory_allocator
has_hip
init_hip
init_hip_generator
register_hip_types
SoftArgMaxOperatorTester
batch_size
channels
iterations
testq8
pytorch_u8rmax_ukernel_sse2
Mt19937
Mt19937DataPod
Mt19937Engine
data
init_with_uint32
invoke
is_valid
mix_bits
next_state
seed
set_data
twist
check_device
unsafe_storage_fromth
unsafe_tensor_fromth
VGG
convolution_q8
dw_conv3x3
dw_conv3x3d2
dw_conv5x5
lazy_static
mobile_netv1
mobile_netv2
res_net18
res_net50
shuffle_netv1g1
shuffle_netv1g2
shuffle_netv1g3
shuffle_netv1g4
shuffle_netv1g8
shuffle_netv2x05
shuffle_netv2x10
shuffle_netv2x15
shuffle_netv2x20
squeeze_netv10
squeeze_netv11
DimCounter
get_data_ptrs
increment
is_done
max_2d_step
serial_for_each
almost_equal
check_rtol
exactly_equal
test_global_average_pool
test_hardswish
test_xnn_pack_ops_global
test_xnn_pack_ops_hard_swish
decrement_kernel
error_kernel
expect_calls_decrement
expect_calls_increment
expect_calls_increment_unboxed
expect_calls_increment_with_key
increment_kernel
kernel_for_schema_inference
lazy_static
operator_registration_test_stack_based_kernel_call_kernels_with_dispatch_key_set_convention_redispatches_to_lower_priority
operator_registration_test_stack_based_kernel_given_multiple_operators_and_kernels_when_registered_in_one_registrar_then_calls_right
operator_registration_test_stack_based_kernel_given_multiple_operators_and_kernels_when_registered_in_registrars_then_calls_right
operator_registration_test_stack_based_kernel_given_when_registered_then_can_also_be_called_unboxed
operator_registration_test_stack_based_kernel_given_when_registered_then_can_be_called
operator_registration_test_stack_based_kernel_given_when_registered_without_specifying_schema_then_fails_because_it_cannot_infer_from
operator_registration_test_stack_based_kernel_given_when_runs_out_of_scope_then_cannot_be_called_anymore
redispatching_kernel_with_dispatch_key_set
MAGMAQueue
MagmaStreamSyncGuard
T
cuda_int_cast
default
drop
get_queue
magma_int_cast
pin_memory
mkldnn_zero
ZipMicrokernelTester
g
iterations
n
test
declare_dispatch
define_dispatch
layer_norm
layer_norm_backward_cpu
layer_norm_cpu
layer_norm_cpu_out_a
layer_norm_cpu_out_b
lazy_static
math_native_layer_norm
Conv2DParams
available
conv2d_clamp_run
create
create_conv2d_clamp_pre_pack_op_context
run
CUDAHooks
batchnorm_min_epsilon_cu_dnn
compiled_with_cu_dnn
compiled_with_mio_pen
cu_fft_clear_plan_cache
cu_fft_get_plan_cache_max_size
cu_fft_get_plan_cache_size
cu_fft_set_plan_cache_max_size
current_device
device_synchronize
get_cuda_device_allocator
get_default_cuda_generator
get_devce_index_with_primary_context
get_device_from_ptr
get_num_gpu_s
get_pinned_memory_allocator
has_cu_dnn
has_primary_context
hascuda
hascudart
hasmagma
initcuda
is_pinned_ptr
lazy_static
load_nvrtc
nvrtc
register_cuda_hooks
show_config
supports_depthwise_convolution_with_cu_dnn
supports_dilated_convolution_with_cu_dnn
version_cu_dnn
versioncudart
affine_grid_generator
affine_grid_generator_4d
affine_grid_generator_4d_backward
affine_grid_generator_5d
affine_grid_generator_5d_backward
affine_grid_generator_backward
linspace_from_neg_one
make_base_grid_4d
make_base_grid_5d
reduce_ops_test_max_values_and_min
SparseCsrTensor
get_sparse_csr_impl
cpu_allocation_plan_test_with_control_flow
cpu_allocation_plan_test_with_profiling_alloc
main
run_with_control_flow
CUFFT_CHECK
cuda_get_error_enum
InferUnsqueezeGeometryResult
alias
alias_with_sizes_and_strides
apply_diag
as_strided
as_strided_qtensorimpl
as_strided_tensorimpl
block_diag
broadcast_tensors
broadcast_to
can_use_native_serial_stack
cat_a
cat_b
cat_cpu
cat_out_a
cat_out_b
cat_out_cpu
cat_sparse
check_cat_no_zero_dim
check_cat_shape_except_dim
check_cat_sparse_dims
check_t
chunk
column_stack
column_stack_out
define_dispatch
detach
diag
diag_backward
diag_cpu_out
diag_embed
diagflat
diagonal_a
diagonal_b
diagonal_backward
dsplit_a
dsplit_b
dstack
dstack_out
expand
expand_as
flatten_a
flatten_b
flatten_c
flatten_d
flatten_dense_tensors
handle_unflatten_exception
hsplit_a
hsplit_b
hstack
hstack_out
index_select_sparse
infer_squeeze_geometry_a
infer_squeeze_geometry_b
infer_unsqueeze_geometry
instead
make_qtensor
maybe_native_stack
meshgrid
moveaxis_a
moveaxis_b
movedim_a
movedim_b
narrow_a
narrow_b
narrow_copy_dense
narrow_copy_dense_cpu
narrow_copy_dense_cpu_out
narrow_copy_sparse
numel
numpy_t
of
permute
promotion
propagate_transposed_names
ravel
repeat
reshape
reshape_as
reshape_from_tensor
row_stack
row_stack_out
select_a
select_b
select_backward
select_sparse
set
set_cpu
set_storage_cpu
set_tensor
shape_as_tensor
should_skip
sizes_match_except
slice
slice_backward
sparse_transpose
split
split_with_sizes
squeeze_a
squeeze_b
squeeze_c
squeeze_d
squeeze_qtensor_a
squeeze_qtensor_b
stack_a
stack_b
stack_cpu
stack_out_a
stack_out_b
stack_out_cpu
sum_to_size
swapaxes_a
swapaxes_b
swapdims_a
swapdims_b
t_a
t_b
tensor_split_a
tensor_split_b
tensor_split_c
tile
transpose_a
transpose_b
transpose_c
unbind_a
unbind_b
unflatten_a
unflatten_b
unflatten_dense_tensors
unfold
unsafe_chunk
unsafe_split
unsafe_split_with_sizes
unsafe_view
unsqueeze_a
unsqueeze_b
unsqueeze_qtensor
unsqueeze_sparse
view
view_as
vsplit_a
vsplit_b
vstack
vstack_out
Long
as
topk_impl_loop
fractional_max_pool2d_backward_cpu
fractional_max_pool2d_backward_out_cpu
fractional_max_pool2d_backward_out_cpu_template
fractional_max_pool2d_backward_out_frame
fractional_max_pool2d_backward_out_single_batch_frame
fractional_max_pool2d_generate_intervals
fractional_max_pool2d_out_frame
fractional_max_pool2d_out_single_batch_frame
lazy_static
WrapFunctionIntoFunctor
lazy_static
QLinearPackWeightInt8
calc_col_offsets_transpose
lazy_static
prepack
register_linear_params
run
lazy_static
maximum
minimum
GemmMicrokernelTester
a_stride
a_zero_point
b_zero_point
biasn
c_stride
iterations
k
kr
ks
m
mr
multiplier
n
np
nr
packedk
packedn
pytorch_qnnp_conv_dynamic_quantization_params
pytorch_qnnp_fp16_clamping_params
pytorch_qnnp_fp32_clamping_params
q8gemm_compute_row_sum
qmax
qmin
test
NnapiCompilation
default
get_operand_type
init
lazy_static
load_platform_library
make_smart_ptr
run
type
compute_cpu
repeat_interleave_a
repeat_interleave_b
repeat_interleave_common
repeat_interleave_cpu
fractional_max_pool3d_backward_cpu
fractional_max_pool3d_backward_out_cpu
fractional_max_pool3d_backward_out_cpu_template
fractional_max_pool3d_backward_out_frame
fractional_max_pool3d_backward_out_single_batch_frame
fractional_max_pool3d_cpu
fractional_max_pool3d_out_cpu
fractional_max_pool3d_out_cpu_template
fractional_max_pool3d_out_frame
fractional_max_pool3d_out_single_batch_frame
generate_intervals
ConvolutionOperatorTester
batch_size
create_convolution_op
dilated_kernel_height
dilated_kernel_width
dilation
dilation_height
dilation_width
groups
iterations
kernel_height
kernel_size
kernel_width
padding
padding_bottom
padding_height
padding_left
padding_right
padding_top
padding_width
per_channel
pytorch_qnnp_operator
qmax
qmin
subsampling
subsampling_height
subsampling_width
testq8
pytorch_q8avgpool_ukernel_mp8x9p8q_neon
NameVector
are_distinct
are_names_equal
assert_names_equal
batch_dims
broadcast_to_outnames
check_feature_names_are_distinct
check_for_misalignment
check_names_for_dot
compute_baddbmm_outnames
compute_bmm_outnames
compute_broadcast_outnames
compute_cat_outnames
compute_cdist_outnames
compute_diagonal_outnames
compute_dot_product_outnames
compute_included_idxs
compute_matmul_outnames
compute_matmul_outnames_with_dimname_list
compute_squeeze_outnames
dimname_to_position
dimnames_to_positions
feature_dims
has_names
num_batch_dims
propagate_names
propagate_names_except
propagate_names_for_addmm
propagate_names_for_addmv
propagate_names_for_expand
propagate_names_for_reduction
propagate_names_from_src_to_result
propagate_names_if_nonempty
report_nyi_dimname_overload
report_positional_error
to_dimname_repr
unify_from_right
ChannelShuffleOperatorTester
batch_size
channels
group_channels
groups
iterations
testx8
bit_rate
embeddingbag_4bit
embeddingbag_byte
unpack
version
pytorch_qnnp_requantize_precise_neon
Conv2dOpContext
SerializationTypeConv2dPrePack
VulkanConv2dOpContext
create_context
run
unpack
check_in_bounds_for_storage
check_set_storage
maybe_resize_storage_cpu
resize
resize_as
resize_as_sparse
resize_impl_cpu
set_strided
pytorch_qnnp_create_fully_connected_nc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_fully_connected_nc_q8
pytorch_qnnp_status
requires_grad
barf
macro_rules
VectorizedBFloat16
abs
acos
angle
arange
asin
atan
atan2
bfloat16_binary_op_as_fp32
bfloat16_compare_as_fp32
blend
blendv
ceil
clamp
clamp_max
clamp_min
conj
convert
copysign
cos
cosh
cvtbf16_fp32
cvtfp32_bf16
eq
erf
erfc
erfinv
exp
expm1
floor
fmadd
fmod
frac
ge
gt
hypot
i0
i0e
igamma
igammac
imag
lazy_static
le
lgamma
loadu
log
log10
log1p
log2
lt
map
maximum
merge_compare_result
minimum
ne
neg
operator_m256i
pow
real
reciprocal
round
rsqrt
set
sin
sinh
size
size_type
sqrt
store
tan
tanh
trunc
value_type
zero_mask
init
lazy_static
pytorch_q8avgpool_parameters
pytorch_q8conv_parameters
pytorch_q8conv_xzp_parameters
pytorch_q8dwconv_mp_parameters
pytorch_q8dwconv_up_parameters
pytorch_q8gavgpool_parameters
pytorch_q8gemm_sparse_parameters
pytorch_q8sum_rows_parameters
pytorch_qnnp_deinitialize
pytorch_qnnp_initialize
pytorch_qnnp_parameters
pytorch_u8maxpool_parameters
pytorch_x8zip_parameters
sparse_mask_cuda
sparse_mask_out_cuda
lazy_static
batch_check_errors
batch_check_errors_with_tensor_of_infos
batch_count
batch_iterator_with_broadcasting
broadcast_batch_size
check_all_same_dim
check_linalg_compatible_dtype
check_linalg_compatible_dtype_tensor
check_same_device
check_uplo
clone_batched_column_major
compute_geometry_for_q
compute_lrw_ork_dim
copy_batched_column_major
create_dim_backshift_permutation
create_reverse_permutation
create_u_s_vt
get_epsilon
linalg_broadcast_batch_dims
linalg_solve_is_vector_rhs
matrix_stride
move_to_end
parse_qr_mode
same_stride_to
single_check_errors
lazy_static
assert_same_type
of
test_half_2string
test_half_arithmetic
test_half_cast
test_half_common_math
test_half_comparisions
test_half_construction
test_half_numeric_limits
to_string
Loop2d
find_split_dim
foreach_reduced_elt
parallel_dim_reduction
parallel_reduce
round_columns
two_pass_reduction
use_two_pass_reduction
cpu_max_pool
cpu_max_pool_backward
cpu_max_pool_backward_channels_last
cpu_max_pool_channels_last
max_pool2d_backward_kernel_impl
max_pool2d_kernel_impl
register_dispatch
ClampOperatorTester
batch_size
channels
iterations
qmax
qmin
testu8
DropoutDescriptorParams
DropoutState
RNNDescriptorParams
RNNDescriptors
RNNParams
TensorDescriptorListParams
add_projection_weights
cell_size
change
copy_params
copy_weights_to_flat_buf_views
cudnn_impl
cudnn_init_dropout_state
cudnn_rnn
cudnn_rnn_backward
cudnn_rnn_backward_weight
cudnn_rnn_flatten_weight
descriptor
descriptors
get_algo
get_descs
get_dropout_state
get_expected_data_ptrs
get_num_weights
get_parameters
get_x_descs
get_y_descs
hidden_size
is
lock
lstm_cudnn
lstm_packed_cudnn
num_directions
num_linear_layers
pack_hidden
pack_hidden_tensor
pack_hidden_tuple_tensor_tensor
promote_rnn_math_type
rnn_descriptor
rnn_descriptor_sequence
set
set_algo
set_bidirectional
set_mode
try_get_weight_buf
unlock
unpack_hidden
use_persist_common_heuristics
use_persist_device_heuristics
view_or_copy_one_param
view_or_copy_params
view_params
pytorch_qnnp_create_average_pooling2d_nhwc_q8
pytorch_qnnp_operator
pytorch_qnnp_setup_average_pooling2d_nhwc_q8
pytorch_qnnp_status
pytorch_q8avgpool_ukernel_up8xm_sse2
TH
THBFloat16Storage
THBoolStorage
THByteStorage
THCStorage
THCTensor
THCharStorage
THComplexDoubleStorage
THComplexFloatStorage
THCudaBFloat16Storage
THCudaBoolStorage
THCudaByteStorage
THCudaCharStorage
THCudaComplexDoubleStorage
THCudaComplexFloatStorage
THCudaDoubleStorage
THCudaHalfStorage
THCudaIntStorage
THCudaLongStorage
THCudaShortStorage
THCudaStorage
THDoubleStorage
THFloatStorage
THHalfStorage
THIntStorage
THLongStorage
THShortStorage
THStorage
lazy_static
cpu_sparse_coo_softmax
cpu_sparse_coo_softmax_backward
get_nvalues
get_offsets
get_pools
log_softmax_backward_sparse_cpu
log_softmax_sparse_cpu
softmax_backward_sparse_cpu
softmax_sparse_cpu
sparse_log_softmax_a
sparse_log_softmax_b
sparse_log_softmax_c
sparse_softmax_a
sparse_softmax_b
sparse_softmax_c
Initializer
default
th_magma_malloc_pinned
thc_magma_init
VmapMode
current_vmap_level
decrement_nesting
increment_nesting
lazy_static
define_dispatch
lazy_static
quantized_group_norm_impl
quantized_instance_norm_impl
quantized_layer_norm_impl
is_age_zero_and_altb
mat_add_a
mat_add_b
mat_add_double
mat_add_float
mat_copy_a
mat_copy_b
mat_copy_double
mat_copy_float
unfold3d_acc_cpu
unfold3d_acc_kernel_impl
unfold3d_copy_cpu
unfold3d_copy_kernel_impl
unfold3d_zero_padding_acc_kernel_impl
unfold3d_zero_padding_copy_kernel_impl
NnapiWrapper
check_compilation_create
check_compilation_create_for_devices
check_compilation_finish
check_compilation_free
check_compilation_set_preference
check_device_get_feature_level
check_device_get_name
check_device_get_version
check_event_free
check_event_wait
check_execution_compute
check_execution_create
check_execution_free
check_execution_start_compute
check_get_device
check_get_device_count
check_memory_create_from_fd
check_memory_free
check_model_add_operand
check_model_add_operation
check_model_create
check_model_finish
check_model_free
check_model_get_supported_operations_for_devices
check_model_relax_computation_float_32to_float16
check_model_set_operand_value
check_model_set_operand_value_from_memory
lazy_static
nnapi_wrapper
nnapi_wrapper_load
test_weak_pointer_gets_invalidated
test_weak_pointer_lock
test_weak_pointer_updates_refcounts
DeviceThreadHandlePool
DeviceThreadHandlePoolHandle
PoolWindow
assign_from
drop
release
reserve
TensorName
TensorNameVec
TensorNames
append
check_unique
fmt
to_dimname
to_dimname_vec
unify
unify_from_right_inplace
lazy_static
empty_memory_format
empty_strided
lazy_static
pytorch_q8sumrows_ukernel_4x_neon
define_dispatch
lazy_static
PyTorchQnnpAddQuantizationParams
PyTorchQnnpAvgPoolQuantizationParams
PyTorchQnnpConvDynamicQuantizationParams
PyTorchQnnpConvQuantizationParams
PyTorchQnnpFp16ClampingParams
PyTorchQnnpFp32ClampingParams
PyTorchQnnpFp32RequantizationParams
PyTorchQnnpQ31RequantizationParams
PyTorchQnnpU8ClampingParams
PytorchHgemmUkernelFunction
PytorchQ8avgpoolMpUkernelFunction
PytorchQ8avgpoolParameters
PytorchQ8avgpoolUpUkernelFunction
PytorchQ8convParameters
PytorchQ8convUkernelFunction
PytorchQ8convXzpParameters
PytorchQ8dwconvMpParameters
PytorchQ8dwconvMpUkernelFunction
PytorchQ8dwconvUpParameters
PytorchQ8dwconvUpUkernelFunction
PytorchQ8gavgpoolMpUkernelFunction
PytorchQ8gavgpoolParameters
PytorchQ8gavgpoolUpUkernelFunction
PytorchQ8gemmDqSparsePackedAUkernelFunction
PytorchQ8gemmDqSparseUkernelFunction
PytorchQ8gemmDqUkernelFunction
PytorchQ8gemmSparsePackAUkernelFunction
PytorchQ8gemmSparseParameters
PytorchQ8gemmUkernelFunction
PytorchQ8gemmXzpUkernelFunction
PytorchQ8sumRowsParameters
PytorchQ8sumRowsUkernelFunction
PytorchQ8vaddUkernelFunction
PytorchQnnpParameters
PytorchQnnpPreciseRequantizationParams
PytorchQnnpRequantizationParams
PytorchSconvUkernelFunction
PytorchSgemmUkernelFunction
PytorchU8clampUkernelFunction
PytorchU8lut32normUkernelFunction
PytorchU8maxpoolParameters
PytorchU8maxpoolUkernelFunction
PytorchU8rmaxUkernelFunction
PytorchX8lutUkernelFunction
PytorchX8zipParameters
PytorchXzipcUkernelFunction
PytorchXzipvUkernelFunction
aminmax_all
declare_dispatch
define_dispatch
lazy_static
max
min
QConv1dPackWeightInt8
QConvPackWeightInt8
lazy_static
prepack
run
run_conv
run_deconv
require_equal_size_dim
should_expand
test
test_scalar_tensor_cpu
test_scalar_tensor_tensorcuda
pytorch_q8gavgpool_ukernel_mp8x7p7q_neon
col2hvol
hvol2col
slow_conv_dilated2d_backward_cpu
slow_conv_dilated2d_cpu
slow_conv_dilated3d_backward_cpu
slow_conv_dilated3d_cpu
slow_conv_dilated_all_cpu_template
slow_conv_dilated_location_check
copy_
on
Q8ConvContext
Q8DwConvContext
Q8GemmContext
Q8GemmXzpContext
Q8SumRowsContext
QnnpackDeleter
compute_dwconv_multiipass
compute_dwconv_unipass
compute_q8conv
compute_q8gemm
compute_q8gemm_xzp
compute_sum_rows
invoke
q8conv_context
q8dwconv_context
q8gemm_context
q8gemm_xzp_context
q8sum_rows_context
qnnpack_conv
get_pinned_memory_allocator
ReleaseContext
THMapAllocator
THMapInfo
THRefcountedMapAllocator
THRefcountedMapAllocatorArgCheck
WithFd
close
data
decref
delete_thm_ap_allocator
delete_thr_efcounted_map_allocator
drop
fd
filename
from_data_ptr
get_thd_efault_allocator
incref
initialize_alloc
make_data_ptr
size
stat
to
wait_for_release_handle
AffineQuantizer
NonUniformQuantizer
PerChannelAffineFloatQParamsQuantizer
PerChannelAffineQuantizer
PerTensorAffineQuantizer
UniformQuantizer
axis
check_per_channel_param_dims
dequantize
equal_to
get_qtensorimpl
get_sub_byte_tensor_size
make_per_channel_affine_quantizer
make_per_tensor_affine_quantizer
qscheme
quantize
quantizer
scale
scales
set_quantizer
zero_point
zero_points
lazy_static
replication_pad1d_backward_out_batch
replication_pad1d_backward_out_frame
replication_pad1d_out_batch
replication_pad1d_out_frame
replication_pad2d_backward_cpu
replication_pad2d_backward_out_batch
replication_pad2d_backward_out_cpu
replication_pad2d_backward_out_cpu_template
replication_pad2d_backward_out_frame
replication_pad2d_out_batch
replication_pad2d_out_frame
replication_pad3d_backward_cpu
replication_pad3d_backward_out_batch
replication_pad3d_backward_out_cpu
replication_pad3d_backward_out_cpu_template
replication_pad3d_backward_out_frame
replication_pad3d_out_batch
replication_pad3d_out_frame
shape_check3d
define_dispatch
glu
glu_backward
glu_backward_out
glu_out
pytorch_q8dwconv_ukernel_up8x9_sse2
apply_test_collapse_to_point_tensor
apply_test_collapses_zeros_and_ones
apply_test_contiguous2d
apply_test_contiguous3d
apply_test_excluding_in_contiguous4d
apply_test_invalid_exclusion
apply_test_partial_collapse3d
apply_test_partial_strided_collapse4d
apply_test_roving_exclusion
apply_test_strided_collapse2d
lazy_static
initqnnpack
pytorch_qnnp_status
UNARY_OUTER_LOOP
all_same
binary_kernel_reduce
binary_kernel_reduce_vec
is_contiguous_reduction
is_outer_reduction
lazy_static
must
reduction128
set_result
set_results
set_results_tuple
that
vectorized_inner_reduction
vectorized_outer_reduction
pytorch_qnnp_add_quantize
pytorch_qnnp_avgpool_quantize
pytorch_qnnp_compute_add_quantization_params
pytorch_qnnp_compute_avgpool_quantization_params
pytorch_qnnp_compute_conv_quantization_params
pytorch_qnnp_compute_requantization_params
pytorch_qnnp_compute_scalar_add_quantization_params
pytorch_qnnp_compute_scalar_avgpool_quantization_params
pytorch_qnnp_compute_scalar_fp32_requantization_params
pytorch_qnnp_compute_scalar_requantization_params
pytorch_qnnp_compute_u8_clamping_params
pytorch_qnnp_fp32_requantize
pytorch_qnnp_fp32_requantize_magic
pytorch_qnnp_q31_requantize
pytorch_x8lut_ukernel_scalar
declare_dispatch
lazy_static
get
record_stream_masquerading_ascuda
are_all_returns_tensors
batched_tensor_for_loop_fallback
batched_tensor_inplace_for_loop_fallback
compute_index
is_inplace_op
safe_stack
warn_fallback
DequantizeTensorPerChannelAffineFn
DequantizeTensorPerChannelFloatQparamsFn
DequantizeTensorPerTensorAffineFn
DequantizeTensorPerTensorAffineSubByteFn
QuantizeTensorPerChannelAffineFn
QuantizeTensorPerChannelFloatQparamsFn
QuantizeTensorPerTensorAffineFn
QuantizeTensorPerTensorAffineSubByteFn
check_cpu_tensor
check_float_tensor
check_quantized_tensor
check_rounding_mode
check_same_device
check_same_size
check_zero_point
check_zero_points
declare_dispatch
define_dispatch
dequantize_tensor_per_channel_affine
dequantize_tensor_per_channel_float_qparams
dequantize_tensor_per_tensor_affine
quantize_tensor_per_channel_affine
quantize_tensor_per_channel_float_qparams
quantize_tensor_per_tensor_affine
check_valid_identifier
fmt
from_symbol
is_valid_name
lazy_static
matches
unify
wildcard
clog_debug
clog_define_log_debug
clog_define_log_error
clog_define_log_fatal
clog_define_log_info
clog_define_log_warning
clog_error
clog_info
clog_warning
main
all_close
macro_rules
math_kernel_test_addr
math_kernel_test_bmm
math_kernel_test_mish_backward
math_kernel_test_narrow_copy
math_kernel_test_native_group_norm
math_kernel_test_native_layer_norm
math_kernel_test_silu_backward
max_pool3d_with_indices_backward_cpu
max_pool3d_with_indices_backward_out_cpu
max_pool3d_with_indices_backward_out_cpu_template
max_pool3d_with_indices_backward_out_frame
max_pool3d_with_indices_backward_single_out_frame
max_pool3d_with_indices_cpu
max_pool3d_with_indices_out_cpu
max_pool3d_with_indices_out_cpu_template
max_pool3d_with_indices_out_frame
max_pool3d_with_indices_single_out_frame
channel_shuffle_x8
lazy_static
addcdiv_cpu_kernel
addcmul_cpu_kernel
huber_backward_cpu_kernel
mse_backward_cpu_kernel
register_dispatch
smooth_l1_backward_cpu_kernel
LapackLstsqDriverTypeHash
and
apply_cholesky
apply_cholesky_inverse
apply_eig
apply_geqrf
apply_lapack_eigh
apply_linalg_eig
apply_lstsq
apply_lu
apply_lu_solve
apply_orgqr
apply_ormqr
apply_reflect_conj_tri_single
apply_triangular_solve
cholesky_inverse_kernel_impl
cholesky_kernel
class
classes
dispatching
eig_kernel_impl
geqrf_kernel
i64
invoke
linalg_eig_kernel
linalg_eigh_kernel
lstsq_kernel
lu_kernel
lu_solve_kernel
orgqr_kernel_impl
ormqr_kernel
register_arch_dispatch
register_avx2_dispatch
register_avx_dispatch
register_vsx_dispatch
triangular_solve_kernel
aminmax_kernel_impl
clamp_kernel_impl
clamp_max_kernel_impl
clamp_max_scalar_kernel_impl
clamp_min_kernel_impl
clamp_min_scalar_kernel_impl
clamp_scalar_kernel_impl
compare_base_kernel
compare_base_kernel_core
isin_default_kernel_cpu
isneginf_kernel_impl
isposinf_kernel_impl
max_kernel_impl
min_kernel_impl
mode_kernel_impl
promotion
register_dispatch
where_kernel_impl
declare_dispatch
lazy_static
declare_pytorch_sconv_ukernel_function
pytorch_qnnp_fp32_clamping_params
Foo
FooHalf
apply
conversions
test_overflow
test_scalar
test_scalar_conj
test_scalar_equal
test_scalar_formatting
GAvgPoolMicrokernelTester
iterations
m
n
nr
packedn
test
x_scale
x_stride
x_zero_point
y_max
y_min
y_scale
y_zero_point
avg_pool2d_backward_shape_check
avg_pool3d_backward_shape_check
declare_dispatch
lazy_static
max_pool2d_backward_shape_check
max_pool3d_backward_shape_check
pool2d_shape_check
pool3d_shape_check
pooling_same_mode_padding_lr
safe_downcast
backward
base
data
enforce_invariants
grad_fn
is_leaf
is_view
name
or
print
register_hook
remove_hook
requires_grad
retain_grad
retains_grad
set_data
tensor_data
to_string
variable_data
version
Unfold2dFn
declare_dispatch
define_dispatch
compute_image_size
image_from_tensor
all_equal_numel
all_equal_numel_error
apply_op
apply_preamble
collapse_dims
cpu_tensor_apply2
cpu_tensor_apply3
cpu_tensor_apply4
forward
iterate
iterate_continue
iterate_overflow
lazy_static
max_dim
max_dim_tensors
max_iterate_size
scalar
sort_strides
strided_tensor_iter
strided_tensor_iter_fixed
mkldnn_linear
mkldnn_linear_backward
mkldnn_linear_backward_weights
main
test
tanh_op_small_batch
tanh_op_small_batch_with_qmax
tanh_op_small_batch_with_qmin
tanh_op_strided_batch
tanh_op_strided_batch_with_qmax
tanh_op_strided_batch_with_qmin
tanh_op_unit_batch
tanh_op_unit_batch_with_qmax
tanh_op_unit_batch_with_qmin
tanh_op_zero_batch
make_dual
unpack_dual
CppSignature
DispatchKeySet
eq
make
name
pytorch_q8gemm_ukernel_4x4c2_sse2
lazy_static
declare_pytorch_q8mpdwconv_ukernel_function
declare_pytorch_q8updwconv_ukernel_function
AutoGradMode
GradMode
NoGradGuard
InputMeta
declare_dispatch
lazy_static
register_dispatch
stack_serial_kernel
stack_serial_kernel_impl
compute_linear_combination
compute_linear_combination_out
declare_dispatch
define_dispatch
lazy_static
MPSImageWrapper
allocate_storage
allocate_temporary_storage
buffer
command_buffer
copy_data_from_host
copy_data_to_host
image
prepare
release
set_command_buffer
set_image
synchronize
vst1q_f32_x2
GPU
Handle
assign_from
drop
get
macro_rules
operator_bool
release
reset
vk_deleter_dispatchable_declare
vk_deleter_dispatchable_define
vk_deleter_non_dispatchable_declare
vk_deleter_non_dispatchable_define
check_generator
check_intlist
check_size_nonnegative
checked_dense_tensor_list_unwrap
checked_dense_tensor_unwrap
crash_if_asan
empty_cpu
empty_generic
for
get_generator_or_default
lazy_static
tensor_backend
tensor_complex_backend
tensor_complex_cpu
tensor_cpu
pytorch_q8dwconv_ukernel_mp8x25_neon
batch_norm_cpu_backward_channels_last_impl
batch_norm_cpu_backward_contiguous_impl
batch_norm_cpu_backward_kernel
batch_norm_cpu_channels_last_impl
batch_norm_cpu_collect_linear_and_constant_terms
batch_norm_cpu_collect_stats_channels_last_impl
batch_norm_cpu_collect_stats_contiguous_impl
batch_norm_cpu_collect_stats_kernel
batch_norm_cpu_contiguous_impl
batch_norm_cpu_kernel
register_dispatch
BitStr
CheckWithinDomains
CustomCheck
DomainRange
PreventFma
TestSeed
ValueGen
VecTypeHelper
is_complex
lazy_static
2
ALLOCATION_TYPE
AllocInfo
AllocationInfo
AllocationInfoOffsetGreater
AllocationInfoSizeGreater
AtomicTransactionalIncrement
BLOCK_FLAG
BlockAllocation
BlockInfo
BlockInfoCompareMoveDestination
BlockPointerLess
COLLECTION_TYPE
CallParams
ConstantBuffer
DedicatedAllocation
FLAGS
FreeSpace
ItemBlock
MyBuffer
Node
SECOND_VECTOR_MODE
StackItem
T
TYPE
VMA_CACHE_OPERATION
ValidationContext
VmaAllocation
VmaAllocationCreateFlagBits
VmaAllocationCreateInfo
VmaAllocationInfo
VmaAllocationRequest
VmaAllocation_T
VmaAllocator
VmaAllocatorCreateFlagBits
VmaAllocatorCreateInfo
VmaAllocatorInfo
VmaAllocator_T
VmaBlockDefragmentationContext
VmaBlockVector
VmaBudget
VmaCurrentBudgetData
VmaDefragmentationContext
VmaDefragmentationContext_T
VmaDefragmentationFlagBits
VmaDefragmentationInfo
VmaDefragmentationInfo2
VmaDefragmentationMove
VmaDefragmentationPassInfo
VmaDefragmentationPassMoveInfo
VmaDefragmentationStats
VmaDeviceMemoryCallbacks
VmaListItem
VmaMemoryUsage
VmaMutexLock
VmaMutexLockRead
VmaMutexLockWrite
VmaPair
VmaPairFirstLess
VmaPointerLess
VmaPool
VmaPoolCreateFlagBits
VmaPoolCreateInfo
VmaPoolStats
VmaPool_T
VmaRecordFlagBits
VmaRecordSettings
VmaStatInfo
VmaStats
VmaSuballocation
VmaSuballocationItemSizeLess
VmaSuballocationOffsetGreater
VmaSuballocationOffsetLess
VmaSuballocationType
VmaVulkanFunctions
acceptable
and
available
can
chosen
class
contains
failed
for
index
is
lazy_static
maching
must
of
per
present
stays
succeeded
that
type
where
with
mean_out_quantized_cpu
mean_out_quantized_cpu_mut
mean_quantized_cpu_a
mean_quantized_cpu_b
mean_quantized_cpu_c
qnnpack_mean
layer_norm_backward_kernel_impl
layer_norm_backward_kernel_impl_internal
layer_norm_kernel_impl
layer_norm_kernel_impl_internal
register_dispatch
abs_impl
abs_impl_u8
calc_digamma_f32
calc_digamma_f64
calc_erfinv
calc_erfinv_bf16
calc_gcd
calc_i0
calc_i0_bf16
calc_i0e
calc_i0e_bf16
calc_i1
calc_i1e
calc_igamma
calc_igamma_b_float16
calc_igamma_half
calc_igammac
calc_igammac_b_float16
calc_igammac_half
calc_polygamma_double
calc_polygamma_float
chbevl
chebyshev_coefficients_i0e_a
chebyshev_coefficients_i0e_b
chebyshev_coefficients_i1e_a_double
chebyshev_coefficients_i1e_a_float
chebyshev_coefficients_i1e_b_double
chebyshev_coefficients_i1e_b_float
igam_helper_asymptotic_series
igam_helper_fac
igam_helper_series
igamc_helper_continued_fraction
igamc_helper_series
lanczos_sum_expg_scaled
polevl
polevlf
ratevl
trigamma_f32
trigamma_f64
zeta
PoolingParameters
normalize
CudnnPoolType
create_cu_dnn_handle
destroy_cu_dnn_handle
get_cudnn_handle
pytorch_qnnp_delete_operator
define_dispatch
fake_quantize_learnable_per_channel_affine
fake_quantize_learnable_per_channel_affine_backward
fake_quantize_per_channel_affine
fake_quantize_per_channel_affine_cachemask
fake_quantize_per_channel_affine_cachemask_backward
get_rounded_zero_point
LeakyReLUOperatorTester
batch_size
channels
iterations
negative_slope
qmax
qmin
testq8
Context
NoTF32Guard
alert_cu_blas_config_not_deterministic
alert_not_deterministic
allow_tf32cu_dnn
allow_tf32cublas
are_vmap_fallback_warnings_enabled
benchmark_cu_dnn
check_cu_blas_config_deterministic
cpu
cuda
default_generator
deterministic_algorithms
deterministic_cu_dnn
drop
get_cpu_allocator
get_deprecated_type_properties
get_device_from_ptr
get_num_gpu_s
get_thc_state
get_thh_state
getnvrtc
global_context
has_hip
has_mkl
has_mlc
has_openmp
has_xla
hascuda
hascudart
haslapack
hasmagma
hasmkldnn
hip
init
init_cuda_if_needed
init_hip_if_needed
is_pinned_ptr
is_xnnpack_available
lazy_init_hip
lazy_initcuda
lazy_static
manual_seed
not
q_engine
release_weights_when_prepacking
set_allow_tf32cu_dnn
set_allow_tf32cublas
set_benchmark_cu_dnn
set_default_mobile_cpu_allocator
set_deterministic_algorithms
set_deterministic_cu_dnn
set_display_vmap_fallback_warnings
set_flush_denormal
set_qengine
set_release_weights_when_prepacking
set_user_enabled_cu_dnn
set_user_enabled_mkldnn
should_disable_tf32
supported_qengines
unset_default_mobile_cpu_allocator
user_enabled_cu_dnn
user_enabled_mkldnn
versioncudart
ConvParam
based
pytorch_qnnp_create_channel_shuffle_nc_x8
pytorch_qnnp_operator
pytorch_qnnp_setup_channel_shuffle_nc_x8
pytorch_qnnp_status
AnyClassType
AnyClassTypePtr
AnyEnumType
AnyEnumTypePtr
AnyListType
AnyListTypePtr
AnyTupleType
AnyTupleTypePtr
AnyType
AnyTypePtr
AttributeKind
BoolType
BoolTypePtr
CapsuleType
CapsuleTypePtr
ClassAttribute
ClassType
ClassTypeProperty
ClassTypePtr
ComplexType
ComplexTypePtr
ConstNamedTypePtr
DeviceObjType
DeviceObjTypePtr
DictType
DictTypePtr
EnumNameValue
EnumType
EnumTypePtr
EnumerationType
FloatType
FloatTypePtr
FunctionType
FunctionTypePtr
FutureType
FutureTypePtr
GeneratorType
GeneratorTypePtr
GetTypePtr
InferredType
IntType
IntTypePtr
InterfaceType
InterfaceTypePtr
LayoutType
LayoutTypePtr
ListOfOptionalElements
ListType
ListTypePtr
MatchTypeReturn
NameList
NamedType
NamedTypePtr
NoneType
NoneTypePtr
NumberType
NumberTypePtr
OptNameList
OptionalType
OptionalTypePtr
Output
PyObjectType
PyObjectTypePtr
QSchemeType
QSchemeTypePtr
QuantizerType
QuantizerTypePtr
RRefType
RRefTypePtr
ScalarTypeType
ScalarTypeTypePtr
ShapeSymbol
SingleElementType
StorageType
StorageTypePtr
StreamObjType
StreamObjTypePtr
Stride
StringType
StringTypePtr
SymbolicShape
TensorType
TensorTypePtr
TupleType
TupleTypePtr
TypeEnv
TypeVerbosity
VarType
VarTypePtr
VaryingShape
add_attribute
add_constant
add_forward_hook
add_forward_pre_hook
add_method
add_or_check_attribute
add_property
add_static_method
annotation_str_impl
at
call
cast
cast_named_type
check_forward_hook_schema
check_forward_pre_hook_schema
check_no_any
check_not_exist
clone
cmp
compare
compilation_unit
compute_stride_props
concrete_sizes
constant_names
constant_values
contained_types
contiguous
contiguous_strides_of
create
create_contiguous
create_named
create_with_contained
default
device
dim
dimensioned_only
doc_string
dump
element_type_can_be_inferred_from_members
elements
enum_names_values
eq
find_attribute
find_attribute_slot
find_constant
find_constant_slot
find_forward_hook
find_forward_pre_hook
find_hook
find_method
find_static_method
fmt
from
from_bool_type
from_number_type
from_static_size
function
get
get_attribute
get_attribute_name
get_attribute_slot
get_attributes
get_constant
get_constant_name
get_constant_slot
get_element_type
get_forward_hook_error_message
get_forward_hooks
get_forward_pre_hook_error_message
get_forward_pre_hooks
get_hook
get_inferred
get_key_type
get_kind
get_method
get_name
get_property
get_type
get_type_ptr
get_value_type
has_attribute
has_constant
has_free_variables
has_method
hierarchy
in
index
information
is_bool_list
is_buffer
is_complete
is_complex_double_list
is_double_list
is_inferred_type
is_int_list
is_module
is_parameter
is_static
is_sub_type_impl
is_subtype_of_ext
is_summarized
is_tensor_list
is_unresolved_class_attribute
like
match_tensor
match_type_variables
merge
merge_primitive_option_stride
merge_primitive_option_t
merge_primitive_shape_symbol
methods
name
num_attributes
num_constants
numel
of
of_bools
of_complex_doubles
of_floats
of_ints
of_strings
of_tensor
of_tensors
partial_cmp
properties
qualified_class_name
rank
reason
refine
refinement
repr_str
represents
requires_grad
scalar_type
scalar_type_from_jit_type
schema
size
sizes
specified
starts
static_size
str_
stride_properties
strides
success
symbolic_sizes
tags
to_string
try_eval_type_variables
try_scalar_type_from_jit_type
ty
type_verbosity
undefined
unify_type_list
unify_types
unsafe_change_attribute_type
unsafe_remove_attribute
unsafe_remove_constant
unsafe_remove_method
unshaped_type
value
variable
variables
was
when
with
with_dim
with_possibly_undefined
with_requires_grad
with_scalar_type
with_sizes
with_sizes_strides
with_symbolic_shapes
with_undefined
declare_pytorch_xzipc_ukernel_function
declare_pytorch_xzipv_ukernel_function
add_moments
add_moments_vec
rowwise_moments
rowwise_moments_impl
ContextConv2D
is_signed_to_unsigned
safe_downcast
safe_downcast_internal
declare_pytorch_q8mpgavgpool_ukernel_function
declare_pytorch_q8upgavgpool_ukernel_function
pytorch_qnnp_create_max_pooling2d_nhwc_u8
pytorch_qnnp_operator
pytorch_qnnp_setup_max_pooling2d_nhwc_u8
pytorch_qnnp_status
ID
SparseTensorImpl
coalesced
copy_tensor_metadata
dense_dim
has_storage
indices
nnz
of
raw_resize
refresh_numel
release_resources
resize
resize_and_clear
set_coalesced
set_indices_and_values_unsafe
set_nnz_and_narrow
set_size
set_storage_offset
set_stride
shallow_copy_and_detach
shallow_copy_from
sparse_dim
sparse_tensor_set_to_device_type
stride
strides
tensorimpl_type_name
values
mkldnn_batch_norm
mkldnn_batch_norm_backward
cartesian_prod
combinations
triu_mask
pytorch_q8gemm_ukernel_2x4c8_sse2
pytorch_sse_reduce4_i32
CacheKey
filter_engine_configs
get_alignment
get_conv_descriptor
get_tensor_descriptor
lazy_static
raw_cudnn_convolution_forward_out
Conv2DParams
is_depthwise
Block
lazy_static
upsample_nearest2d
lazy_static
lazy_static
baddbmm_mkl
baddbmm_mkl_template
gemm
gemm_batched
conditional_accessor_1d
conditional_data_ptr
declare_dispatch
lazy_static
mkldnn_gelu
mkldnn_relu
mkldnn_relu_backward
mkldnn_relu_mut
copy_mkldnn
RMaxMicrokernelTester
iterations
n
test
EllipsisIndexType
Slice
TensorIndex
TensorIndexType
apply_select
apply_slice
apply_slicing
bool_to_indexing_tensor
bool_to_indexing_tensor_cpu_orcuda
bool_to_indexing_tensor_non_native_device_type
boolean
copy_to
count_specified_dimensions
dispatch_index
dispatch_index_put
fmt
get_item
handle_dim_in_multi_dim_indexing
index
index_put
integer
is_boolean
is_ellipsis
is_integer
is_none
is_slice
is_tensor
lazy_static
record_tensor_index
scalar_to_tensor
scalar_to_tensor_non_native_device_type
set_item_with_scalar
set_item_with_tensor
slice
slice_prefix1s_size
start
step
stop
tensor
type_convert_indices
filter
fmap
lazy_static
named
VulkanAdapter
has_unified_memory
local_work_group_size
NoTracerDispatchMode
is_dispatch_enabled
set_dispatch_enabled
clamp_quantized_cpu
define_dispatch
hardtanh_out_quantized_cpu
hardtanh_quantized_cpu
hardtanh_quantized_cpu_mut
lazy_static
qnnpack_clamp
quantized_clamp_impl
CompositeRandomAccessor
OperatorBracketsProxy
Output
ReferencesHolder
add
add_assign
assign_from
cmp
data
eq
from
index_mut
of
operator_arrow
operator_reference
operator_references
operator_star
operator_values
partial_cmp
prefix_decrement
prefix_increment
reference
reference_operator_star
references
sub
sub_assign
value_type
values
u8maxpool_16x9p8q_neon_kc_div_16_multipass
u8maxpool_16x9p8q_neon_kc_div_16_multipass_with_qmax
u8maxpool_16x9p8q_neon_kc_div_16_multipass_with_qmin
u8maxpool_16x9p8q_neon_kc_div_16_multipass_with_x_stride
u8maxpool_16x9p8q_neon_kc_div_16_twopass_fulltile
u8maxpool_16x9p8q_neon_kc_div_16_twopass_fulltile_with_qmax
u8maxpool_16x9p8q_neon_kc_div_16_twopass_fulltile_with_qmin
u8maxpool_16x9p8q_neon_kc_div_16_twopass_fulltile_with_x_stride
u8maxpool_16x9p8q_neon_kc_div_16_twopass_subtile
u8maxpool_16x9p8q_neon_kc_div_16_unipass_fulltile
u8maxpool_16x9p8q_neon_kc_div_16_unipass_fulltile_with_qmax
u8maxpool_16x9p8q_neon_kc_div_16_unipass_fulltile_with_qmin
u8maxpool_16x9p8q_neon_kc_div_16_unipass_fulltile_with_x_stride
u8maxpool_16x9p8q_neon_kc_div_16_unipass_subtile
u8maxpool_16x9p8q_neon_kc_eq_16_multipass
u8maxpool_16x9p8q_neon_kc_eq_16_multipass_with_qmax
u8maxpool_16x9p8q_neon_kc_eq_16_multipass_with_qmin
u8maxpool_16x9p8q_neon_kc_eq_16_twopass_fulltile
u8maxpool_16x9p8q_neon_kc_eq_16_twopass_fulltile_with_qmax
u8maxpool_16x9p8q_neon_kc_eq_16_twopass_fulltile_with_qmin
u8maxpool_16x9p8q_neon_kc_eq_16_twopass_subtile
u8maxpool_16x9p8q_neon_kc_eq_16_unipass_fulltile
u8maxpool_16x9p8q_neon_kc_eq_16_unipass_fulltile_with_qmax
u8maxpool_16x9p8q_neon_kc_eq_16_unipass_fulltile_with_qmin
u8maxpool_16x9p8q_neon_kc_eq_16_unipass_subtile
u8maxpool_16x9p8q_neon_kc_gt_16_multipass
u8maxpool_16x9p8q_neon_kc_gt_16_multipass_with_qmax
u8maxpool_16x9p8q_neon_kc_gt_16_multipass_with_qmin
u8maxpool_16x9p8q_neon_kc_gt_16_multipass_with_x_stride
u8maxpool_16x9p8q_neon_kc_gt_16_twopass_fulltile
u8maxpool_16x9p8q_neon_kc_gt_16_twopass_fulltile_with_qmax
u8maxpool_16x9p8q_neon_kc_gt_16_twopass_fulltile_with_qmin
u8maxpool_16x9p8q_neon_kc_gt_16_twopass_fulltile_with_x_stride
u8maxpool_16x9p8q_neon_kc_gt_16_twopass_subtile
u8maxpool_16x9p8q_neon_kc_gt_16_unipass_fulltile
u8maxpool_16x9p8q_neon_kc_gt_16_unipass_fulltile_with_qmax
u8maxpool_16x9p8q_neon_kc_gt_16_unipass_fulltile_with_qmin
u8maxpool_16x9p8q_neon_kc_gt_16_unipass_fulltile_with_x_stride
u8maxpool_16x9p8q_neon_kc_gt_16_unipass_subtile
u8maxpool_16x9p8q_neon_small_n
u8maxpool_16x9p8q_neon_small_n_with_s
u8maxpool_16x9p8q_neon_small_n_with_x_stride
u8maxpool_16x9p8q_neon_small_n_with_y_stride
u8maxpool_16x9p8q_sse2_kc_div_16_multipass
u8maxpool_16x9p8q_sse2_kc_div_16_multipass_with_qmax
u8maxpool_16x9p8q_sse2_kc_div_16_multipass_with_qmin
u8maxpool_16x9p8q_sse2_kc_div_16_multipass_with_x_stride
u8maxpool_16x9p8q_sse2_kc_div_16_twopass_fulltile
u8maxpool_16x9p8q_sse2_kc_div_16_twopass_fulltile_with_qmax
u8maxpool_16x9p8q_sse2_kc_div_16_twopass_fulltile_with_qmin
u8maxpool_16x9p8q_sse2_kc_div_16_twopass_fulltile_with_x_stride
u8maxpool_16x9p8q_sse2_kc_div_16_twopass_subtile
u8maxpool_16x9p8q_sse2_kc_div_16_unipass_fulltile
u8maxpool_16x9p8q_sse2_kc_div_16_unipass_fulltile_with_qmax
u8maxpool_16x9p8q_sse2_kc_div_16_unipass_fulltile_with_qmin
u8maxpool_16x9p8q_sse2_kc_div_16_unipass_fulltile_with_x_stride
u8maxpool_16x9p8q_sse2_kc_div_16_unipass_subtile
u8maxpool_16x9p8q_sse2_kc_eq_16_multipass
u8maxpool_16x9p8q_sse2_kc_eq_16_multipass_with_qmax
u8maxpool_16x9p8q_sse2_kc_eq_16_multipass_with_qmin
u8maxpool_16x9p8q_sse2_kc_eq_16_twopass_fulltile
u8maxpool_16x9p8q_sse2_kc_eq_16_twopass_fulltile_with_qmax
u8maxpool_16x9p8q_sse2_kc_eq_16_twopass_fulltile_with_qmin
u8maxpool_16x9p8q_sse2_kc_eq_16_twopass_subtile
u8maxpool_16x9p8q_sse2_kc_eq_16_unipass_fulltile
u8maxpool_16x9p8q_sse2_kc_eq_16_unipass_fulltile_with_qmax
u8maxpool_16x9p8q_sse2_kc_eq_16_unipass_fulltile_with_qmin
u8maxpool_16x9p8q_sse2_kc_eq_16_unipass_subtile
u8maxpool_16x9p8q_sse2_kc_gt_16_multipass
u8maxpool_16x9p8q_sse2_kc_gt_16_multipass_with_qmax
u8maxpool_16x9p8q_sse2_kc_gt_16_multipass_with_qmin
u8maxpool_16x9p8q_sse2_kc_gt_16_multipass_with_x_stride
u8maxpool_16x9p8q_sse2_kc_gt_16_twopass_fulltile
u8maxpool_16x9p8q_sse2_kc_gt_16_twopass_fulltile_with_qmax
u8maxpool_16x9p8q_sse2_kc_gt_16_twopass_fulltile_with_qmin
u8maxpool_16x9p8q_sse2_kc_gt_16_twopass_fulltile_with_x_stride
u8maxpool_16x9p8q_sse2_kc_gt_16_twopass_subtile
u8maxpool_16x9p8q_sse2_kc_gt_16_unipass_fulltile
u8maxpool_16x9p8q_sse2_kc_gt_16_unipass_fulltile_with_qmax
u8maxpool_16x9p8q_sse2_kc_gt_16_unipass_fulltile_with_qmin
u8maxpool_16x9p8q_sse2_kc_gt_16_unipass_fulltile_with_x_stride
u8maxpool_16x9p8q_sse2_kc_gt_16_unipass_subtile
u8maxpool_16x9p8q_sse2_small_n
u8maxpool_16x9p8q_sse2_small_n_with_s
u8maxpool_16x9p8q_sse2_small_n_with_x_stride
u8maxpool_16x9p8q_sse2_small_n_with_y_stride
u8maxpool_sub16_neon_kc_lt_16_1xm_pool
u8maxpool_sub16_neon_kc_lt_16_1xm_pool_with_qmax
u8maxpool_sub16_neon_kc_lt_16_1xm_pool_with_qmin
u8maxpool_sub16_neon_kc_lt_16_mx1_pool
u8maxpool_sub16_neon_kc_lt_16_mx1_pool_with_qmax
u8maxpool_sub16_neon_kc_lt_16_mx1_pool_with_qmin
u8maxpool_sub16_neon_kc_lt_16_small_n
u8maxpool_sub16_neon_kc_lt_16_small_n_with_qmax
u8maxpool_sub16_neon_kc_lt_16_small_n_with_qmin
u8maxpool_sub16_neon_kc_lt_16_small_n_with_s
u8maxpool_sub16_neon_kc_lt_16_small_n_with_x_stride
u8maxpool_sub16_sse2_kc_lt_16_1xm_pool
u8maxpool_sub16_sse2_kc_lt_16_1xm_pool_with_qmax
u8maxpool_sub16_sse2_kc_lt_16_1xm_pool_with_qmin
u8maxpool_sub16_sse2_kc_lt_16_mx1_pool
u8maxpool_sub16_sse2_kc_lt_16_mx1_pool_with_qmax
u8maxpool_sub16_sse2_kc_lt_16_mx1_pool_with_qmin
u8maxpool_sub16_sse2_kc_lt_16_small_n
u8maxpool_sub16_sse2_kc_lt_16_small_n_with_qmax
u8maxpool_sub16_sse2_kc_lt_16_small_n_with_qmin
u8maxpool_sub16_sse2_kc_lt_16_small_n_with_s
u8maxpool_sub16_sse2_kc_lt_16_small_n_with_x_stride
get_state
set_state
AutoDispatchBelowADInplaceOrView
AutoDispatchBelowAutograd
AutoNonVariableTypeMode
default
dispatch
pytorch_qnnp_x8zip_x2_neon
HGEMM
HGEMM_L1
HGEMM_Op
a
b
c
clamping_params
divide_round_up
k
kc
kc_stride
kr
lazy_static
mc
mr
nc
nc_stride
nr
round_up
set_up
tear_down
w
InternedStrings
SymbolInfo
custom_string
ns
string
symbol
TensorArg
TensorGeometryArg
as
check_all_contiguous
check_all_defined
check_all_same
check_all_same_gpu
check_all_same_numel
check_all_same_size
check_all_same_type
check_backend
check_backend_plural
check_contiguous
check_defined
check_device_type_a
check_device_type_b
check_dim_a
check_dim_b
check_dim_range
check_dim_size
check_layout_a
check_layout_b
check_numel
check_same_dim
check_same_gpu
check_same_numel
check_same_size
check_same_type
check_scalar_type
check_scalar_types
check_size_a
check_size_b
compute_storage_nbytes
compute_stride_a
compute_stride_b
compute_stride_impl
default_strides
deref
fmt
geometry_is_contiguous
maybe_data_ptr_a
maybe_data_ptr_b
operator_star
blend_choice
blend_choice_complex
blend_choice_complex_dbl
blend_choice_dbl
c10_vsx_vec_nan_propag
lazy_static
mask_for_complex
mask_for_complex_dbl
name
tmp
vec_float
vec_max_nan
vec_max_nan_vfloat32
vec_max_nan_vfloat64
vec_min_nan
vec_min_nan_vfloat32
vec_min_nan_vfloat64
vec_neg_f32
vec_neg_f64
vec_neg_i16
vec_neg_i32
vec_neg_i64
vec_signed32
vec_signed64
vec_sldw_aux
vsx_complex_dbl_mask1
vsx_complex_dbl_mask2
vsx_complex_mask1
vsx_complex_mask2
vsx_dbl_mask1
vsx_dbl_mask2
vsx_mask1
vsx_mask2
