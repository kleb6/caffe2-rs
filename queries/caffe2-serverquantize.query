hello! 

I am writing a rust crate called caffe2-serverquantize. 

It is part of workspace containing the rust
translation of the caffe2 operator library.

I would like you to help me write a rust crate
description for it.

Please write the description in the following format:

<short description tag used in the crate header>
<double newline>
<longer, in depth description and mathematical analysis>
<END>

Please indicate that the crate is in the
process of being translated from C++ to
Rust. It is possible some of the function
bodies are in the process of translation.

Below are listed some tokens from this crate. 

Please describe the mathematical ideas
identified by this information alongside the
relevant mathematical equations in unicode.

thanks!

please format your response in the Markdown file format.

here are the tokens:
BaseType
FullyConnectedDNNLowPAcc16Op
declare_int32
register_cpu_operator_with_engine
run_on_device
ReluDNNLowPOp
as
register_cpu_operator_with_engine
reluavx2
run_on_device
l2_minimization_kernelavx2
Int8GenQuantParamsMinMaxOp
register_cpu_operator
run_on_device
tensor_inference_function
ConvPoolDNNLowPOpBase
create_shared_int_32buffer
declare_bool
declare_int
drop
fp_32op
measure_quantization_error
run_on_device_epilogue
run_with_shared_buffer
array
fp16quant_fp32_to_fp16
isclose
isrelclose
mse
test_case
test_vector_case
testarray
average_pool_3d_avx2
average_pool_avx2
max_pool_avx2
dynamic_histogram_hist_similar
Sigmoid
compute
BatchMatMulDNNLowPOp
register_cpu_operator_with_engine
run_on_device
sigmoid_sigmoid_unit_test
reluavx2
reluavx2_u16
reluavx2_u8
server_quanize_kl_minimization_example
GroupNormDNNLowPOp
GroupNormFP32Op
affine_batch_channel_dequantizednchw
affine_batch_channel_dequantizednhwc
affine_batch_channel_quantizednchw
affine_batch_channel_quantizednhwc
compute_dequantized_fused_params
compute_quantized_fused_params
compute_quantized_inv_std
dequantized_group_momentsnchw
dequantized_group_momentsnhwc
get_quantization_parameters
quantize_beta
quantize_gamma
quantize_gamma_impl
quantized_group_momentsnchw
quantized_group_momentsnhwc
register_cpu_operator_with_engine
run_on_device
run_on_device_with_ordernchw
run_on_device_with_ordernhwc
server_quantize_l1_minimization_example
QuantizeDNNLowPOp
register_cpu_operator_with_engine
run_on_device
BatchPermutationDNNLowPOp
BatchPermutationFP32Op
register_cpu_operator_with_engine
run_on_device
L1ErrorMinimization
NormMinimization
NormMinimizationKind
P99
choose_quantization_params
choose_quantization_params_impl
default
nonlinear_quantization_params_search
spatial_bn_nhwc_avx2
spatial_bn_nhwc_avx2_u8
spatial_bn_nhwc_avx2_uint8
TanhFunctor
invoke
register_cpu_operator_with_engine
im_2col3DNHWC
im_2colNCHW
im_2colNHWC
im_2col_nd_nchw
Tanh
compute
get_pass_region_end
get_pass_region_end_dequantized
get_saturation_region_begin
sgn
quanitze_p99_example
server_quantize_l2_minimization_approx_example
tanh_tanh_unit_test
ConvDNNLowPOp
ConvFp32Op
acc16
conv_nhwc_core
conv_nhwc_ref
declare_bool
define_bool
dispatchfbgemm
filter_quantization_params
get_conv3d_param
get_conv_param
get_quantization_parameters
im_2colnhwc
is_convgemm
kernel_dim
no_im_2colnhwc
partition_grouped_nhwc_conv
pre_compute_row_column_offsets
quantize_bias
quantize_weight
register_cpu_operator_with_engine
requantization_params
run_on_device_epiloguenchw
run_on_device_epiloguenhwc
run_on_device_with_ordernchw
run_on_device_with_ordernhwc
take_depth_wise3x3x_3fast_path
take_depth_wise3x_3fast_path
takeg_conv_fast_path
ConvDNNLowPPackWeightOp
ConvFp32Op
FCFp32Op
FullyConnectedDNNLowPPackWeightOp
Int8ConvDNNLowPPackedWeightBlob
Int8ConvDNNLowpPackedWeightBlobShapeFunctions
Int8FCDNNLowPPackedWeightBlob
Int8FCDNNLowpPackedWeightBlobShapeFunctions
caffe_known_type
compute_column_offsets
constexpr
count_outliers
declare_double
declare_int32
extract_outlier_matrix
get_conv3d_param
get_conv_param
get_external_tensor_info
get_external_tensor_type
get_type_meta_id
in
is_quantized
is_same_meta_type
load_info_of_blob
quantize_conv_bias
quantize_weight
register_cpu_operator
register_cpu_operator_with_engine
register_external_tensor_functions
run_on_device
setup_external_tensor_descriptor
take_depth_wise3x3x_3fast_path
take_depth_wise3x_3fast_path
takeg_conv_fast_path
tensor_inference_function
to
MulDNNLowPOp
MulFp32Op
get_quantization_parameters
register_cpu_operator_with_engine
run_on_device
server_quantize_l2_minimization_example
FullyConnectedDNNLowPOp
cost_inference_function
declare_bool
define_bool
get_quantization_parameters
register_cpu_operator_with_engine
run_on_device
tensor_inference_function
Int8ConvDNNLowPPackedWeightBlob
Int8FCDNNLowPPackedWeightBlob
DynamicHistogram
Histogram
add
add_collect_with_single_thread
finalize
get_histogram
max
min
remap_histograms
DequantizeDNNLowPOp
register_cpu_operator_with_engine
run_on_device
find_min_max
find_min_maxf32
get_float_tensor_data
has_dnn_lowp_engine_from_op_base
has_dnn_lowp_engine_from_op_def
lazy_static
ConcatDNNLowPOp
get_quantization_parameters
register_cpu_operator_with_engine
run_on_device
get1d_partition
get1d_partition_of2d
get_work_per_thread
ChooseQuantizationTest
choose_quantization_test_l2minimization_test
instantiate_test_case_p
element_wise_sumavx2
QuantizationErrorStats
get_quantization_factory_of
get_quantization_factory_of_
get_static_quantization_params_of
has_dnn_lowp_engine_
has_dnnlow_pengine_
has_static_quantization
measure_quantization_error
report_quantization_error
set_static_quantization_params
set_static_quantization_params_
ResizeNearestDNNLowPOp
ResizeNearestFP32Op
register_cpu_operator_with_engine
run_on_device
requantization_batch_requantization_unit_test
requantization_requantization_unit_test
ElementwiseLinearDNNLowPOp
ElementwiseLinearFp32Op
get_quantization_parameters
register_cpu_operator_with_engine
run_on_device
AveragePool
AveragePoolDnnLowPOp
AveragePoolFp32Op
MaxPool
MaxPoolDnnLowPOp
MaxPoolFp32Op
finalize
initialize
process
process_xdata_ydata
register_cpu_operator_with_engine
run_on_device_with_orderNCHW
run_on_device_with_orderNHWC
LSTMUnitDNNLowPOp
drop
fp_32op
get_quantization_parameters
lstm_unit
register_cpu_operator_with_engine
run_on_device
FbGemmPackOp
allow_inplace
caffe_known_type
register_cpu_operator
run_on_device
tensor_inference_function
fp32_to_bfp14
fp32_to_bfp16
fp32_to_bfp16_round
fp32_to_bfp16_scalar
fp32_to_bfp24
fp32_to_fp16
transpose_4rows
Int8GenQuantParamsOp
Int8QuantParamsBlob
Int8QuantSchemeBlob
caffe_known_type
register_cpu_operator
run_on_device
tensor_inference_function
ConvReluOp
cost_inference_function
register_cpu_operator
run_on_device_with_ordernchw
run_on_device_with_ordernhwc
tensor_inference_function
choose_quantization_params
get_norm
L2ErrorMinimization
default
l2minimization_kernelavx2
SigmoidFunctor
invoke
register_cpu_operator_with_engine
ChannelShuffleDNNLowPOp
ChannelShuffleFp32Op
register_cpu_operator_with_engine
run_on_device
run_on_device_with_orderNCHW
run_on_device_with_orderNHWC
AddDNNLowPOp
AddFp32Op
get_quantization_parameters
register_cpu_operator_with_engine
run_on_device
allow_inplace
get_quantization_parameters
register_cpu_operator_with_engine
run_on_device
BinaryElementwiseDNNLowPOp
Eigen
UnaryElementwiseWithArgsDNNLowPOp
run_on_device
GatherDNNLowPOp
SumDNNLowPOp
do_run_with_type
drop
element_wise_sumavx2
fp_32op
register_cpu_operator_with_engine
run_on_device
Int8QuantSchemeBlobFillOp
register_cpu_operator
run_on_device
tensor_inference_function
FullyConnectedFakeLowpFPOp
FullyConnectedGradientFakeLowpFPOp
do_run_with_type
fp32_to_bfp14
fp32_to_bfp16
fp32_to_bfp16_round
fp32_to_bfp16_scalar
fp32_to_bfp24
fp32_to_fp16
register_cpu_operator_with_engine
run_on_device
SpatialBNReluOp
allow_inplace
enforce_inplace
register_cpu_operator
run_on_device
ConvDNNLowPAcc16Op
acc16
conv_nhwc_acc16_ref
conv_outlier
declare_bool
declare_int32
define_double
define_int32
dispatchfbgemm
get_quantization_parameters
register_cpu_operator_with_engine
run_on_device_with_ordernchw
run_on_device_with_ordernhwc
SpatialBNDNNLowPOp
compute_fused_param
register_cpu_operator_with_engine
run_on_device
spatial_bn_nhwc_avx2
server_quantize_elementwise_sum_benchmark
QuantizationFactory
QuantizationKind
adjust_hist_to_include_zero
choose_quantization_params
choose_quantization_params_with_histogram
choose_quantization_params_with_histogram_and_precision
choose_quantization_params_with_precision
choose_quantization_params_with_values
choose_quantization_params_with_values_and_precision
choose_quantization_params_with_values_len_weight
choose_requantization_multiplier
default
define_bool
define_double
define_int32
define_string
get_activation_kind
get_activation_precision
get_default_instance
get_eltwise_quantize_precision
get_preserve_activation_sparsity
get_preserve_weight_sparsity
get_weight_kind
get_weight_precision
set_activation_p99threshold
set_weight_p99threshold
string_to_kind
ComputeEqualizationScaleOp
register_cpu_operator
run_on_device
tensor_inference_function
FbFCPackedOperator
PackedGemmMatrixFP16ShapeFunctions
caffe2_initialize_fbgemm
cost_inference_function
do_run_with_type
get_external_tensor_info
get_external_tensor_type
get_fbgemm_tensor_info
get_type_meta_id
is_quantized
is_same_meta_type
load_info_of_blob
register_caffe2_init_function
register_cpu_operator
register_external_tensor_functions
run_on_device
setup_external_tensor_descriptor
tensor_inference_function
affine_batch_channel_and_requantize_nhwc_avx2
affine_batch_channel_and_requantizenchwavx2
affine_batch_channel_and_requantizenchwavx2_u8
affine_batch_channel_and_requantizenhwcavx2_u8
compute_quantized_fused_paramsAVX2
segment_moments_avx2
segment_moments_avx2_u8
vector_moments_avx2
vector_moments_avx2_u8
KLDivergenceMinimization
choose_quantization_params
store_matrix_in_matrix_market_format
get_or_create_fbgemm_packb_matrix
SumReluOp
do_run_with_type
register_cpu_operator
run_on_device
OpWrapper
T
get
ResizeNearest3DDNNLowPOp
ResizeNearest3DFP32Op
register_cpu_operator_with_engine
run_on_device
