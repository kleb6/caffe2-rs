hello! could you please help me write a crate-description for the rust crate "caffe2-serverquantize" containing the following symbols? not all of the symbols are defined in this crate, but they are used somehow (please do not explicitly list the symbols, or write anything other than a simple description.  no description header is necessary. however, please be descriptive without making too many assumptions):

?
ACC
Add
Arc
BaseType
CPUs
CSC
ColPtr
CompressedSparseColumn
DNNLOWP
DoNothing
DoSpmdmOnInpBuffer
ExtractOutlierMatrix
FC
FCDNNLowPPackedWeightBlob
FLAGS
FUSE
FullyConnectedDNNLowPAcc
FullyConnectedDNNLowPOp
GEMM
Get
GetQuantizationParameters
GetQuantizedOutputData
GetSingleArgument
INFO
If
Input
InputIsType
InputTensorCPU
Int
Intel
K
LOG
M
MeasureQuantizationError
NoTranspose
OPERATOR
Op
OperatorDef
Output
OutputTensorCPU
Pack
PackAMatrix
PackAWithRowOffset
PackBMatrix
ParseDNNLowPOperatorArguments
PropagateOutputTensorQuantizationParams
Proportion
Quantize
QuantizeInputIfNeeded
Quantized
RELU
ReQuantizeForFloat
ReQuantizeOutput
Resize
Self
Separate
SpMDM
TODO
Transpose
W
WARNING
We
Workspace
Wq
Xdata
Y
Ydata
acc
accumulate
accumulation
already
and
aware
axis
b
base
be
bias
bit
blob
block
buf
but
cache
caffe
canonical
cast
channel
channelwise
cnt
column
const
constant
copy
cpu
debug
declare
def
dequantize
dequantized
device
dim
dnnlowp
doNothingObj
doesn
empty
encounter
engine
false
faster
fbgemm
fbgemmPacked
filter
format
frequency
from
fuse
getRowOffsetBuffer
group
id
in
index
input
j
k
layer
ll
log
main
match
matrix
multipliers
namespace
nbits
ncols
needed
new
non
nullptr
occurences
offset
offsets
omp
op
operator
out
outType
outlier
outliers
output
pack
packA
packed
packedBufferSize
parallel
parallelization
parameters
per
point
points
pragma
qparams
quant
quantization
quantize
quantized
ref
register
reinterpret
relu
reqObj
requantization
requantize
reset
resize
row
rowOffsetBufferSize
saturation
scale
scales
shape
signed
size
specified
spmdmObj
static
swap
temp
this
thread
threads
type
typename
u
uint
use
using
vec
w
weight
will
with
work
wq
ws
yet
zero

?
Box
Choose
DNNLOWP
Dequantize
Even
GE
GetCpuId
GetInputTensorQuantizationParamsOf
GetMutable
GetQuantizationFactoryOf
If
Input
InputIsType
Int
IsType
LE
OPENMP
OPERATOR
Operator
OperatorDef
OperatorStorage
Output
Outputs
PropagateOutputTensorQuantizationParams
QuantizationFactory
Quantize
QuantizeInputIfNeeded
ReLU
Relu
ReluAVX
ReluDNNLowPOp
ResizeLike
Self
TensorCPU
TensorQuantizationParams
The
Workspace
Y
as
avx
be
because
can
cast
chosen
const
context
cpu
def
dequantized
device
dnnlowp
endif
engine
fbgemm
follows
ifdef
ignored
in
inplace
input
internal
limits
lowest
max
memcpy
n
namespace
needed
new
not
nullptr
omp
operator
output
parallel
parameters
params
point
pragma
pre
qfactory
qparams
quantization
quantized
register
relu
reluavx
resize
same
should
sizeof
static
storage
temp
there
this
type
u
use
using
was
with
ws
y
zero

Using
VLEN
add
and
begin
beginning
belong
bin
bins
blendv
buf
castsi
center
cmpeq
compilation
constexpr
cvtepi
cvtps
dbg
default
delta
density
distances
dst
end
entirely
epi
error
first
floor
floorf
fmadd
from
gather
generates
identity
in
instead
inverse
kernelavx
loop
m
mask
max
min
minimization
mm
mode
mul
nbins
norm
precision
ps
remainder
set
setzero
sizeof
src
start
storeu
sub
to?
v
vectorized
which
width
within

CPU
ChooseQuantizationParams
DataType
DefaultEngine
EVERY
Engine
FLOAT
GenQuantParamsMinMax
GenQuantParamsMinMaxOp
Generate
GetDefaultInstance
INFO
Input
InputSize
Int
LOG
OPERATOR
Operator
OperatorDef
OperatorStorage
Optional
Output
QuantParamsBlob
QuantSchemeBlob
QuantizationFactory
Self
Tensor
TensorProto
TensorQuantizationParams
TensorShape
The
We
Workspace
activations
add
and
are
based
be
bias
bound
const
contains
context
cpu
def
default
device
dims
dnnlowp
false
function
generating
given
in
inference
info
input
kind
last
lower
max
min
new
only
op
operator
options
out
output
outputs
param
parameters
params
point
preserve
ptr
qfactory
qparam
quant
quantization
quantized
register
reset
samples
scale
scheme
set
sparsity
specifies
storage
tensor
that
this
type
unique
upper
use
which
wrapper
ws
zero

BASE
BUFFER
BaseType
Box
Buffer
CONV
CPU
Conv
ConvDNNLowPPackedWeightBlob
ConvPoolDNNLowPOpBase
ConvPoolOpBase
ConvRelu
CreateBlob
CreateSharedInt
DNNLOWP
Dequantize
DequantizeInput
Drop
FLAGS
FP
Fp
Get
GetBlob
GetMutable
GetOutputQuantizationParams
GetQuantizationFactoryOf
GetQuantizedOutputData
GetStaticQuantizationParamsOf
HasStaticQuantization
INT
Input
InputIsType
InputSize
InputTensorCPU
Int
IsType
LOG
MUTEX
MeasureQuantizationError
Must
OP
OPENMP
Op
OpWrapper
OperatorDef
Output
OutputTensorCPU
Outputs
POOL
ParseDNNLowPOperatorArguments
PrimInt
PropagateOutputTensorQuantizationParams
QuantizationErrorStats
QuantizationFactory
ROWWISE
ReinitializeTensor
Relu
ReportQuantizationError
RunOnDevice
RunOnDeviceEpilogue
SHARED
Self
String
TODO
Tensor
TensorCPU
TensorOptions
TensorQuantizationParams
This
To
WARNING
Workspace
Y
accuracy
actual
actually
an
and
arguments
as
base
because
before
belong
buffer
but
caffe
call
cfg
col
computation
const
conv
create
debug
declare
def
default
dequantize
dequantized
device
dims
dnn
dnnlowp
doesn
don
drop
duplication
endif
engine
epilogue
error
even
experimental
export
false
fbgemm
feature
first
fixed
followed
force
fp
functions
guard
h
has
here
higher
ident
idx
ifdef
implementation
in
initialized
inplace
input
leave
lock
lowp
macro
max
measure
mostly
mutex
mutexBlob
mutexPtr
namespace
need
needed
never
new
now
nullptr
offset
omp
only
op
openmp
operator
operators
options
original
out
output
override
overwrite
params
parse
parsed
performance
point
pool
precision
ptr
qfactory
qparams
quantization
quantized
really
ref
reference
relies
reset
resize
rules
runWithSharedBuffer
scale
set
shared
slow
some
stats
store
support
temp
temporarily
tensor
tensorcpu
testing
than
their
this
though
threads
type
unique
use
uses
using
values
very
we
will
with
ws
y
zero

BFP
EXPECT
Error
FP
INFO
If
LOG
None
Quant
Relative
TRUE
acurracy
always
be
better
bfp
bit
caffe
case
const
device
diff
dist
distribution
do
doesn
double
error
fabs
fp
hex
ii
infinity
input
isclose
isinf
isrelclose
limits
mantissa
minimum
mse
mt
one
output
overshoot
r
random
range
rd
real
relerr
representable
round
s
scalar
setprecision
should
showbase
size
sqrt
sz
test
testarray
than
total
truncation
type
uniform
usize
v
value
vbfp
vfp
vv
we
y

?
AVX
D
NHWC
Optimized
VLEN
Xdata
Y
Ydata
Yh
add
average
avx
c
cast
channels
clipped
const
constexpr
cvtepi
cvtepu
cvtps
d
dend
depth
dstart
epi
epu
extract
fmadd
fp
h
height
hend
hstart
idx
in
input
intrinsics
kernel
layout
loadl
loadu
loop
m
mask
max
maximum
min
minimum
mm
multiplier
n
nearbyint
out
pad
pd
permute
permutevar
ph
point
pool
pooled
ps
pw
reinterpret
remainder
rounded
scale
set
setzero
shuffle
si
size
storeu
stride
temp
u
uint
using
v
vectorized
w
wend
width
wstart
xdata
xff
ydata
zero

Add
Compute
Construct
DynamicHistogram
EXPECT
Finalize
GetHistogram
HistSimilar
Histogram
INFO
LOG
TRUE
and
begin
compare
const
constexpr
default
distribution
dynamic
element
end
endl
engine
error
errors
generator
hist
histogram
histograms
j
make
max
maximum
min
minimum
n
nbins
new
normal
normalized
ptr
random
reset
result
size
squared
ss
static
str
stringstream
test
unique

ABS
BITS
Compute
DEFAULT
ERR
GetSaturationRegionBegin
IN
MAX
NUM
OUT
Option
Quantized
Self
Sigmoid
Tanh
TensorQuantizationParams
abs
as
assert
bits
compute
computed
different
err
hood
in
input
just
let
mapped
max
new
or
out
output
parameters
params
point
precision
qparams
quantization
scale
sigmoid
sq
tanh
temp
u
under
unwrap
use
we
zero

?
A
API
AVX
Adjust
Allocate
B
BASE
BREAKDOWN
BaseType
Batch
BatchMatMul
BatchMatMulDNNLowPOp
BatchMatMulOp
Both
Box
Bq
Calculate
Choose
ChooseQuantizationParams
ChooseRequantizationMultiplier
Continuing
D
DNNLOWP
DNNLowPOp
DefaultEngine
DequantizeInput
Did
DoNothing
Either
Expected
FUSE
First
For
Fp
GEMM
Get
GetCpuId
GetInputTensorQuantizationParamsOf
GetOutputQuantizationParams
GetQuantizedOutputData
GetSingleArgument
GetWeightPrecision
How
INFO
Ignoring
In
Input
InputIsType
InputTensorCPU
Inputs
Int
IsType
K
LOG
M
MEASURE
MSC
MeasureQuantizationError
Mutually
NoTranspose
OPENMP
OPERATOR
Only
Op
Operator
OperatorDef
Optimize
OutputTensorCPU
PackAWithQuantRowOffset
PackAWithRowOffset
PackBMatrix
ParseDNNLowPOperatorArguments
Perform
Pre
PropagateOutputTensorQuantizationParams
Quantize
QuantizeInputIfNeeded
RELU
ReQuantizeForFloat
ReQuantizeOutput
Requantization
RequantizationParams
Requantize
Resize
RunOnDevice
RunOnDeviceEpilogue
Self
Standard
TIME
TensorCPU
TensorQuantizationParams
That
The
These
This
Transpose
Treat
VER
Vector
WARNING
We
Workspace
Y
Zero
\in
above
actually
added
all
analogous
and
are
as
assert
assign
avx
b
back
base
batch
batched
batches
be
because
begin
bias
bit
blocks
bq
broadcast
broadcasted
broadcasting
buf
buffer
calculate
can
case
cast
cfg
chrono
clear
clock
col
collapse
cols
column
combine
compute
computed
const
constant
count
cpu
debug
def
default
dequantize
device
dim
dimMismatch
dimMismatchErrorString
dimension
dimensional
dimensions
dimnum
dims
dimsLessThan
dnnlowp
doNothingObj
does
don
double
dt
duration
elements
emplace
empty
end
endif
engine
ensure
erase
error
event
example
excluding
exclusive
fact
falls
false
far
fast
fbgemm
fbgemmPacked
first
flag?
flags
flattened
follows
forget
gemm
gemms
getRowOffsetBuffer
gops
group
groups
have
here
id
idx
ifdef
in
increment
indicates
individual
inner
input
insert
instances
integers
into
invocation
ith
itr
j
jth
k
last
leading
len
longer
m
many
match
matrices
matrix
max
measure
min
multiplier
n
namespace
ncols
ndim
ndims
need
new
no
noBroadcastErrorMsg
not
now
nullptr
offset
offsets
omp
one
only
op
operation
operator
optional
or
other
otherwise
our
out
outer
output
outputProcObj
pack
packA
packed
packedBufferSize
parallel
parameters
params
partition
path
per
pmat
point
pointer
pragma
product
push
qfactory
qparams
quant
quantization
quantized
r
range
rbegin
re
rea
real
reason
ref
register
reinterpret
requantization
requires
resize
respecting
respective
respectively
row
rowOffsetBufferSize
s
same
scale
second
set
shapes
should
signed
since
single
size
slices
slow
so
soning
ss
static
step
str
stride
strides
stringstream
sub
suffix
sum
supported
supports
system
taken
temp
tensor
tensors
that
their
this
those
thread
threads
tid
time
trailing
trans
transpose
type
u
uint
unsigned
use
using
value
ve
vec
vectors
view
we
weight
will
with
would
ws
y
you
zero

Compute
Dequantize
EXPECT
GetInputQuantizationParams
GetOutputQuantizationParams
INFO
LE
LOG
NSAMPLES
Quantize
Sigmoid
SigmoidUnitTest
abs
approx
const
default
distribution
double
engine
err
exp
fabs
fbgemm
generator
max
random
real
sigmoid
sq
sqrt
sum
test
uint
uniform
y

Y
cast
const
constexpr
cur
epi
epu
kVLen
loadu
m
max
mm
n
point
r
reinterpret
reluavx
set
si
static
storeu
u
uint
v
y
zero

?
ChooseQuantizationParams
Histogram
KLDivergenceMinimization
Max
Min
TensorQuantizationParams
Usage
argc
argv
assert
atoi
back
bins
cerr
cnt
endl
example
false
file
getline
hist
ifstream
in
index
ist
istringstream
kl
line
max
min
minimization
name
nbins
ofstream
op
out
output
preserve
push
qparams
quanize
server
size
sparsity
tensor
test
type
uint
while

?
ARG
AffineBatchChannelAndRequantizeNCHWAVX
AffineBatchChannelAndRequantizeNHWCAVX
AffineBatchChannelDequantizedNCHW
AffineBatchChannelDequantizedNHWC
AffineBatchChannelQuantizedNCHW
AffineBatchChannelQuantizedNHWC
Arg
BASE
BETA
BaseType
Beta
C
Choose
ChooseQuantizationParams
ChooseRequantizationMultiplier
ComputeDequantizedFusedParams
ComputeQuantizedFusedParams
ComputeQuantizedFusedParamsAVX
ComputeQuantizedInvStd
ConstEigenArrayMap
ConstEigenVectorArrayMap
DNNLOWP
DNNLowPOp
Dequantize
DequantizeInput
DequantizedGroupMomentsNCHW
DequantizedGroupMomentsNHWC
EigenArrayMap
EigenVectorArrayMap
Fp
G
GAMMA
Gamma
Get
GetCpuId
GetInputTensorQuantizationParamsOf
GetOutputQuantizationParams
GetPreserveWeightSparsity
GetQuantizationParameters
GetQuantizedOutputData
GetSingleArgument
GetWeightPrecision
GroupNorm
GroupNormDNNLowPOp
GroupNormFP
GroupNormOp
HxW
INFO
INPUT
Input
InputIsType
InputTensorCPU
Int
InvSigma
InvStd
IsTest
K
LE
LOG
MAX
MIN
MeasureQuantizationError
Moments
Mu
NCHW
NE
OP
OPENMP
OPERATOR
Op
OpSchema
OperatorDef
Output
OutputTensorCPU
ParseDNNLowPOperatorArguments
PropagateOutputTensorQuantizationParams
QUANTIZATION
QuantizationFactory
Quantize
QuantizeBeta
QuantizeGamma
QuantizeGammaImpl
QuantizeInputIfNeeded
QuantizedGroupMomentsNCHW
QuantizedGroupMomentsNHWC
Requantize
ResizeLike
Reverse
RunOnDevice
RunOnDeviceWithOrderNCHW
RunOnDeviceWithOrderNHWC
SINGLE
Self
StorageOrder
StringToStorageOrder
TensorCPU
TensorQuantizationParams
UNKNOWN
VectorMomentsAVX
Workspace
Y
abs
affine
arr
avoid
avx
base
batch
beta
bias
c
cached
cast
channel
colwise
compute
const
constant
context
cpu
def
default
dequantize
dequantized
dequantizednchw
dequantizednhwc
device
dim
dims
dnnlowp
endif
engine
epsilon
error
false
fbgemm
from
fused
gamma
group
hxw
ifdef
in
inner
input
internal
inv
j
k
loop
math
mean
measure
momentsnchw
momentsnhwc
mu
multiplier
n
ndim
new
nullptr
omp
operator
order
ordernchw
ordernhwc
out
outer
output
outputs
overriding
parallel
param
parameters
params
point
pragma
precision
ptr
qfactory
qparams
quantization
quantize
quantized
quantizednchw
quantizednhwc
real
ref
register
requantization
resize
right
round
rowwise
rsig
scale
shift
sig
size
square
static
stride
sum
sumsq
tags
temp
test
this
transpose
type
u
use
var
weight
with
ws
y
zero

?
ChooseQuantizationParams
ErrorMinimization
Histogram
Max
Min
TensorQuantizationParams
Usage
argc
argv
assert
atoi
back
bins
cerr
cnt
emplace
endl
example
false
file
getline
hist
hists
ifstream
in
index
infos
ist
istringstream
line
max
min
minimization
name
nbins
ofstream
omp
op
out
output
parallel
pragma
precision
preserve
push
qparams
quantize
server
size
sparsity
tensor
test
tuple
type
uint
while

Box
DNNLOWP
GetActivationPrecision
GetInputTensorQuantizationParamsOf
GetMutable
GetQuantizationFactoryOf
GetStaticQuantizationParamsOf
HasStaticQuantization
Input
InputSize
Int
IsType
OPERATOR
Operator
OperatorDef
OperatorStorage
Outputs
ParseDNNLowPOperatorArguments
PropagateOutputTensorQuantizationParams
QuantParamsBlob
QuantizationFactory
Quantize
QuantizeDNNLowPOp
ROWWISE
ResizeLike
Self
TensorCPU
TensorQuantizationParams
Workspace
and
arguments
base
blob
caffe
const
context
cpu
def
default
device
dnnlowp
engine
false
fbgemm
identical
in
input
namespace
new
operator
out
output
outputs
parsed
point
precision
ptr
qfactory
qparam
qparams
register
scale
shape
this
type
u
unique
use
using
with
ws
zero

BASE
BaseType
BatchPermutation
BatchPermutationDNNLowPOp
BatchPermutationFP
Choose
CopyOp
DNNLOWP
DNNLowPOp
Even
FIXME
GT
GetInputTensorQuantizationParamsOf
INDICES
INPUT
Indices
Input
InputTensorCPU
Int
K
OPENMP
OPERATOR
OUTPUT
Op
OperatorDef
Output
OutputTensorCPU
ParseDNNLowPOperatorArguments
PropagateOutputTensorQuantizationParams
ResizeLike
Self
Workspace
Y
as
base
batch
be
because
chosen
const
cpu
d
def
device
dim
dnnlowp
endif
engine
equal
ifdef
ignored
in
indices
input
memcpy
must
namespace
ndim
new
omp
operator
origIdx
output
outputs
parallel
parameters
params
permutation
permuteIdx
pragma
pre
qfactory
qparams
quantization
register
same
should
sizeof
tags
there
this
type
u
use
using
vs
with
ws

CDF
ChooseQuantizationParams
Default
ErrorMinimization
Exploit
Faster
Filter
GetDefaultInstance
GetNorm
Histogram
NormMinimization
NormMinimizationKind
Option
Org
P
QuantizationErrorMinimization
QuantizationFactory
SCALE
SMALL
Self
Some
THRESHOLD
TensorQuantizationParams
Using
VLOG
accumulate
adjust
alpha
and
approx
approximate
back
base
begin
beginning
belong
best
beta
bin
bins
bounds
break
calculate
center
choose
continue
cout
covers
decide
default
delta
density
distances
distributions
dnnlowp
double
dst
end
endl
entirely
enum
errors
experiment
falls
false
find
finished
first
floor
found
from
granularity
hist
histogram
in
include
input
j
kind
left
let
lowerbound
max
min
minimal
move
nbins
new
next
nonlinear
norm
or
org
out
outliers
over
params
precision
preserve
push
qfactory
quantile
quantization
quick
range
right
round
scale
search
selected
size
sparsity
src
start
step
stepsize
sum
symmetric
threshold
to?
total
trait
uint
unwrap
upperbound
use
which
while
width
with
within
zero

C
HxW
OPENMP
ReluFused
SpatialBNNHWCAVX
Y
add
alpha
avx
beta
bn
c
cast
clamp
clipped
const
constexpr
cur
cvtepi
cvtepu
cvtps
down
endif
epi
extract
false
fbgemm
fmadd
fused
hxw
ifdef
in
j
kVLen
loadl
loadu
long
lrintf
m
mask
max
min
mm
n
nhwc
omp
out
outer
parallel
permute
permutevar
point
pragma
ps
ptr
quantized
r
reinterpret
relu
result
rounded
set
shuffle
size
spatial
static
sub
u
uint
v
xff
y
zero

Compute
DNNLOWP
GetOutputQuantizationParams
Int
Self
Tanh
TanhFunctor
TensorQuantizationParams
UnaryElementwiseWithArgsDNNLowPOp
const
cpu
engine
invoke
n
new
operator
output
params
quantization
register
tanh
u
with
y

?
@note
BVLC
Baseline
C
DNHWC
Fast
From
G
GEMMs
H
IncreaseIndexInDims
Intel
IsAGeZeroAndALtB
Loop
Note
OPENMP
Q
R
S
THNN
The
This
Torch
W
Y
accumulate
an
and
are
axes
axis
b
be
bottom
c
caffe
can
channel
channels
col
colNCHW
colNHWC
cols
column
com
compute
const
context
copy
d
dilation
dimension
dkernel
dst
efficiently
endif
equal
false
forward
frame
frames
github
groups
h
height
https
ifdef
ih
im
image
img
in
index
indices
inner
input
iter
iw
iy
j
k
kernel
kh
kw
layout
left
lies
memcpy
multiplies
n
nchw
nd
next
nip
no
offset
omp
order
out
outer
output
over
pad
padded
padding
parallel
path
per
pixel
point
pragma
previous
pull
pulled
r
rest
result
reverse
right
row
rows
s
shape
should
simply
size
sizeof
so
spatial
src
stride
temp
that
top
unfolded
use
utils
w
we
whether
width
with
y
zero

ABS
Activation
BITS
Choose
DEFAULT
Dequantize
ERR
Efficient
Function
GetPassRegionEnd
GetSaturationRegionBegin
Hyperbolic
IEEE
IN
INFO
Implementation
Integration
LOG
Large
MAX
Mirhassani
NUM
Networks
Neural
OUT
Option
PRINT
Scale
Self
Systems
TABLE
TANH
Tangent
Tanh
TensorQuantizationParams
The
Transactions
VLSI
Very
We
Zamanlooy
abs
adjusted
an
and
approach
approximated
as
assert
assuming
atanh
average
begin
bit
bits
break
cast
ceil
compute
const
default
dequantized
described
divided
double
end
endif
err
fabs
fbgemm
floor
ifdef
in
index
input
integer
into
largest
let
log
lut
mag
map
mapped
max
min
multiplier
nearbyint
new
or
out
output
parameters
params
pass
per
point
pos
pq
precision
processing
qmax
qparams
quantization
range
ranges
region
resize
s
saturation
scale
sgn
smallest
sq
static
sub
tanh
u
uint
unsigned
unwrap
use
used
val
value
with
y
zero

ChooseQuantizationParams
Histogram
Max
Min
P
TensorQuantizationParams
Usage
argc
argv
assert
back
bins
cerr
cnt
endl
example
file
getline
hist
ifstream
in
index
ist
istringstream
line
max
min
name
nbins
ofstream
op
out
output
push
qparams
quanitze
size
tensor
test
type
uint
while

?
ErrorMinimization
Histogram
Max
Min
NonlinearQuantizationParamsSearch
TensorQuantizationParams
Usage
VLOG
approx
argc
argv
assert
atoi
back
bins
cerr
cnt
emplace
endl
example
false
file
getline
hist
hists
ifstream
in
index
infos
ist
istringstream
line
max
min
minimization
name
nbins
ofstream
op
out
output
precision
preserve
push
qparams
quantize
server
size
sparsity
tensor
test
tuple
type
uint
while

Compute
Dequantize
EXPECT
For
GetInputQuantizationParams
GetOutputQuantizationParams
GetPassRegionEndDequantized
GetSaturationRegionBegin
INFO
LE
LOG
NSAMPLES
Quantize
Tanh
TanhUnitTest
abs
approx
bit
can
const
default
distribution
double
down
endl
engine
err
fabs
fbgemm
generator
in
max
out
pq
random
real
sq
sqrt
sum
tanh
test
uint
uniform
we
y

?
@return
A
AVX
Acc
Adjust
AdjustOutputTensorQuantizationParamsWithFollowedBy
B
BASE
BIAS
BREAKDOWN
BaseType
Bias
Buffer
C
CHANNEL
CHW
CONV
CPU
Cannot
Choose
ChooseQuantizationParams
ChooseRequantizationMultiplier
Col
ColNCHW
ColNHWC
ColNdNCHW
ComputeColumnOffsets
Conv
ConvDNNLowPOp
ConvDNNLowPPackWeightOp
ConvDNNLowPPackedWeightBlob
ConvFp
ConvNHWCCore
ConvOp
ConvPoolDNNLowPOpBase
ConvPoolOpBase
ConvRelu
Convolution
Convolutional
Create
CreateSharedInt
D
DAGNet
DNHWC
DNNLOWP
DParam
DPartitionOf
DequantizeInput
DispatchFBGEMM
DoNothing
Dump
FC
FILTER
FIXME
FLAGS
FastPath
Filter
FilterQuantizationParams
For
Fp
From
G
GEMM
GRAN
GROUP
Get
GetConv
GetConvParam
GetCpuId
GetDims
GetDimsSize
GetInputTensorQuantizationParamsOf
GetOutputQuantizationParams
GetOutputSize
GetQuantizationParameters
GetSingleArgument
GetStaticQuantizationParamsOf
H
HWC
HasStaticQuantization
HxW
INFO
INPUT
IS
If
Im
In
Input
InputBlob
InputIsType
InputSize
InputTensorCPU
Int
IsConvGEMM
IsType
It
K
KernelDim
LE
LOG
M
MEASURE
MSC
Make
MeasureQuantizationError
NCHW
NHWC
NoIm
NoTranspose
Normal
ON
OPENMP
OPERATOR
OUT
Only
Op
OpenMP
OperatorDef
Output
OutputTensorCPU
POOL
PackAMatrix
PackAWithIm
PackAWithRowOffset
PackBMatrix
PackWeightMatrixForGConv
PackedDepthWiseConvMatrix
ParseDNNLowPOperatorArguments
Pre
PreComputeRowColumnOffsets
PrimInt
PropagateOutputTensorQuantizationParams
Q
QuantizationGranularity
Quantize
QuantizeBias
QuantizeWeight
R
ReQuantizeOutput
Relu
ReluFused
RequantizationParams
Requantize
Resize
RunOnDevice
RunOnDeviceEpilogueNCHW
RunOnDeviceEpilogueNHWC
RunWithSharedBuffer
Running
S
SIMD
See
Self
SetDeviceTensor
Share
Shouldn
StorageOrder
StoreMatrixInMatrixMarketFormat
TENSOR
TIME
TODO
TakeDepthWise
TakeGConvFastPath
Tensor
TensorCPU
TensorQuantizationParams
The
This
Transpose
VER
VLOG
W
WARNING
We
When
Workspace
Wq
Xdata
Y
Ydata
abs
acc
accumulate
across
act
activation
activations
actually
add
already
an
and
anymore
are
arguments
as
assert
assign
asymmetric
avoid
avx
b
back
base
basically
batch
because
been
beforehand
begin
best
bias
biases
bit
both
buf
buffer
but
caffe
call
called
can
cast
cblas
cc
cfg
channel
channels
choose
chosen
chrono
clock
col
colnhwc
column
compute
computed
condition
const
constructor
context
conv
convgemm
convolution
convolutions
copy
core
correctly
corresponding
count
cpu
create
createSharedBuffer
d
debug
declare
def
define
defined
depth
depthwise
dequantize
device
dilation
dim
dimension
dims
dispatchfbgemm
divisible
dnnlowp
do
doNothingObj
does
doesn
don
done
double
dt
dump
duplication
duration
during
dynamic
empty
end
endif
enforced
engine
epiloguenchw
epiloguenhwc
equal
error
even
except
executed
falls
false
fast
fbgemm
fbgemmGroupwiseConv
fbgemmOptimizedGConv
fbgemmPacked
filename
filling
filter
find
first
fold
folded
followed
force
fp
frames
from
function
fuse
fused
fusion
gconv
gemm
getRowOffsetBuffer
goes
gops
group
grouped
groups
groupwise
h
has
have
height
here
id
ifdef
im
image
img
in
inplace
input
insert
inside
instructions
integer
integers
intermediate
internal
into
iteration
j
k
kernel
layer
layout
ld
log
m
make
match
math
matmul
matrix
max
means
measure
measured
min
modify
ms
multi
multiple
multiplier
multipliers
multiplies
multiply
must
mutex
name
namespace
ndim
need
new
nhwc
no
non
not
now
npos
nthreads
nullptr
number
numbers
occurences
offset
offsets
old
omp
once
one
only
op
operator
operators
ops
or
order
ordernchw
ordernhwc
out
output
outputProcObj
pack
packA
packW
packa
packed
packedBufferSize
pad
padding
pads
parallel
parallelization
param
parameters
params
parsed
partition
pass
path
per
performs
pmat
point
points
pragma
pre
private
prologue
provides
ptr
push
qfactory
qparams
quant
quantization
quantize
quantized
r
race
raw
real
reason
reduction
ref
reference
region
register
reinterpret
replace
reqObj
requantization
requantize
rescaling
reset
resize
ret
row
rowOffsetBufferSize
rowOffsetBufferSizeGConv
s
same
scale
setting
shape
shared
should
signed
single
size
slow
small
so
something
spatial
special
static
storage
stored
stride
sum
support
supported
supports
sure
swap
system
tags
take
takeg
temp
tensor
tensors
that
then
this
thread
threading
threads
tid
time
times
type
typename
u
uint
unsigned
use
used
using
usize
value
values
ve
very
void
w
way
we
weight
weights
well
while
why
width
wise
with
within
without
works
wrong
ws
y
zero

?
@param
ACC
Adjust
Assume
B
BASE
BIAS
Bias
Blob
Bq
C
CONV
CPU
Cannot
Check
ChooseQuantizationParams
ColPtr
CompressedSparseColumn
ComputeColumnOffsets
Conv
ConvDNNLowPAcc
ConvDNNLowPOp
ConvDNNLowPPackWeightOp
ConvDNNLowPPackedWeightBlob
ConvDNNLowpPackedWeightBlobShapeFunctions
ConvFp
ConvOp
ConvPackWeight
ConvPoolDNNLowPOpBase
CopySameDevice
CountOutliers
Create
DATATYPE
DNNLOWP
DNNLowPOp
DParam
DataType
Default
DefaultEngine
Density
DeviceOption
Explicitly
ExternalTensorDescriptor
ExternalTensorFunctionsBase
ExtractOutlierMatrix
FC
FCDNNLowPPackedWeightBlob
FCDNNLowpPackedWeightBlobShapeFunctions
FCDefaultTransposeWeight
FCFp
FCPackWeight
FILTER
FIXME
FLAGS
Falling
FastPath
Filter
FullyConnectedDNNLowPPackWeightOp
FullyConnectedOp
Get
GetConv
GetConvParam
GetCpuId
GetRaw
GetSingleArgument
GetTensorInfo
GetWeightPrecision
HasSingleArgumentOfType
Helper
INFO
INT
Id
If
In
InputBlob
InputIsType
InputSize
InputTensorCPU
Int
IsType
K
KRSC
LE
LOG
M
MAX
MIN
Make
NOTE
ONNXIFI
OPERATOR
Only
Op
OperatorDef
Option
Output
OutputSize
POOL
Pack
PackBMatrix
PackWeightMatrixForGConv
PackWeightOp
PackedDepthWiseConvMatrix
Per
Pre
Prepack
Proportion
QuantizationFactory
Quantize
QuantizeBias
QuantizeConvBias
QuantizeWeight
ROWWISE
RefImplementations
Register
ReinitializeTensor
Resize
ResizeLike
RoundToFloat
RowIdx
SIGNED
SKYLAKE
Save
See
Self
Set
Should
Skylake
Store
THRESHOLD
THROW
TODO
TakeDepthWise
TakeGConvFastPath
Tensor
TensorCPU
TensorProto
TensorQuantizationParams
TensorShape
The
Then
This
Transpose
Type
TypeIdentifier
TypeMeta
UINT
Unsupported
Values
W
WARNING
We
Weight
When
Workspace
Wq
Y
abs
acc
accelerator
accumulate
accumulation
activation
actually
after
all
allocate
also
always
an
and
are
args
argument
as
assign
avx
axis
b
back
base
basically
bdata
be
because
before
begin
benefits
bias
biases
bit
blob
buffer
but
c
caffe
called
calling
can
canonical
capacity
cast
cbegin
cc
cend
cfg
change
changed
channel
channels
channelwise
check
choose
clamp
clear
cnt
col
cols
column
compute
computed
const
constexpr
consumed
context
conv
convenient
core
count
cpu
cur
d
dataType
debug
declare
def
demand
density
depth
depthwise
desc
descriptor
device
dilation
dim
dimensions
dims
dnnlowp
dnntensor
do
don
double
due
dummy
duplication
during
elsewhere
emplace
end
engine
even
everything
external
extract
fact
fake
fall
fallback
false
fast
faster
fbgemm
fbgemmOptimizedGConv
filter
first
forced
format
fp
from
front
function
functions
gconv
going
group
groups
groupwise
h
happen
have
here
higher
id
implemented
in
index
inference
info
information
init
input
instead
invocations
isOffline
j
just
k
kONNXIFI
kernel
know
known
layer
layout
let
like
limitation
load
local
log
m
make
matrix
max
meaningful
means
meant
memcpy
memory
meta
might
min
move
multiple
multiplies
n
name
nbits
nearest
need
net
new
non
not
now
nullptr
number
occurences
offline
offset
offsets
once
only
onnxifi
op
operator
operators
ops
optionally
or
original
out
outlier
outliers
output
outputing
outputs
pack
packed
packing
pad
pads
param
part
pass
path
per
performance
pmat
point
populate
pre
predictor
provide
provided
pt
purpose
push
qfactory
qparam
qparams
quant
quantization
quantizationAxis
quantizationParams
quantize
quantized
r
reason
ref
register
reinterpret
reset
resize
ret
reuse
right
round
row
rows
s
same
save
saving
scale
scales
second
set
setup
shape
shapes
sharing
should
shouldn
signed
since
single
size
sizeof
smaller
so
space
specify
static
still
stride
sum
support
synced
tags
take
takeg
tensor
than
that
their
this
threshold
transposed
type
typename
u
uint
unpacked
unsigned
unwrap
up
use
used
using
usize
v
void
w
way
we
weight
weights
will
wise
with
work
ws
yet
zero

?
A
B
BASE
BinaryElementwiseDNNLowPOp
BinaryElementwiseOp
C
Choose
ChooseRequantizationMultiplier
ComputeLegacyBroadcastSizes
DNNLOWP
Dimension
GetInputTensorQuantizationParamsOf
GetOutputQuantizationParams
GetQuantizationParameters
GetQuantizedOutputData
In
InputTensorCPU
Int
Mul
MulDNNLowPOp
MulFp
MulFunctor
Num
OPENMP
OPERATOR
Op
OperatorDef
OutputTensorCPU
Quantize
QuantizeInputIfNeeded
Requantize
ResizeLike
RunOnDeviceEpilogue
Self
Workspace
allowed
and
axis
base
broadcast
broadcasting
const
cpu
def
device
did
elementwise
enable
endif
engine
false
fbgemm
first
forget
ifdef
in
j
k
mismatch
multiplier
n
needed
new
omp
only
operator
ops
out
parallel
parameters
params
place
point
post
pragma
pre
qfactory
qparams
quantization
quantized
raw
real
register
requantization
scale
set
size
temp
tensor
this
tie
type
u
using
utils
with
ws
you
zero

?
ChooseQuantizationParams
ErrorMinimization
Histogram
Max
Min
TensorQuantizationParams
Usage
VLOG
argc
argv
assert
atoi
back
bins
cerr
cnt
emplace
endl
example
false
file
getline
hist
hists
ifstream
in
index
infos
ist
istringstream
line
max
min
minimization
name
nbins
ofstream
op
out
output
precision
preserve
push
qparams
quantize
server
size
sparsity
tensor
test
tuple
type
uint
while

?
@PERF
ACC
AVX
Adjust
AdjustOutputTensorQuantizationParamsWithFollowedBy
Arc
BASE
BREAKDOWN
BaseType
Both
CHANNEL
Caffe
Calculate
Choose
ChooseRequantizationMultiplier
ComputeColumnOffsets
CopyItemsSameDevice
CostInferenceForFC
DETAILED
DNNLOWP
DNNLowPOp
DefaultEngine
Dequantize
DequantizeInput
Dequantized
DoNothing
Dump
Expose
FC
FCDNNLowPPackedWeightBlob
FCDefaultTransposeWeight
FCRelu
FCShapeInference
FLAGS
Falling
Fp
From
FullyConnectedDNNLowPOp
FullyConnectedOp
Get
GetActivationPrecision
GetCpuId
GetInputTensorQuantizationParamsOf
GetOrCreateFbgemmPackBMatrix
GetOutputQuantizationParams
GetQuantizationParameters
GetQuantizedOutputData
GetSigned
GetSingleArgument
GetWeightPrecision
IN
INFO
IS
If
Input
InputBlob
InputIsType
InputSize
InputTensorCPU
Int
IsType
K
LE
LOG
M
MEASURE
MeasureQuantizationError
NoTranspose
ON
OPERATOR
OUT
Only
Op
OperatorDef
Output
OutputSize
OutputTensorCPU
PATH
PackAMatrix
PackAWithQuantRowOffset
PackAWithRowOffset
PackBMatrix
ParseDNNLowPOperatorArguments
Pre
PropagateOutputTensorQuantizationParams
QuantParamsBlob
QuantizationGranularity
Quantize
QuantizeInputIfNeeded
QuantizeWeight
ROWWISE
ReQuantizeForFloat
ReQuantizeOutput
Relu
ReluFused
RequantizationParams
Requantize
Resize
ResizeLike
RunOnDevice
SIMD
SLOW
See
Self
StoreMatrixInMatrixMarketFormat
TIME
TensorCPU
TensorQuantizationParams
Transpose
Type
VLOG
W
WARNING
Wdata
We
When
Workspace
Wq
Xdata
Y
Ydata
abs
activation
an
and
anymore
are
arguments
as
assert
assign
asymmetric
attached
available
avx
axis
b
back
base
batch
be
because
begin
bias
biases
bind
bit
blob
buf
buffer
cache
caffe
can
canonical
capacity
cast
cc
channelwise
chrono
clock
cnt
col
column
compute
computed
const
constant
context
copy
cost
count
cpu
currently
debug
debugging
declare
def
default
define
dequantize
dequantized
device
dim
dnnlowp
doNothingObj
doesn
don
double
dt
dump
duration
dynamic
element
empty
end
endif
enforce
enforced
engine
error
even
falls
false
fast
fbgemm
fbgemmPacked
filter
first
fit
flag
fold
folded
followed
force
fp
from
function
fuse
gemm
getRowOffsetBuffer
gops
group
groups
have
here
honored
id
ifdef
implementation
in
index
inference
initialize
inplace
input
inside
instead
instruction
instructions
integer
integers
internal
into
invocation
its
j
k
ld
log
make
matmul
matrix
max
means
measure
min
modify
ms
multiplier
multipliers
multiply
namespace
need
new
not
now
nullptr
occurences
offset
offsets
old
only
op
operator
operators
ops
option
or
out
output
outputProcObj
outputs
overflow
overflowed
overwrite
own
pack
packA
packed
packedBufferSize
param
parameters
params
parsed
path
point
points
populated
pre
precision
provides
ptr
qfactory
qparam
qparams
quant
quantization
quantize
quantized
raw
real
reason
ref
register
reinterpret
requantization
requantize
resize
row
rowOffsetBufferSize
s
same
scale
scales
selected
set
shape
shared
shrink
signed
size
slow
so
static
sum
supports
swap
system
temp
tensor
tensors
that
then
this
thread
threads
time
type
typename
u
uint
underflow
underflowed
unique
unsigned
use
used
uses
using
usize
value
vec
very
w
way
we
weight
weights
why
with
wq
ws
y
zero

Arc
CPU
CompressedSparseColumn
Conv
ConvDNNLowPPackedWeightBlob
DNNLOWP
Dense
FC
FCDNNLowPPackedWeightBlob
Int
Only
PackBMatrix
PackWeightMatrixForGConv
PackWeightMatrixForGConvB
Packed
PackedDepthWiseConvMatrix
Sparse
Tensor
TensorQuantizationParams
The
acc
accumulation
base
before
bias
bit
but
column
common
d
default
depthwise
gconv
holding
in
information
matrix
meta
nbits
non
offsets
only
operator
original
outlier
outliers
packing
qparams
tensor
use
values
w
weight
with

?
Add
An
BINNING
Box
Dynamic
DynamicHistogram
FACTOR
GetHistogram
Histogram
Indicate
Max
Min
OVER
Option
RemapHistograms
Self
The
This
add
adjusting
an
and
any
assert
based
begin
beginning
bin
bins
c
cast
ceil
change
cnt
collect
collects
com
const
contains
continue
corresponds
count
curr
current
dst
dynamically
end
equi
exactly
exception
expands
extremum
fastest
final
finalize
first
from
hist
histogram
histograms
https
implemented
in
indexing
invariant
ith
last
len
let
limits
lowest
make
mapped
matches
max
maximum
min
minimum
more
most
my
nbins
new
not
nullptr
old
or
over
questions
re
remainder
remap
reset
rint
round
see
series
should
single
size
spread
src
stackoverflow
static
th
that
thread
time
u
uint
unique
unwrap
values
version
way
we
width
with
zero

?
Box
DNNLOWP
Dequantize
DequantizeDNNLowPOp
DequantizeRowWise
GetInputTensorQuantizationParamsOf
GetQuantizationFactoryOf
Input
InputIsType
Int
IsType
LOG
OPERATOR
Operator
OperatorDef
OperatorStorage
Output
QuantizationFactory
ROWWISE
ResizeLike
Self
TensorCPU
TensorQuantizationParams
WARNING
Workspace
accuracy
an
and
const
context
cpu
debug
def
device
dnnlowp
engine
experimental
fbgemm
feature
fixed
higher
identical
in
input
mostly
namespace
new
operator
outputs
performance
point
precision
qfactory
qparams
register
shape
slow
storage
testing
than
this
type
u
using
very
with
ws

A
DNNLOWP
DynamicHistogram
ENGINE
FindMinMax
HasDNNLowPEngine
HashMap
IsType
NetBase
NetObserver
ObserverBase
OperatorDef
OperatorStorage
PREFIX
String
TensorCPU
Useful
are
base
c
case
collects
const
copies
debug
def
dnn
engine
fbgemm
find
from
global
has
in
j
lazy
len
long
lowp
map
max
maxf
min
multiple
name
network
new
nullptr
op
orig
ref
resize
same
size
static
str
strncmp
table
temp
tensor
that
there
use

?
All
Axis
BASE
BaseType
CPU
ChooseRequantizationMultiplier
Concat
ConcatDNNLowPOp
ConcatOp
CopyMatrix
D
DNNLOWP
DNNLowPOp
DPartitionOf
Expect
GE
Get
GetDimFromOrderString
GetInputTensorQuantizationParamsOf
GetOutputQuantizationParams
GetQuantizationParameters
GetQuantizedOutputData
GetSingleArgument
HasArgument
Input
InputSize
InputTensorCPU
Int
IsType
LT
NCHW
OPENMP
OPERATOR
OperatorDef
Output
OutputSize
OutputTensorCPU
Quantize
RequantizationParams
Requantize
Resize
RunOnDeviceEpilogue
Self
Tensor
The
Workspace
Y
add
after
along
and
are
arg
axis
base
before
begin
but
can
cast
channels
char
check
compatible
const
context
continue
copy
cpu
def
device
different
dim
dimension
dimensions
dims
dnnlowp
end
endif
engine
expected
fbgemm
got
have
ifdef
in
input
insert
j
math
multiplier
must
name
namespace
ndim
new
not
nthreads
nullptr
number
offset
omp
only
operator
order
out
output
parallel
parameters
params
point
pragma
qfactory
qparams
quantization
range
real
register
reinterpret
requantization
resize
same
scale
size
sizeof
split
stored
temp
tensors
this
thread
threads
tid
type
u
use
using
vec
vs
with
ws
zero

?
D
DPartition
First
GE
Get
GetWorkPerThread
If
In
LT
Option
Optionally
Otherwise
When
align
any
assigned
be
begin
can
case
ceil
columns
d
end
floor
force
handled
just
let
m
max
min
multiple
n
nthreads
number
only
or
over
parallelize
parallelized
partition
partitioned
partitioning
per
row
rows
single
size
some
this
thread
threads
tid
tie
try
unwrap
use
usize
we
within
won
work
works

Bool
ChooseQuantizationParams
ChooseQuantizationTest
Combine
EXPECT
ErrorMinimization
FLOAT
GetParam
Histogram
INFO
InstantiationName
LOG
Max
Min
MinimizationTest
Range
TensorQuantizationParams
TestWithParam
assert
back
begin
bins
case
chrono
clock
cnt
const
count
default
double
dt
duration
end
error
hist
histogram
id
includes
input
instantiate
ist
istringstream
max
min
minimizing
nbins
now
point
precision
preserve
push
qparam
quantization
rage
scale
sec
size
sparsity
static
str
system
takes
test
testing
tie
time
uint
use
zero

?
Input
QuantUtilsAvx
ReluFused
See
VLEN
acc
add
aligned
and
assumes
b
but
c
cast
cc
clamped
const
cvtepi
cvtps
details
element
epi
epu
fbgemm
fmadd
function
in
input
instruction
j
len
loadl
m
mask
max
min
mm
more
nearbyint
output
packed
packs
packus
permute
permutevar
point
ps
reinterpret
requantizeOutputProcessingAvx
rounded
scale
sequence
set
setzero
si
so
src
static
storeu
sub
subtract
sumavx
then
this
transformed
u
uint
v
val
w
we
wise
xy
xyzw
y
z
zero
zw

?
@return
AddArgument
AdjustOutputTensorQuantizationParamsWithFollowedBy
Already
ArgumentHelper
Box
CPU
ChooseQuantizationParams
DNNLOWP
Dequantize
ENGINE
FC
FLAGS
FindMinMax
Get
GetActivationKind
GetActivationPrecision
GetInputQuantizationParams
GetMutable
GetPreserveActivationSparsity
GetQuantizationFactoryOf
GetSingleArgument
GetStaticQuantizationParamsOf
GetWeightKind
HasDNNLowPEngine
HasSingleArgumentOfType
HasStaticQuantization
Histogram
IF
INFO
If
Input
InputIsType
Int
IsType
LOG
Let
MAX
MIN
Max
Measure
Need
NetDef
Operator
OperatorDef
OperatorStorage
Option
Output
OutputArgumentIdxString
OutputScaleArgumentName
OutputZeroPointArgumentName
Outputs
P
PREFIX
PropagateOutputTensorQuantizationParams
QUANTIZATION
Qparams
Quantization
QuantizationErrorStats
QuantizationFactory
QuantizationKind
Quantize
Relu
SetStaticQuantizationParams
Should
Sigmoid
Skip
String
StringToKind
Symmetric
Tanh
Tensor
TensorCPU
TensorQuantizationParams
Unknown
UseStaticQuantization
VLOG
WARNING
When
Y
about
abs
activation
actual
actually
add
adjust
already
an
and
arg
args
argument
arguments
as
assigned
assigning
avoid
back
based
be
been
beg
bins
but
c
caffe
can
cares
check
chosen
clear
cnt
collected
comparing
const
constant
consumer
consumers
continue
debug
def
dequantize
distribution
distributions
dnn
dnnlow
dnnlowp
doesn
doing
dynamic
eltwise
engine
err
error
exiting
fabs
factory
false
fbgemm
file
first
floats
fly
followed
force
format
fp
from
getline
has
helper
hist
histogram
idx
ifstream
in
index
input
invalid
invoked
ist
istringstream
j
just
kind
know
len
let
limited
line
looking
lowp
make
match
max
measure
method
min
more
multiplier
name
nan
nbins
needed
net
new
not
number
nwords
offset
one
only
op
operation
operator
operators
optionally
or
output
outputs
parameter
params
parse
passed
pengine
per
performance
point
power
precisely
precision
preserve
producer
profiling
propagate
ptr
push
qfactory
qparams
quantization
quantize
quantized
range
re
recommended
ref
reference
regardless
region
relative
report
requantization
resize
resulted
results
row
rows
rowwidth
s
scale
seekg
selection
sense
set
shared
should
size
sparsity
specified
sq
sqrt
ss
stat
static
store
str
stringstream
strncmp
sum
temp
tensor
th
that
threshold
type
u
uint
unique
unwrap
use
used
uses
usize
values
ve
was
we
weight
while
will
wise
with
without
word
zero

BASE
BaseType
C
Choose
DNNLOWP
DNNLowPOp
Even
GT
GetInputTensorQuantizationParamsOf
GetSingleArgument
IH
IW
InputTensorCPU
Int
NHWC
OH
OPENMP
OPERATOR
OW
Op
OperatorDef
OutputTensorCPU
ParseDNNLowPOperatorArguments
PropagateOutputTensorQuantizationParams
Resize
ResizeNearest
ResizeNearestDNNLowPOp
ResizeNearestFP
ResizeNearestOp
Self
StorageOrder
StringToStorageOrder
Workspace
Y
as
base
be
because
chosen
const
cpu
def
device
dim
dnnlowp
endif
engine
height
ifdef
ignored
in
input
memcpy
min
n
namespace
ndim
nearest
new
omp
operator
order
output
parallel
parameters
params
pragma
pre
qfactory
qparams
quantization
register
resize
same
scale
should
sizeof
there
this
type
u
use
using
width
with
ws
y

BatchRequantizationUnitTest
Bits
ChooseQuantizationParams
ChooseRequantizationMultiplier
Dequantize
Dest
EXPECT
GEMM
GetDefaultInstance
INFO
LE
LEN
LOG
Make
NITER
Precise
QuantizationFactory
Quantize
Requantization
RequantizationParams
RequantizationUnitTest
Requantize
Rescaling
Source
TensorQuantizationParams
Test
This
We
actual
an
and
any
avoid
be
because
begin
bigger
bit
but
can
cast
ceil
clamp
clipping
common
constexpr
correctly
cycle
default
determined
dis
dist
distribution
don
double
dst
due
elements
end
endl
eng
engine
err
error
example
expected
exponent
extend
fabsf
fbgemm
floor
gen
generate
handled
has
have
in
include
input
intentionally
j
long
max
min
most
multiplier
nearbyint
negative
not
number
offset
one
orig
output
parameter
params
per
plus
point
powf
pre
precision
proportion
qfactory
qparams
quantization
random
range
ranges
rdtsc
real
requantization
results
right
round
saturation
scale
scaling
shift
should
shouldn
signed
sq
sqrt
src
static
sum
sure
test
than
uint
uniform
unsigned
use
used
value
values
vary
which
will
with
zero
~

BASE
BaseType
Choose
ChooseQuantizationParams
ChooseRequantizationMultiplier
D
DNNLOWP
DNNLowPOp
DefaultEngine
ElementwiseLinear
ElementwiseLinearDNNLowPOp
ElementwiseLinearFp
ElementwiseLinearOp
GetInputTensorQuantizationParamsOf
GetOutputQuantizationParams
GetQuantizationParameters
GetQuantizedOutputData
GetSingleArgument
InputTensorCPU
Int
OPENMP
OPERATOR
Op
OperatorDef
OutputTensorCPU
Quantize
QuantizeInputIfNeeded
RequantizationParams
Requantize
ResizeLike
RunOnDeviceEpilogue
Self
Workspace
Y
axis
b
base
canonical
const
cpu
d
def
device
dim
dnnlowp
empty
endif
engine
false
fbgemm
from
ifdef
in
index
multiplier
n
namespace
ndim
new
omp
operator
out
parallel
parameters
params
point
pragma
qfactory
qparams
quantization
quantized
raw
real
register
requantization
resize
scale
signed
size
temp
this
type
u
using
weight
with
ws
zero

?
ArrayView
ArrayViewMut
AveragePool
AveragePoolDnnLowPOp
AveragePoolFp
AveragePoolFunctor
BASE
BaseType
CONV
ConvPoolDNNLowPOpBase
ConvPoolOpBase
DNNLOWP
DequantizeInput
Do
Even
Fp
Get
GetInputTensorQuantizationParamsOf
GetOutputQuantizationParams
GetOutputSize
GetQuantizedOutputData
InputTensorCPU
Int
MaxPool
MaxPoolDnnLowPOp
MaxPoolFp
MaxPoolFunctor
OPENMP
OPERATOR
Op
OperatorDef
OutputTensorCPU
POOL
Pad
ParseDNNLowPOperatorArguments
PoolOp
Pooling
PrimInt
Quantize
QuantizeInputIfNeeded
RunOnDevice
RunOnDeviceEpilogue
Self
THROW
The
Unsupported
Workspace
Xdata
Y
Ydata
Yh
as
average
avx
base
be
because
break
c
case
cast
channel
channels
chosen
col
const
cpu
cwiseMax
d
def
default
dend
depth
device
dilation
dim
dnnlowp
does
dstart
endif
engine
error
false
finalize
global
h
height
hend
hstart
idx
ifdef
ignored
image
in
index
initialize
input
kernel
limits
loop
lowest
main
mat
max
maximum
maxpool
measure
min
minimum
multiplier
n
namespace
ndarray
ndim
nearbyint
needed
new
not
now
offset
omp
op
operator
orderNCHW
orderNHWC
out
output
pad
pads
parallel
parameters
pd
ph
point
pool
pooled
pooling
pragma
pre
precision
process
pw
qfactory
qparams
quantization
ref
register
reinterpret
right
same
scale
should
signed
size
smaller
stride
support
switch
temp
than
there
this
type
u
uint
use
using
value
w
wend
width
with
ws
wstart
xdata
y
ydata
zero

?
Box
C
CELL
CPU
Cdata
ChooseQuantizationParams
ChooseRequantizationMultiplier
Compute
Ctemp
D
DNNLOWP
Dequantize
DequantizeInput
Drop
Extract
Fp
G
GATES
Gates
Get
GetInputQuantizationParams
GetInputTensorQuantizationParamsOf
GetMutable
GetOutputQuantizationParams
GetQuantizationFactoryOf
GetQuantizationParameters
GetSingleArgument
GetStaticQuantizationParamsOf
H
HIDDEN
HasStaticQuantization
Hdata
Htemp
Input
InputIsType
InputTensorCPU
Int
LENGTHS
LSTMUnit
LSTMUnitDNNLowPOp
LSTMUnitOp
M
Max
MeasureQuantizationError
Min
Op
OpWrapper
OperatorDef
OperatorStorage
Output
OutputTensorCPU
Outputs
ParseDNNLowPOperatorArguments
PrimInt
PropagateOutputTensorQuantizationParams
QuantizationErrorStats
QuantizationFactory
Quantize
QuantizeInputIfNeeded
ReportQuantizationError
RequantizationParams
Requantize
ResizeLike
RunOnDevice
SEQ
Self
Sigmoid
TIMESTEP
Tanh
Tensor
TensorCPU
TensorQuantizationParams
Workspace
an
arguments
as
back
base
bias
c
cast
cell
const
cpu
d
def
default
dequantize
device
dnnlowp
drop
engine
error
false
fbgemm
first
forget
fp
h
hidden
higher
host
idx
in
input
last
lengths
lstm
max
measure
min
multiplier
n
namespace
needed
new
o
often
only
op
operator
or
out
output
parameters
params
parsed
point
prev
qfactory
qparams
quantization
quantized
real
register
requantization
rescaled
reset
resize
resolution
scale
seq
seqLengths
sigmoid
signed
since
size
states
static
stats
tanh
temp
tensorcpu
than
this
times
u
uint
unit
use
used
using
valid
with
ws
xNxG
zero

Block
C
DataType
DefaultEngine
Do
Engine
Expilictly
FIRST
FLOAT
FbGemmPack
FbGemmPackOp
FbGemmPackTranspose
GetSingleArgument
Input
K
LOG
NoTranspose
OPERATOR
Operator
OperatorDef
OperatorStorage
Output
PackedGemmMatrixFP
Prepack
Self
TPacked
TensorProto
TensorShape
Transpose
TransposeWeight
TypeMeta
WARNING
Workspace
Y
allow
alpha
axis
back
be
caffe
canonical
class
col
const
context
cpu
def
default
deprecated
device
dim
dims
false
fbgemm
format
from
function
in
index
inference
inplace
known
layout
major
mat
matSize
matrix
memcpy
move
new
no
not
only
op
operator
out
outputs
pack
packed
packing
phantomA
phantomB
pmat
ptr
push
register
reset
resultPtr
row
set
size
sizeof
soon
src
storage
tensor
testing
this
type
typename
unique
unpackFromSrc
use
usize
vec
w
weight
will
ws

Align
IEEE
NOTE
Results
add
and
be
bfp
broadcast
but
c
cast
clang
const
constexpr
convert
current
cvtph
cvtps
dest
different
easier
epi
exponent
format
formatting
fp
irange
ld
load
loadu
m
mantissa
mask
maskload
masks
maskstore
mm
off
offset
ph
preserving
ps
read
reducing
reinterpret
round
scalar
set
should
si
sign
size
source
ss
st
storeu
truncation
use
usize
v
vin
wants
wmask
woffset
xFFFC
xFFFF
xFFFFFF

M
Storing
TODO
Transpose
ab
after
are
b
bit
but
byte
c
cd
const
constexpr
d
dst
epi
even
everything
fbgemm
general
granularity
hi
in
integers
interleaving
j
lanes
lddqu
lo
loop
m
making
matrix
mm
more
move
n
odd
order
permute
permuted
remainder
routine
row
rows
scalar
si
so
src
storeu
that
this
transpose
u
unpackhi
unpacklo
unsigned
vectorized
with
xN
y

ChooseQuantizationParams
DataType
DefaultEngine
Engine
Expilictly
FLOAT
GenQuantParams
GenQuantParamsOp
Generate
GetDefaultInstance
Input
Int
OPERATOR
Operator
OperatorDef
OperatorStorage
Output
QuantParamsBlob
QuantSchemeBlob
QuantizationFactory
Self
String
StringToKind
TensorProto
TensorQuantizationParams
TensorShape
The
TypeMeta
Workspace
activations
add
and
based
caffe
const
contains
context
cpu
def
device
dim
dims
dnnlowp
function
generating
given
in
inference
info
input
kind
known
last
new
operator
options
or
out
output
outputs
param
parameters
params
point
preserve
ptr
qfactory
qparam
quant
quantization
register
reset
samples
scale
scheme
set
sparsity
specifies
storage
tensor
that
this
type
unique
use
wrapper
ws
zero

Apply
Blob
BlobGetMutableTensor
Box
CHECK
ConvOp
ConvPoolOpBase
ConvRelu
ConvReluOp
CostInferenceForConv
CostInferenceFunctionType
CreateBlob
Delegate
GetBlob
GetDeviceType
GetRaw
InputSize
Inputs
NOTNULL
OPENMP
OpSchema
Operator
OperatorDef
Output
Relu
RunOnDeviceWithOrderNCHW
RunOnDeviceWithOrderNHWC
Self
ShareExternal
Tensor
TensorInferenceForConv
Workspace
back
base
blobs
cast
const
conv
cost
cpu
def
device
endif
false
function
ifdef
inference
input
local
max
meta
name
new
omp
op
operator
ordernchw
ordernhwc
output
outputs
parallel
pragma
push
register
reset
static
tensor
this
use
void
with
ws

ChooseQuantizationParams
CpuId
FakeLowP
GetCpuId
GetDefaultInstance
GetNorm
Glow
Histogram
Look
MinimizationKernelAVX
NNPI
NormMinimization
NormMinimizationKind
OPENMP
QuantizationFactory
SCALE
SMALL
THRESHOLD
TensorQuantizationParams
This
Use
Using
VLOG
accumulate
adjust
and
are
around
as
assert
assume
avx
be
because
begin
beginning
belong
best
bin
bins
brute
c
caffe
center
check
choose
combination
consistent
const
covers
cpuid
cutoff
d
delta
density
distances
distributed
dnnlowp
don
dst
dynamic
end
endif
entirely
err
error
errors
every
first
floorf
fma
force
fp
from
hist
histogram
ifdef
implementation
in
include
integral
internal
irange
kind
leads
left
mapping
max
min
nbins
norm
omp
one
only
or
over
pair
parallel
params
pick
pragma
precision
preserve
preserving
qfactory
quantization
range
right
round
scale
scales
schedule
search
second
selected
size
small
smallest
sparsity
src
start
starting
subnormal
sum
symmetric
this
to?
total
uniformly
use
values
want
we
which
width
with
within
zero

A
Default
ErrorMinimization
NormMinimization
Self
base
bin
bins
default
dst
error
kernelavx
minimization
minimizes
nbins
norm
precision
quantization
scheme
start
that
use
width

Compute
DNNLOWP
GetOutputQuantizationParams
Int
Self
Sigmoid
SigmoidFunctor
TensorQuantizationParams
UnaryElementwiseWithArgsDNNLowPOp
const
cpu
engine
invoke
n
new
operator
output
params
quantization
register
sigmoid
u
with
y

?
ARG
BASE
BaseType
C
ChannelShuffle
ChannelShuffleDNNLowPOp
ChannelShuffleFp
ChannelShuffleOp
Choose
ConstEigenMatrixMap
DNNLOWP
DNNLowPOp
EigenMatrixMap
Even
G
GetCpuId
GetInputTensorQuantizationParamsOf
GetSingleArgument
GxK
HxW
InputTensorCPU
Int
K
NCHW
NE
OP
OPENMP
OPERATOR
Op
OperatorDef
OutputTensorCPU
ParseDNNLowPOperatorArguments
PropagateOutputTensorQuantizationParams
ResizeLike
RunOnDeviceWithOrderNCHW
RunOnDeviceWithOrderNHWC
SINGLE
Self
StorageOrder
StringToStorageOrder
TensorQuantizationParams
Transpose
UNKNOWN
Workspace
Y
as
avx
axes
base
be
because
block
cast
channel
chosen
const
context
cpu
def
device
dim
dims
dnnlowp
endif
engine
fbgemm
from
group
ifdef
ignored
in
input
j
mat
math
matrix
namespace
ndim
new
omp
operator
order
orderNCHW
orderNHWC
output
parallel
parameters
params
pragma
pre
qfactory
qparams
quantization
register
reinterpret
rows
same
should
shuffle
size
stride
there
this
transpose
type
u
uint
use
using
value
with
ws

?
A
Add
AddDNNLowPOp
AddFp
AddFunctor
B
BASE
BinaryElementwiseDNNLowPOp
BinaryElementwiseOp
C
ChooseQuantizationParams
ChooseRequantizationMultiplier
ComputeLegacyBroadcastSizes
DNNLOWP
DPartition
Dimension
ElementWiseSumAVX
Find
Get
GetCpuId
GetEltwiseQuantizePrecision
GetInputTensorQuantizationParamsOf
GetOutputQuantizationParams
GetPreserveActivationSparsity
GetQuantizationParameters
GetQuantizedOutputData
In
InputSize
InputTensorCPU
Int
IsType
Max
Min
NOTE
Num
OPENMP
OPERATOR
Op
OperatorDef
OutputTensorCPU
Quantize
ReluFused
RequantizationParams
Requantize
ResizeLike
RunOnDeviceEpilogue
SameTypeAsInput
Self
So
TensorQuantizationParams
VLEN
Workspace
addition
all
allowed
and
are
assert
avx
axis
base
begin
broadcast
broadcasting
const
constexpr
cpu
def
device
did
different
dnnlowp
does
elementwise
enable
end
endif
engine
everything
false
fast
fbgemm
first
fixed
floating
fma
forget
global
ifdef
in
input
intermediate
internal
j
k
limits
lowest
max
min
mismatch
multiplier
n
needed
new
numerically
omp
only
operator
ops
out
parallel
parameters
params
path
place
point
post
pragma
pre
qfactory
qparams
quantization
quantized
raw
real
register
requantization
same
scale
set
size
slow
tensor
that
they
this
thread
threads
tie
type
u
uint
unlike
using
utils
value
with
ws
you
zero

AVX
AdjustOutputTensorQuantizationParamsWithFollowedBy
BREAKDOWN
BaseType
Can
ChooseQuantizationParams
ChooseRequantizationMultiplier
DNNLOWP
DPartition
Element
ElementWiseSumAVX
FMA
Find
First
Get
GetCpuId
GetEltwiseQuantizePrecision
GetInputTensorQuantizationParamsOf
GetOutputQuantizationParams
GetPreserveActivationSparsity
GetQuantizationParameters
GetQuantizedOutputData
INFO
INT
InputSize
InputTensorCPU
Int
IsType
It
LOG
MAX
MEASURE
Max
MeasureQuantizationError
Min
NOTE
OPENMP
OperatorDef
Output
OutputTensorCPU
ParseDNNLowPOperatorArguments
Quantize
Relu
ReluFused
RequantizationParams
Requantize
ResizeLike
RunOnDeviceEpilogue
Same
Self
So
Sum
SumDNNLowPOp
SumRelu
TIME
VLEN
Workspace
acc
actually
addition
all
allow
and
are
arguments
as
avx
be
begin
but
can
chrono
clock
const
constexpr
correctly
count
cpu
cross
def
dequantize
device
devices
different
dimension
dnnlowp
does
double
dt
duration
end
endif
engine
error
everything
false
fast
fbgemm
final
fixed
floating
fma
followed
from
fused
global
have
identical
ifdef
in
inplace
input
intermediate
internal
j
len
limits
lowest
make
max
measure
measured
min
ms
multiplier
namespace
new
not
now
numerically
omp
operator
out
output
outputs
parallel
parameters
params
parsed
path
point
pragma
precision
prologue
qfactory
qparams
quant
quantization
raw
real
register
requantization
requantize
s
same
scale
setting
shape
size
slow
sum
support
sure
system
tensor
tensors
that
they
this
thread
threads
tie
time
type
u
uint
unlike
using
value
vec
we
wise
with
ws
zero

ARG
Args
Array
BinaryElementwiseDNNLowPOp
ConstEigenArrayMap
ConstEigenVectorArrayMap
DNNLowPOp
Do
Dynamic
Eigen
EigenArrayMap
EigenVectorArrayMap
FP
Figure
For
Functor
Get
GetMutable
GetOutputQuantizationParams
Input
Int
Map
NCHW
NE
OP
OPERATOR
Operator
OperatorDef
OperatorStorage
Outputs
ParseDNNLowPOperatorArguments
PropagateOutputTensorQuantizationParams
R
RequantizationParams
ResizeLike
Run
RunWithBroadcast
SINGLE
Self
SetStaticQuantizationParams
String
TensorCPU
UnaryElementwiseWithArgsDNNLowPOp
Unrecognizable
Unsupported
Workspace
an
and
argument
arguments
arithmetic
axis
b
base
be
broadcast
broadcasting
cannot
colwise
const
context
correct
declare
def
default
device
dnnlowp
eigen
enable
enabled
even
explicit
export
false
find
from
functor
good
ident
index
input
macro
n
name
new
not
npos
op
operator
operators
or
order
out
output
params
parsed
post
pre
provides
requantization
rowwise
rules
scalar
semantic
semantically
simultaneously
size
specify
storage
str
this
type
typename
use
used
vectorize
void
way
ws

BASE
Box
CopyItemsSameDevice
D
DATA
DNNLOWP
DNNLowPOp
DequantizeInput
DispatchHelper
Drop
Fp
GE
GPU
Gather
GatherDNNLowPOp
GatherOp
Get
GetInputTensorQuantizationParamsOf
GetMutable
GetOutputQuantizationParams
GetQuantizationFactoryOf
GetStaticQuantizationParamsOf
HasStaticQuantization
INDICES
If
Index
Input
InputIsType
Int
O
OPERATOR
Op
OpWrapper
OperatorDef
Output
Outputs
ParseDNNLowPOperatorArguments
PrimInt
PropagateOutputTensorQuantizationParams
QuantizationErrorStats
QuantizationFactory
Quantize
ReluFused
ReportQuantizationError
RequantizationParams
Resize
ResizeLike
RunOnDevice
Self
SumDNNLowPOp
SumOp
TF
TODO
TensorCPU
TensorQuantizationParams
TensorTypes
Workspace
already
arguments
b
base
be
begin
best
block
bounds
bytesize
c
call
cast
char
const
context
cpu
def
default
delegate
dequantize
device
dim
dnnlowp
do
does
doing
drop
element
end
endup
engine
error
false
fbgemm
fp
from
id
idx
idxs
implement
in
indices
input
insert
intermediate
itemsize
least
len
mattering
measure
memcpy
namespace
ndim
new
not
op
operator
or
out
output
params
parsed
point
points
prefetching
probably
qfactory
qparams
quantization
raw
register
requantization
reset
s
scale
shape
should
size
src
starts
static
stats
sumavx
this
type
u
using
vec
we
wise
with
ws
zero

DataType
DefaultEngine
Engine
Flag
GetSingleArgument
Int
MAX
MIN
OPERATOR
Operator
OperatorDef
OperatorStorage
Output
QUANTIZATION
QuantSchemeBlob
QuantSchemeBlobFill
QuantSchemeBlobFillOp
STRING
Self
TensorProto
TensorShape
The
Workspace
add
and
args
be
blob
const
context
cpu
def
device
dims
false
function
generate
generating
given
in
inference
kind
new
not
operator
options
or
out
output
outputs
param
params
preserve
ptr
qscheme
quant
quantization
register
reset
scheme
set
sparsity
specifies
storage
tensor
that
this
type
unique
use
used
would
wrapper
ws

?
A
Add
B
BFLOAT
BFP
Caffe
CblasNoTrans
CblasTrans
Compute
DB
DCHECK
DW
DX
DY
DataType
DefaultEngine
Dimension
DoRunWithType
Engine
Error
FAKE
FC
FCGradient
FLOAT
FP
FullyConnectedFakeLowpFPOp
FullyConnectedGradientFakeLowpFPOp
Gemm
Gemv
GetDeviceType
GetSingleArgument
IEEE
INFO
If
InnerProductOp
Input
K
LE
LOG
M
MATH
Math
OPERATOR
Operator
OperatorDef
OperatorStorage
Output
OutputSize
Q
Quantize
ROUND
ReinitializeTensor
ResizeLike
Run
Self
Set
Tensor
TensorProto
This
To
TransposeWeight
W
Wh
Workspace
Xh
Y
an
and
as
axis
b
batch
better
bfp
bh
bias
bits
blob
c
cache
can
canonical
checking
chop
class
computation
compute
const
context
convert
cpu
dB
dW
dX
dY
dYh
db
def
default
dest
device
dim
dimErrorString
do
does
don
empty
engine
every
exponent
extra
false
fc
fill
fits
fp
from
generate
grad
h
helper
index
invariant
its
least
local
log
mantissa
math
mismatch
multiplier
name
need
new
nlines
not
object
occurences
one
operator
output
preserving
purpose
recreate
reducing
register
reshape
resize
rest
round
rounding
running
s
scalar
shape
significant
size
skip
so
source
static
storage
str
term
that
this
time
truncation
type
use
usize
vec
w
we
with
ws
y
zero

OperatorDef
Output
RunOnDevice
Self
SpatialBNOp
SpatialBNRelu
SpatialBNReluOp
Workspace
allow
base
context
cpu
def
device
enforce
false
inplace
max
new
operator
output
outputs
register
size
use
vec
ws

?
ACC
AVX
Arc
BASE
BIAS
BREAKDOWN
BaseType
C
CHW
CONV
CPUs
CSC
Check
Col
ColNCHW
ColNHWC
ColNdNCHW
CompressedSparseColumn
Consider
Conv
ConvDNNLowPAcc
ConvDNNLowPOp
ConvDNNLowPPackedWeightBlob
ConvOutlier
ConvRelu
Convolution
CountOutliers
DETAILED
DNNLOWP
Dense
Density
DispatchFBGEMM
DoNothing
DoSpmdmOnInpBuffer
ExtractOutlierMatrix
FILTER
FIRST
FLAGS
Falling
For
G
GEMM
GRAN
GROUP
Get
GetCpuId
GetDims
GetDimsSize
GetOutputSize
GetQuantizationParameters
GetSingleArgument
HWC
IN
INFO
INPUT
If
Im
In
Input
InputIsType
InputSize
InputTensorCPU
Int
Intel
K
KernelDim
LE
LOG
M
MEASURE
MIN
Main
MeasureQuantizationError
NCHW
NHWC
NoIm
NoTranspose
NumOfRows
OPENMP
Only
Op
OpenMP
OperatorDef
OperatorStorage
Outlier
OutputTensorCPU
PATH
POOL
PackAMatrix
PackAWithRowOffset
PackBMatrix
PartitionGroupedNHWCConv
Pre
PropagateOutputTensorQuantizationParams
Proportion
Q
QuantizationGranularity
Quantized
R
ReQuantizeOutput
ReluFused
Resize
RunOnDeviceEpilogueNCHW
RunOnDeviceEpilogueNHWC
RunOnDeviceWithOrderNCHW
RunOnDeviceWithOrderNHWC
RunWithSharedBuffer
Running
S
SKYLAKE
SLOW
Self
Separate
SetDeviceTensor
Short
Skylake
SpMDM
StorageOrder
TENSOR
THRESHOLD
TIME
Tensor
The
This
Threshold
Thresholds
Transpose
VLOG
W
WARNING
We
Workspace
Wq
Xdata
Y
acc
accumulate
accumulation
again
already
an
and
anyway
are
as
assert
assign
available
avx
aware
b
back
base
be
because
before
begin
benefits
bias
bit
blob
block
buf
buffer
but
caffe
call
called
can
cast
cfg
channels
chrono
clock
cnt
col
column
const
constexpr
context
conv
convolution
copy
corresponding
count
cpu
cut
d
debug
decide
declare
def
default
define
density
device
dilation
dim
dimension
dims
dispatchfbgemm
divisible
dnnlowp
do
doNothingObj
does
doesn
double
dt
due
duration
empty
encounter
end
endif
engine
enough
equal
everything
example
fallback
falling
falls
false
fast
faster
fbgemm
fbgemmPacked
filter
first
folded
followed
format
frequency
function
fusion
gemm
getRowOffsetBuffer
give
gops
group
groups
groupwise
h
have
height
higher
id
ifdef
im
image
img
in
initialize
input
insert
instead
into
invocation
j
k
kernel
know
layer
layout
ld
lier
like
ll
log
m
main
make
match
math
matrix
max
means
min
ms
much
multiplication
multipliers
n
namespace
nbits
ndim
need
new
nhwc
no
non
not
nothing
now
nthreads
nullptr
number
occurences
offset
offsets
omp
only
op
operator
ops
or
order
ordernchw
ordernhwc
out
outType
outlier
outliers
output
overflow
overflowed
overhead
pack
packA
packW
packa
packed
packedBufferSize
pad
pads
parallel
parameters
params
path
per
performance
pmat
point
points
pragma
prepacked
private
prologue
properly
provide
push
qparams
quant
quantization
quantize
quantized
r
re
reason
ref
region
register
reqObj
requantization
reset
resize
row
rowOffsetBufferSize
saturation
shape
shared
should
since
single
size
slow
smaller
so
sparse
spatial
specified
speedup
spmdmObj
static
storage
stored
stride
sum
supported
supports
sure
swap
system
temp
than
things
this
thread
threads
threshold
tid
time
times
too
type
typename
u
uint
underflow
underflowed
use
using
very
w
we
weight
well
whether
width
will
with
within
won
works
wq
ws
y
zero

?
ARG
Adjust
AdjustOutputTensorQuantizationParamsWithFollowedBy
BASE
BIAS
Bias
C
ComputeFusedParam
ConstEigenVectorArrayMap
DNNLOWP
DNNLowPOp
EST
EigenVectorArrayMap
EstMean
EstVar
FEATURE
GE
GT
GetCpuId
GetDeviceType
GetInputTensorQuantizationParamsOf
GetOutputQuantizationParams
GetQuantizedOutputData
GetSingleArgument
HxW
INPUT
Input
InputTensorCPU
Int
It
MEAN
MeasureQuantizationError
NCHW
NCHW\
NE
NEEDS
NHWC\
Note
OP
OPERATOR
OUTPUT
OperatorDef
Output
OutputSize
OutputTensorCPU
ParseDNNLowPOperatorArguments
QuantizeInputIfNeeded
ReinitializeTensor
Relu
ReluFused
Resize
RunOnDeviceEpilogue
SCALE
SINGLE
Scale
Self
SpatialBN
SpatialBNDNNLowPOp
SpatialBNNHWCAVX
SpatialBNOp
SpatialBNRelu
StorageOrder
StringToStorageOrder
Tensor
UNKNOWN
VAR
Workspace
Y
\
actually
alpha
and
are
argument
arguments
arr
assumes
avx
base
be
beta
bias
bn
but
c
cbegin
cend
clamp
compute
considering
const
correctly
cpu
def
dequantize
device
dim
dims
dnnlowp
double
down
either
engine
epsilon
error
false
fbgemm
followed
fp
from
fused
hxw
implementation
in
inference
input
internal
j
long
lrintf
make
max
mean
measure
measured
n
ndim
new
nhwc
not
only
op
operator
or
order
out
output
outputs
param
parsed
point
qfactory
qparams
quantization
quantized
register
relu
rsqrt
s
scale
scales
setting
should
size
so
spatial
still
sure
tags
temp
test
this
u
var
with
works
ws
y
zero

?
ElementWiseSumAVX
GB
LEN
argc
argv
atoi
avx
b
bench
benchmark
bytes
c
caffe
chrono
clock
count
cout
double
dt
duration
elementwise
endl
false
internal
now
point
quantize
s
server
size
sizeof
sum
system
time
uint

?
@note
A
APPROX
Add
Adjust
Allowed
Choose
ChooseQuantizationParams
ChooseRequantizationMultiplier
Default
ErrorMinimization
FLAGS
FindMinMax
Get
GetActivationKind
GetActivationPrecision
GetHistogram
GetPreserveActivationSparsity
GetPreserveWeightSparsity
GetWeightKind
GetWeightPrecision
Given
Histogram
INFO
KL
KLDivergenceMinimization
Kullback
LOG
Leibler
MAX
MIN
Max
Min
Minimizes
Multiplication
NonlinearQuantizationParamsSearch
Option
P
Pad
Parse
Precision
QUANTIZATION
Quantization
QuantizationFactory
QuantizationKind
Represents
RequantizationParams
RoundingRightShift
Self
So
String
StringToKind
Take
TensorQuantizationParams
Use
When
accumulated
accumulation
activation
activations
addition
additional
adjust
after
all
amount
an
and
approx
approximate
approximated
arithmetic
assert
aware
based
be
begin
best
bin
bins
bit
bits
caffe
can
case
cast
ceil
choose
collection
considered
const
copy
critical
default
define
determined
determines
device
distribution
divergence
dnnlowp
double
during
element
elementwise
eltwise
end
enum
error
errors
factory
false
fast
fbgemm
few
first
fixed
floating
force
frequency
from
full
generate
gflags
granularity
handled
hist
histogram
how
ignoring
in
include
input
instance
integer
intermediate
just
kind
kl
len
let
like
linear
log
looking
lower
main
mapped
maps
matmul
max
method
min
more
multiplication
multiplier
multipliers
nbins
nbits
need
needs
network
networks
neural
new
non
norm
not
number
numbers
offline
offset
often
omp
only
operations
optimize
option
or
out
outlier
outliers
pair
parameter
params
pass
path
percentage
percentail
performance
point
policy
positive
power
practice
precision
preserve
preserving
printed
produce
produces
provides
qparams
quantization
quantize
quantized
range
real
remove
represented
representing
requantization
rescaling
restrict
right
runtime
s
scale
scales
scaling
scheme
search
select
set
shift
should
signed
simple
singleton
size
slow
so
some
sparsity
specified
specifies
spill
static
sufficient
switch
symmetric
tensors
that
this
threads
threshold
tolower
transform
uint
unwrap
use
used
value
values
we
weight
weights
which
whose
width
wise
with
workload
works
zero
zeros

?
ComputeEqualizationScale
ComputeEqualizationScaleOp
CreateTensorShape
DataType
FLOAT
Generate
GetDimsVector
Given
Input
K
M
Operator
OperatorDef
OperatorStorage
Output
RunOnDevice
S
Scale
Self
TensorProto
TensorShape
The
W
WcolMax
Workspace
XcolMax
abs
activations
and
based
be
columns
compute
computed
const
context
cpu
def
device
dim
equalization
equalize
from
function
given
idx
in
inference
input
j
last
matrix
max
multiplied
new
operator
or
out
output
outputs
parameter
register
s
samples
scale
shape
size
sqrt
storage
tensor
that
use
want
we
weight
will
with
ws

A
Add
B
Blob
Box
C
CPU
Caffe
Call
CblasNoTrans
CostInferenceForFC
CostInferenceFunctionType
DATATYPE
DCHECK
DataType
DefaultEngine
DeviceOption
Dimension
DoRunWithType
Engine
Error
ExternalTensorDescriptor
ExternalTensorFunctionsBase
FC
FCShapeInference
FLOAT
FbFCPacked
FbFCPackedOperator
FbGemmPack
Fbgemm
Gemm
Get
GetDeviceType
GetFbgemmTensorInfo
GetSingleArgument
Half
Id
If
InitFbgemmContext
InitializeFbgemm
Input
K
LE
Load
M
Make
NoTranspose
ONNXIFI
OPERATOR
OpSchema
Operator
OperatorDef
OperatorStorage
Output
PROTO
PackedGemmMatrixFP
Register
RegisterTensorInfoFunction
Resize
Run
Same
Self
Set
ShapeFunctions
Suppose
Tensor
TensorProto
This
To
Transpose
TypeIdentifier
TypeMeta
W
Workspace
Y
accumulation
all
an
and
as
axis
b
back
batch
bias
bind
blob
buffer
but
c
cache
caffe
can
canonical
capacity
cast
cblas
cfg
checking
class
computation
compute
const
context
convert
cost
cpu
dataType
def
default
desc
descriptor
device
dim
dimErrorString
dimensions
do
don
emplace
empty
every
external
false
fbgemm
fill
follows
fp
from
function
functions
gemm
generates
half
has
helper
id
in
index
inference
info
init
initialize
input
interface
invariant
isOffline
load
local
make
mat
matSize
math
matrix
meta
mismatch
move
multiplier
need
net
new
no
not
nullptr
numCols
numRows
object
offline
offsets
one
op
operator
output
outputs
packed
params
phantomA
phantomB
pmat
predict
prepacked
ptr
quantization
quantizationParams
quantized
recreate
register
reinterpret
reshape
resize
rest
same
scales
set
setup
shape
shapes
size
skip
so
src
static
still
storage
str
tc
tensor
term
that
this
time
type
typename
u
uint
unique
unused
use
used
usize
vec
void
w
we
weight
with
wrapper
ws
y
your

AVX
C
G
HxW
INIT
K
OPENMP
REQUANTIZE
RequantizationParams
Requantize
SegmentMomentsAVX
ULL
Y
ab
add
affine
and
arr
avx
b
batch
beta
bias
blend
c
cast
channel
clipped
compute
const
constexpr
cur
cvtepu
do
dst
endif
epi
even
export
extract
false
fbgemm
fused
gamma
hxw
ident
ifdef
init
j
k
kOneInt
kVLen
ll
loadl
loadu
m
macro
madd
mask
max
min
mm
moments
mu
mul
mullo
multiplier
n
nhwc
nudge
odd
offset
omp
outer
parallel
params
paramsAVX
permute
permutevar
point
post
pragma
prev
ptr
qparams
quantized
r
reinterpret
requantize
requantizenchwavx
requantizenhwcavx
result
right
rounded
rsig
rules
scale
segment
set
setzero
shift
shuffle
si
size
slli
src
srli
static
storeu
sub
sum
sumsq
u
uint
use
v
while
xaa
xff
y
zero

A
ChooseQuantizationParams
GetDefaultInstance
GetHistogram
Histogram
KL
KLDivergenceMinimization
Kullback
Leiber
Look
Max
Min
OPENMP
Option
QuantizationErrorMinimization
QuantizationFactory
TensorQuantizationParams
VLOG
and
assert
base
begin
best
bin
bins
ceil
choose
combination
const
continue
corresponds
covers
destination
divergence
double
dst
dyn
end
endif
every
false
first
fraction
hist
ifdef
kl
left
length
let
log
mapping
max
min
minimizes
nbins
non
not
omp
one
or
outliers
pair
parallel
params
pick
pragma
precision
preserve
qfactory
quantization
quantized
range
right
round
rounded
scheme
second
selected
size
smallest
sparsity
src
start
sum
that
total
uint
unwrap
use
value
width
with
zero

FILE
MatrixMarket
String
c
cast
column
const
d
d\n
dumped
end
f\n
fclose
find
fopen
format
fp
fprintf
general\n
in
insert
integer
integral
j
m
major
market
matrix
mtx
n
name
names
namespace
npos
order
pos
real
rfind
set
size
static
store
str
substr
type
uses
using
value
w

ACC
Arc
Create
If
Note
PackBMatrix
TODO
This
an
and
any
are
as
bandwidth
c
cache
cached
can
cfg
cheaper
compare
computing
const
copies
create
deduplicate
different
during
end
entry
equals
existing
expensive
fbgemm
fdata
find
groups
guard
hash
here
hit
improved
itr
key
ld
lock
m
make
many
map
matrix
memory
metaEquals
miss
mutex
n
nets
new
nullptr
one
only
op
operator
or
orig
otherwise
packb
packed
pmat
ptr
purpose
quantized
release
reuse
s
same
save
second
share
shared
sharing
static
tensors
that
there
this
threads
trans
tuple
usage
use
void
weak
weight
with

DoRunWithType
Input
IsType
M
OPERATOR
OperatorDef
Output
Self
Sum
SumOp
SumRelu
SumReluOp
THROW
Workspace
and
base
bit
but
cast
cpu
def
device
do
double
false
input
ints
max
name
new
only
operator
output
register
size
static
supports
type
use
was
with
ws

Blob
BlobGetMutableTensor
Box
CHECK
CPU
ChooseQuantizationParams
CreateBlob
Dequantize
FIXME
FindMinMax
Get
GetBlob
GetInputTensorQuantizationParamsOf
GetRaw
GetSingleArgument
Input
InputIsType
InputSize
Inputs
Int
NOTNULL
OpType
OpWrapper
OperatorDef
OperatorStorage
Option
QuantizationFactory
Relu
ResizeLike
Self
ShareExternal
TensorCPU
TensorQuantizationParams
This
Workspace
Wrap
against
back
before
bias
blobs
cast
class
const
contained
container
context
debug
def
dequantize
device
dnnlowp
doesn
error
example
fbgemm
floating
followed
fp
in
index
input
let
loading
local
max
measure
measuring
meta
min
model
name
namespace
new
op
operator
option
or
out
output
params
point
push
qfactory
qtensor
quantization
quantize
quantized
re
reference
reset
running
shadow
shouldn
so
tensor
type
unwrap
use
using
void
we
with
work
ws

BASE
BaseType
C
Choose
D
DDNNLowPOp
DFP
DNNLOWP
DNNLowPOp
DOp
Even
GT
GetInputTensorQuantizationParamsOf
GetSingleArgument
IF
IH
IW
InputTensorCPU
Int
NHWC
OF
OH
OPENMP
OPERATOR
OW
Op
OperatorDef
OutputTensorCPU
ParseDNNLowPOperatorArguments
PropagateOutputTensorQuantizationParams
Resize
ResizeNearest
Self
StorageOrder
StringToStorageOrder
Workspace
Y
as
base
be
because
buffer
chosen
const
cpu
def
device
dim
dnnlowp
endif
engine
frames
height
ifdef
ignored
in
input
memcpy
min
n
namespace
ndim
nearest
new
omp
operator
order
output
parallel
parameters
params
pragma
pre
qfactory
qparams
quantization
register
resize
same
scale
shape
should
sizeof
temporal
there
this
type
u
use
using
width
with
ws
y
