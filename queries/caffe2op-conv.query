hello! 

I am writing a rust crate called caffe2op-conv. 

It defines a mathematical operator used in DSP
and machine learning computations.

I would like you to help me write a rust crate
description for it.

Please write the description in the followin format:

<short description tag used in the crate header>
<double newline>
<longer, in depth description and mathematical analysis>
<END>

Please indicate that the crate is in the
process of being translated from C++ to
Rust. It is possible some of the function
bodies are in the process of translation.

Below are listed some tokens from this crate. 

Please describe the mathematical ideas
identified by this information alongside the
relevant mathematical equations in unicode.

thanks!

please format your response in the Markdown file format.

here are the tokens:
ConvTransposeUnpoolBase
adj_h
adj_w
compute_size_and_pad
kernel_h
kernel_w
pad_b
pad_l
pad_r
pad_t
run_on_device
run_on_device_with_orderNCHW
run_on_device_with_orderNHWC
stride_h
stride_w
conv_doc_generator
EigenConvOp
run_on_device_with_orderNHWC
run_on_device_with_order_nchw
GetConvTransposeGradient
get_gradient_defs
register_cpu_operator
register_gradient
register_cpu_operator_with_engine
CudnnConvOpBase
drop
duplicate_conv_desc
given
set_conv_desc_compute_type
set_tensor_nd_descriptor_with_group
ConvTransposeMobileOp
register_cpu_operator_with_engine
CudnnConvTransposeOpBase
drop
set_tensor_4ddescriptor_with_group
run_on_device_with_order_nchw
run_on_device_with_order_nhwc
tensor_inference_for_conv_gradient
reinterleave_multithreaded
ConvPoolOpBase
OpSchema
allocate_and_copy
compute_pads
compute_size_and_pad
compute_size_and_pad64
cost_inference_for_conv
dilation_h
dilation_w
get_dims
get_dims_size
has_pad
has_stride
kernel_h
kernel_w
pad_b
pad_l
pad_r
pad_t
run_on_device
run_on_device_with_orderNCHW
run_on_device_with_orderNHWC
set_bias_multiplier
set_device_tensor
stride_h
stride_w
tensor_inference_forLC
tensor_inference_for_conv
tensor_inference_for_pool
tensor_inference_for_schema
run_on_device_with_order_nchw
run_on_device_with_order_nhwc
CudnnConvTransposeOp
run_on_device
GetConvGradient
get_gradient_defs
CudnnConvOpBase
drop
duplicate_conv_desc
given
set_conv_desc_compute_type
set_tensor_nd_descriptor_with_group
conv_transpose_op_example
compare
relative_error
conv_op_example
run1x_1conv_on_device_with_orderNCHW
run1x_1conv_on_device_with_orderNHWC
run_on_device_with_orderNHWC
run_on_device_with_order_nchw
CudnnConvTransposeGradientOp
register_cuda_operator
run_on_device
ConvTransposeGradientOp
ConvGradientOp
DocFn
run_on_device_with_order_nchw
run_on_device_with_order_nhwc
algorithms_cache_test_caches_correctly
algorithms_cache_test_keys_differ_if_flags_are_different
algorithms_cache_test_keys_differ_if_one_vector_is_empty
ConvOp
reinterleave_rows
ConvTransposeOp
inherit_onnx_schema
register_cpu_operator
run_tile_contiguous
cost_inference_function
register_cpu_operator
register_gradient
tensor_inference_function
register_cudnn_operator
computation
run_on_device_with_orderNCHW
run_on_device_with_orderNHWC
ConvFwdAlgorithmWithCost
CudnnConvOp
do_run_with_type
run_on_device
ConvBwdDataAlgorithmWithCost
ConvBwdFilterAlgorithmWithCost
CudnnConvGradientOp
do_run_with_type
drop
run_on_device
OpSchema
cost_inference_for_conv_gradient
create_shared_buffer_cpu_context
create_shared_buffer_cuda_context
run_with_shared_buffer_cpu_context
run_with_shared_buffer_cuda_context
cost_inference_function
inherit_onnx_schema
register_cpu_operator
tensor_inference_function
conv_transpose_mobile_test
rand_int
AlgorithmsCache
get_algorithm
register_cuda_operator
