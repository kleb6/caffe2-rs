hello! 

I am writing a rust crate called caffe2op-i8quantized. 

It is part of workspace containing the rust
translation of the caffe2 operator library.

I would like you to help me write a rust crate
description for it.

Please write the description in the following format:

<short description tag used in the crate header>
<double newline>
<longer, in depth description and mathematical analysis>
<END>

Please indicate that the crate is in the
process of being translated from C++ to
Rust. It is possible some of the function
bodies are in the process of translation.

Below are listed some tokens from this crate. 

Please describe the mathematical ideas
identified by this information alongside the
relevant mathematical equations in unicode.

thanks!

please format your response in the Markdown file format.

here are the tokens:
Int8ReshapeOp
allow_inplace
do_run_with_type
register_cpu_operator
run_on_device
Int8AddOp
allow_inplace
as
cost_inference_function
drop
register_cpu_operator
run_on_device
with
Int8FlattenOp
register_cpu_operator
run_on_device
tensor_inference_function
Int8ConcatOp
cost_inference_function
inherit_onnx_schema
register_cpu_operator
run_on_device
tensor_inference_function
Int8QuantizeOp
int_8quantize
register_cpu_operator
run_on_device
tensor_inference_function
Int8FCOp
cost_inference_function
drop
register_cpu_operator
run_on_device
tensor_inference_function
Int8RoIAlignOp
PreCalc
pre_calc_for_bilinear_interpolate
rOIAlign_forward
register_cpu_operator
run_on_device
Int8SliceOp
do_run_with_type
inherit_onnx_schema
int8_slice_op_example
register_cpu_operator
run_on_device
Int8LeakyReluOp
allow_inplace
cost_inference_function
drop
identical_type_and_shape
register_cpu_operator
run_on_device
add_error_tolerance
biasdq
biasq
dq
int_8copy
q
random_int
Int8MaxPoolOp
drop
register_cpu_operator
run_on_device_with_orderNHWC
tensor_inference_function
Int8SoftmaxOp
drop
identical_type_and_shape
register_cpu_operator
run_on_device
biassetq
int8_add
int8_average_pool
int8_channel_shuffle
int8_concat
int8_conv
int8_conv2
int8_conv_transpose
int8_depthwise_conv
int8_depthwise_conv3x3
int8_depthwise_conv5x5
int8_disabled_leaky_relu
int8_disabled_transpose
int8_fc
int8_flatten
int8_given_int_tensor_fill
int8_given_tensor_fill
int8_grouped1x_1conv
int8_max_pool
int8_quant_de_quant
int8_relu
int8_reshape
int8_resize_nearest
int8_sigmoid
int8_slice
int8_softmax
int8_sum_relu
setq
Int8ConvOp
cost_inference_function
drop
register_cpu_operator
run_on_device_with_orderNHWC
tensor_inference_function
Int8ConvTransposeOp
drop
register_cpu_operator
run_on_device_with_orderNHWC
int_8ro_ialign_ro_ialign
Int8SigmoidOp
drop
identical_type_and_shape
register_cpu_operator
run_on_device
Int8ReluOp
OpSchema
allow_inplace
cost_inference_for_relu
cost_inference_function
drop
identical_type_and_shape
inherit_onnx_schema
register_cpu_operator
run_on_device
Int8AveragePoolOp
drop
register_cpu_operator
run_on_device_with_order_nhwc
tensor_inference_function
Int8ChannelShuffleOp
drop
identical_type_and_shape
register_cpu_operator
run_on_device_with_orderNHWC
Int8DequantizeOp
identical_type_and_shape
int_8dequantize
register_cpu_operator
run_on_device
Int8GivenIntTensorFillOp
Int8GivenTensorFillOp
char
extract_values
fill
int32
register_cpu_operator
run_on_device
tensor_inference_function
Activation
activation_limits
initQNNPACK
multiply_by_quantized_multiplier_greater_than_one
multiply_by_quantized_multiplier_smaller_than_one
qnnp_status
quantize_multiplier_greater_than_one
quantize_multiplier_smaller_than_one
quantize_uint8
round
Int8ResizeNearestOp
register_cpu_operator
run_on_device
Int8TransposeOp
register_cpu_operator
run_on_device
