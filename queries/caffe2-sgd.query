hello! could you please help me write a crate-description for the rust crate "caffe2-sgd" containing the following symbols? not all of the symbols are defined in this crate, but they are used somehow (please do not explicitly list the symbols, or write anything other than a simple description.  no description header is necessary. however, please be descriptive without making too many assumptions):

?
AVG
AfterApply
All
An
Avg
Axpy
CPU
CURV
CopyVector
CpuContext
CurvWin
Curvature
D
Default
Distance
Div
Dot
Exp
G
GAvg
GPU
GRAD
GetDeviceType
GetLrMu
GetSingleArgument
Grad
Gradient
INIT
INPUT
ITER
If
Input
InputIsTensorType
Iter
Iteration
LR
Learning
Log
LrAvg
MEMORY
MOMENT
MU
Maximum
Memory
Moment
Momentum
MomentumSgdUpdate
Moving
MovingAverage
MuAvg
Mul
NAME
Note
OPERATOR
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputCurvWin
OutputG
OutputGAvg
OutputLrAvg
OutputMoment
OutputMuAvg
OutputParam
OutputScalarsMemory
PARAM
Param
Parameters
READ
ReduceMax
ReduceMin
ReinitializeTensor
SCALAR
SCALARS
SGD
Scalar
ScalarsMemory
Scale
Self
Sqrt
Sub
Takes
Temporary
Tensor
The
Usually
VAR
VECTOR
WIN
Workspace
YF
YellowFin
YellowFinOp
YellowFinOpInputs
YellowFinOpOutputs
ZeroDebiasFactor
\
abs
add
after
all
allocated
allow
and
any
apply
are
args
arguments
arxiv
automatic
aux
auxiliary
average
averages
be
being
beta
calculations
cell
change
coefficient
computed
computing
const
context
cpu
curv
curvature
d
deb
debias
def
define
device
different
dim
distance
do
during
elt
end
epsilon
factor
false
from
generic
grad
gradient
have
https
implementations
in
initialization
inplace
input
internal
iter
just
latest
learning
limit
listed
live
lr
make
math
matrix
max
memory
methods
min
moment
momentum
moving
mu
n
nesterov
network
new
norm
not
number
numerical
one
operator
opt
optimization
org
other
out
output
outputs
param
parameters
performs
persistent
pow
pre
publication
purposes
range
ranges
rate
ratio
register
remove
root
scalar
scalars
scratch
see
separate
sgd
shared
should
sign
size
sqrt
squared
stateful
step
steps
storage
stored
suitable
tags
tensor
tensors
these
this
timeframe
tuner
tunes
undef
update
updated
use
used
valid
variables
variance
vec
w
want
width
win
ws
y
you
zero

C
Dot
F
MSC
THROW
TODO
Unsupported
VER
XT
Y
YT
Z
ZT
above
can
const
ctx
cvtss
dot
endif
generic
half
ifdef
implementations
math
n
see
sh
specialize
specialized
tmp
use
vec
vectorize
y
z

?
AUX
AdaGrad
AdagradT
Additional
Approx
Approximately
AuxGrad
AuxParam
Auxiliary
CPU
Cannot
Decay
Default
DispatchHelper
Enforce
Exact
Fused
GE
GPU
GRAD
GT
Gather
GetSingleArgument
Given
Grad
Gradient
INDICES
Indices
Input
Integer
LENGTHS
LR
Lengths
LengthsRangeFill
Lr
MOMENT
Match
Moment
NULL
Non
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputMoment
OutputParam
PARAM
Param
Parameters
Resize
ResizeLike
RowWiseSparseSimdAdagradFusedWithSparseLengthsWeightedSumGradientApproxOp
SIndex
Self
SparseAdagrad
SparseAdagradFusedWithSparseLengthsMeanGradient
SparseAdagradFusedWithSparseLengthsMeanGradientApprox
SparseAdagradFusedWithSparseLengthsSumGradient
SparseAdagradFusedWithSparseLengthsSumGradientApprox
SparseAdagradFusedWithSparseLengthsSumGradientOp
SparseAdagradFusedWithSparseLengthsWeightedSumGradient
SparseAdagradFusedWithSparseLengthsWeightedSumGradientApprox
SparseAdagradFusedWithSparseLengthsWeightedSumGradientApproxOp
SparseAdagradFusedWithSparseLengthsWeightedSumGradientOp
SparseLengthsIndicesInGradientMeanGradient
SparseLengthsIndicesInGradientSumGradient
SparseLengthsIndicesInGradientWeightedSumWithMainInputGradient
SparseLengthsMean
SparseLengthsSum
SparseLengthsWeightedSum
SparseSimdAdagradOp
Specifically
TLengths
Tdata
Tensor
TensorTypes
The
There
Updated
VLOG
Workspace
Yet
access
adagrad
additional
and
approximate
are
args
as
aux
auxGrad
auxParamIn
auxiliary
be
because
being
below
block
bound
buffer
call
case
computed
condition
const
containing
context
could
cpu
dataIndex
debug
decay
def
default
dense
dependencies
dependency
device
dim
dimension
do
dot
elements
embedding
enforce
epsilon
equal
everything
false
first
fma
from
fuse
fused
fuses
fusing
grad
gradIn
gradient
gradients
h
hence
here
hi
history
idx
ignores
in
indices
inlined
inplace
input
internal
invoke
kernel
learning
length
lengths
lengthsInput
localOffset
loop
loops
lr
max
mean
moment
momentIn
momentOut
momentum
must
n
name
negative
new
nh
not
numSegments
nw
offsetI
offsetIdx
one
op
operator
optimization
ordering
out
output
outputs
param
paramIn
paramOut
parameters
params
pattern
phantomA
phantomB
positional
pref
prefdist
prefetch
prefetching
ptr
r
race
range
rangeIndex
rate
re
reading
register
returns
runs
s
same
second
segmentGradsInput
size
slices
sparse
sqrt
start
storage
sum
supported
tags
temp
that
there
these
this
tmpIndex
type
typename
types
update
updated
use
vec
version
violate
w
weight
weights
which
with
within
would
writing
ws

?
@note
Actual
AdaGrad
Adagrad
AdagradOp
AdagradOpInputs
AdagradOpOutputs
Block
Concretely
Cost
CostInferenceForAdagrad
CostInferenceForRowWiseSparseAdagrad
CostInferenceForSparseAdagrad
CostInferenceFunctionType
D
Decay
Default
DispatchHelper
EFFECTIVE
Each
Effective
Enforce
FBGEMM
For
GE
GRAD
GenerateSparseAdaGrad
GetSingleArgument
Given
Grad
Gradient
INDICES
If
Incorrect
Indices
Input
LR
Lr
MOMENT
Moment
NDEBUG
NVCC
OPERATOR
OUTPUT
OpSchema
OpSchemaCost
Operator
OperatorDef
OperatorStorage
Optionally
Output
OutputEffectiveLR
OutputMoment
OutputParam
OutputSize
OutputUpdate
PARAM
Param
Parameters
ResizeLike
RowWiseSparseAdagrad
RowWiseSparseAdagradOp
RunOnDevice
SIMD
SIndex
See
Self
Sparse
SparseAdaGradSignature
SparseAdagrad
SparseAdagradOp
SparseAdagradOpInputs
SparseAdagradOpOutputs
SparseSimdAdagradOp
TensorShape
TensorTypes
Type
UPDATE
Updated
VLOG
Workspace
access
accumulated
across
adagrad
add
adding
additions
allow
also
an
and
applied
applying
args
as
assume
average
backward
be
block
bound
bytes
c
calculated
call
case
cast
compatibility
compatible
computed
computes
const
context
cost
count
counter
counting
cpu
debug
decay
decayed
def
defined
dense
device
dim
division
do
effective
effectiveLROut
element
embedding
endif
enforce
engine
entire
epsilon
equal
factor
false
fbgemm
flop
flops
fma
from
function
given
gj
grad
gradIn
gradient
gradients
grads
h
halflife
hi
history
hs
idx
ifndef
in
including
index
indexes
indexing
indices
inference
inlined
inplace
input
internal
into
j
kernel
last
learning
least
length
lr
lrout
max
modified
moment
momentIn
momentOut
moments
momentum
momwnr
multiplications
must
n
nElemFromDim
need
new
nh
not
note
nullptr
number
nw
offsetI
offsetIdx
one
op
operator
optimistically
optimization
optional
out
output
outputs
param
paramIn
paramOut
parameters
params
per
plain
pref
prefdist
prefetch
prefetching
processed
range
rate
read
register
reinterpret
requires
returns
row
rows
rowwise
runs
s
same
shape
shapes
should
size
sizeof
sparse
sqrt
square
squared
step
storage
sum
supported
tags
tensor
that
this
transforms
type
uint
unused
updading
update
updateOut
updated
updates
updating
use
using
value
vec
w
weight
well
wise
with
within
written
ws

AdaDelta
Adadelta
AdadeltaOp
AdadeltaUpdate
Average
Concretely
DELTA
Default
DispatchHelper
Enforce
GE
GRAD
GT
Given
Grad
Gradient
INDICES
Indices
Input
LR
LT
Learning
MOMENT
MomentDelta
MomentGrad
NDEBUG
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputMomentDelta
OutputMomentGrad
OutputParam
PARAM
Param
Parameters
ResizeLike
RunOnDevice
SIndex
Self
Sparse
SparseAdadelta
SparseAdadeltaOp
SparseAdadeltaOpInputs
SparseAdadeltaOpOutputs
TensorTypes
Updated
Workspace
abs
accumulated
adadelta
allow
an
and
args
arxiv
as
attributes
average
base
be
block
bound
call
case
computed
computes
const
constraints
context
cpu
d
debug
decay
decayed
def
default
delta
dense
device
di
dim
do
domain
endif
enforce
epsilon
factor
from
given
grad
gradIn
gradient
gradients
h
hi
history
https
idx
ifndef
in
indices
inplace
input
learning
lr
moment
momentDeltaIn
momentDeltaOut
momentIn
momentOut
n
nd
new
ng
nh
not
nw
offsetI
offsetIdx
one
operator
org
out
output
outputs
param
paramIn
paramOut
parameter
parameters
rate
register
returns
runs
shapes
should
size
sqrt
square
squared
storage
sum
tags
this
type
update
updated
updates
use
vec
w
with
ws

?
Adam
AdamOp
CPU
Concretely
D
Default
DeviceOption
DispatchHelper
Each
Effective
Enforce
First
GE
GRAD
GetSingleArgument
Given
Grad
Gradient
INDICES
ITER
In
Indices
Input
InputIsTensorType
Iter
LR
LiyuanLucasLiu
Lr
MOMENT
Moment
NDEBUG
OPERATOR
OUTPUT
Operator
OperatorDef
OperatorStorage
Optional
Out
Output
OutputGrad
OutputMoment
OutputParam
OutputSize
PARAM
Param
Parameters
RAdam
Rectified
ResizeLike
RowWiseSparseAdam
RowWiseSparseAdamOp
SIndex
SMA
Second
Self
Sparse
SparseAdam
SparseAdamOp
Tensor
TensorTypes
Update
Updated
Workspace
\
abs
across
adam
added
allow
an
and
applied
approximated
are
args
arxiv
as
author
averaging
be
beta
blob
block
bound
but
calculated
call
can
case
com
compute
computed
condition
conservative
const
context
correction
cpu
customized
debug
def
default
dense
dev
device
dim
do
element
enable
enableRAdam
endif
enforce
entire
eps
epsilon
equal
false
first
follows
from
function
github
given
gj
grad
gradIn
gradOut
gradient
has
hat
history
https
idx
ifndef
implementation
in
included
indices
inf
inference
inplace
input
iter
iteration
iters
j
learning
length
live
lives
lr
m
make
master
mi
modified
momemnt
moment
momentum
more
multiplier
n
new
ng
ngi
nm
not
np
number
nv
nw
o
offsetI
offsetIdx
op
operator
option
optional
org
out
output
outputs
pair
paper
param
paramIn
paramOut
parameters
pow
power
py
r
radam
rate
register
returns
rho
row
rows
runs
s
second
setting
shape
shapes
should
since
size
sparse
sqrt
square
sum
tags
tensor
this
type
update
updated
use
v
value
values
vec
vi
w
which
with
ws

COUNTER
Count
Counter
Default
DispatchHelper
Exponential
GE
GetSingleArgument
INDICES
ITER
If
Indices
Input
Iter
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputCounter
OutputPrevIter
PREV
PrevIter
RowWiseCounter
RowWiseCounterOp
SIndex
Self
Sparse
TensorTypes
Updated
Workspace
applied
args
bound
call
const
context
counter
cpu
curr
current
debug
decay
def
delta
device
do
double
enforce
exp
gradient
halflife
idx
indices
inplace
input
iter
iteration
last
log
max
n
neg
new
nonpositive
not
number
off
one
operator
out
output
outputs
prev
r
rate
recent
register
rho
rows
should
size
storage
such
tags
that
this
turned
type
update
use
with
ws

?
After
CPU
CUDAContext
Current
DeviceOption
DispatchHelper
Every
GetSingleArgument
ITER
Input
Iter
Iteration
OPERATOR
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputWeights
ResizeLike
RunOnDevice
Scale
Self
Tensor
TensorTypes
The
Training
Updated
WEIGHTS
WeightScale
WeightScaleOp
Weights
Workspace
`scale`
`stepsize`
allow
applied
args
bound
caffe
call
const
constant
context
cpu
cuda
cudaMemcpy
cudaMemcpyDefault
def
dev
device
do
factor
function
gradient
has
in
inference
inplace
input
iter
iteration
iterations
kernel
lives
make
math
max
memcpy
multiplicative
multiply
n
new
not
number
nw
op
operator
option
out
output
outputs
pair
passes
perform
register
rescaling
scale
scaling
should
size
sizeof
stepsize
storage
tags
this
type
update
upper
use
vec
void
w
weight
weights
with
ws

CPU
FP
GRAD
GetDeviceType
GetSingleArgument
Grad
Input
InputIsTensorType
Iter
LR
Lr
MOMENTUM
Momentum
MomentumSGDUpdateOp
OPERATOR
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputGrad
OutputMomentum
OutputParam
PARAM
Param
ResizeLike
Self
Workspace
const
context
decay
def
default
device
fp
input
live
lr
m
momentum
n
nesterov
new
ng
nm
operator
output
param
sgd
size
tags
this
type
update
use
weight
ws

Accelerated
Adjusted
Both
CPU
Concretely
DispatchHelper
Enforce
Full
GRAD
GetDeviceType
GetSingleArgument
Grad
Gradient
GradientSlice
INDICES
Indices
Input
InputIsTensorType
Iter
LR
Learning
Lr
MOMENTUM
MomemtumSGDUpdate
Momentum
MomentumSGD
MomentumSGDOp
MomentumSGDUpdate
MomentumSGDUpdateOp
NULL
Nesterov
Note
OPERATOR
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputGrad
OutputMomentum
OutputParam
PARAM
Param
Performs
Resize
ResizeLike
SGD
SIndex
Self
SparseMomentumSGDOp
SparseMomentumSGDUpdate
SparseMomentumSGDUpdateOp
TensorShape
TensorTypes
Updated
Whether
Workspace
actually
adjusted
allow
an
analogous
and
are
args
arguments
as
be
blob
blobs
block
boolean
but
call
computes
const
context
corresponding
cpu
def
default
device
difference
dim
dimension
do
does
enforce
false
faster
first
from
full
function
given
grad
gradIn
gradOut
gradient
gradients
hyperparameter
idx
in
indices
inference
inplace
input
into
live
lr
m
mi
moment
momentum
momentumIn
momentumOut
n
nesterov
new
ng
nm
not
offsetI
offsetIdx
operator
out
output
outputs
param
paramIn
paramOut
parameter
parameters
perform
performed
performs
place
potentially
rate
register
returns
same
sgd
shape
shapes
should
size
storage
tables
tags
tensor
this
thus
type
unused
update
updated
updates
use
using
vec
which
with
ws

CPU
FP
GRAD
GetDeviceType
GetSingleArgument
Grad
Input
InputIsTensorType
Iter
LR
Lr
MOMENTUM
Momentum
MomentumSGDUpdateOp
OPERATOR
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputGrad
OutputMomentum
OutputParam
PARAM
Param
ResizeLike
Self
Workspace
all
but
compute
const
context
decay
def
default
device
fp
in
input
live
lr
m
momentum
n
nesterov
new
ng
nm
operator
output
param
perform
precision
read
set
sgd
size
tags
this
type
update
use
weight
will
ws

An
ClipTensorByScaling
ClipTensorByScalingOp
Clips
CopyFrom
GT
GetSingleArgument
If
Input
InputSize
OPERATOR
Operator
OperatorDef
Output
Scale
Self
Tensor
That
The
This
Threshold
Value
Workspace
additional
against
allow
and
args
as
async
based
be
become
before
called
can
clipped
clipping
compared
computed
const
context
could
cpu
def
determine
device
do
down
final
floats
gradient
greater
in
inplace
input
larger
math
must
new
norm
not
op
operator
optional
original
outputs
performed
pre
provided
ratio
register
representing
same
scale
scaling
should
size
tensor
than
this
threshold
use
used
usually
val
value
vec
way
whether
which
will
would
ws

?
A
AUX
AVX
Accessing
AdaGrad
Additional
After
Approx
Approximately
Assuming
AuxGrad
AuxParam
Auxiliary
B
BW
Before
Block
CPU
Cannot
D
DECAY
Decay
Default
DispatchHelper
DoRunWithType
Embedding
Enforce
Exact
FMA
Fused
GE
GPU
GRAD
GT
Gather
GetSingleArgument
Given
Grad
Gradient
HAS
HINT
Half
INDICES
Indices
Input
Integer
IsMeanFlag
LENGTHS
LR
Lengths
LengthsRangeFill
Lr
MM
MOMENT
Match
Moment
NULL
Non
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputMoment
OutputParam
PARAM
Param
Parameters
Reduce
Resize
ResizeLike
RowWiseAdagradT
RowWiseAdagradUpdateInlined
RowWiseSparseAdagrad
RowWiseSparseAdagradFusedWithSparseLengthsMeanGradient
RowWiseSparseAdagradFusedWithSparseLengthsMeanGradientApprox
RowWiseSparseAdagradFusedWithSparseLengthsSumGradient
RowWiseSparseAdagradFusedWithSparseLengthsSumGradientApprox
RowWiseSparseAdagradFusedWithSparseLengthsSumGradientOp
RowWiseSparseAdagradFusedWithSparseLengthsWeightedSumGradient
RowWiseSparseAdagradFusedWithSparseLengthsWeightedSumGradientApprox
RowWiseSparseAdagradFusedWithSparseLengthsWeightedSumGradientApproxOp
RowWiseSparseAdagradFusedWithSparseLengthsWeightedSumGradientOp
RowWiseSparseSimdAdagradFusedWithSparseLengthsWeightedSumGradientApproxOp
SIMD
SIndex
See
Self
So
SparseLengthsIndicesInGradientMeanGradient
SparseLengthsIndicesInGradientSumGradient
SparseLengthsIndicesInGradientWeightedSumWithMainInputGradient
SparseLengthsMean
SparseLengthsMeanGradient
SparseLengthsSum
SparseLengthsSumGradient
SparseLengthsWeightedSum
SparseLengthsWeightedSumGradient
SparseSimdAdagradOp
Specifically
THROW
TLengths
Tdata
Tensor
TensorTypes
The
There
Unsupported
Updated
Ur
VLEN
VLOG
WEIGHT
Workspace
Yet
about
acc
access
adagrad
add
additional
ai
alignas
analysis
and
approximate
are
args
as
assuming
aux
auxGrad
auxParamIn
auxiliary
average
b
be
because
being
below
block
bound
buffer
c
call
case
cast
castps
char
com
compute
computed
condition
const
constexpr
containing
context
could
cpu
cvtph
cvtss
dataIndex
debug
decay
def
default
dense
dependencies
dependency
details
device
dim
dimension
do
dot
elements
embedding
endif
enforce
engine
epsilon
equal
evaluation
everything
extractf
false
fb
first
fma
fmadd
from
fuse
fused
fuses
fusing
fusion
grad
gradIn
gradient
h
hadd
hence
here
hi
history
https
idx
ifdef
ignores
in
indices
inlined
inplace
input
internal
invoke
j
kSize
kernel
ldG
learning
len
length
lengths
lengthsInput
load
loadu
localOffset
loop
loops
lr
m
max
mean
memory
mm
moment
momentIn
momentOut
momentum
more
mul
must
n
name
nearest
negative
new
not
numParams
numSegments
offsetI
offsetIdx
one
op
operator
optimization
option
ordering
out
output
outputs
param
paramIn
paramOut
parameters
params
partial
pattern
phantomA
phantomB
positional
pref
prefdist
prefetch
prefetching
ps
ptr
quip
r
race
range
rangeIndex
rate
re
read
reading
reads
register
reinterpret
results
returns
round
rounding
row
rowWiseAdagradT
rowwise
runs
s
same
saving
scalar
second
segmentGradsInput
segments
set
setzero
shapes
si
size
slices
sparse
sq
sqrt
square
start
step
stochastic
storage
storate
store
storeu
sum
supported
tags
temp
that
there
these
this
tmpIndex
total
traffic
type
typename
types
update
updated
use
v
value
vec
version
violate
w
wM
we
weight
weights
whi
which
wi
wise
with
within
would
write
writes
writing
ws

?
ALPHA
Alpha
Cannot
DCHECK
DoRun
FATAL
Ftrl
FtrlOp
FtrlParams
GPU
GRAD
GetSingleArgument
Grad
HasArgument
INDICES
In
Index
Indices
Input
InputSize
IsType
K
LOG
M
NZ
OMP
OPERATOR
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputNZ
OutputVar
ResizeLike
SIMD
SIndex
Self
SparseFtrl
SparseFtrlOp
TODO
Unsupported
Use
VAR
Var
Workspace
Z
abs
allow
alpha
alphaInv
and
argument
based
be
beta
block
both
bounds
compute
const
context
cpu
cxj
def
device
do
dzhulgakov
enforce
ftrl
grad
gradient
idx
idxs
implement
in
indices
inplace
input
inv
lambda
learning
n
name
necessary
new
nn
not
nw
nz
omp
op
operation
operator
out
output
outputs
override
parallel
params
place
polymorphism
pragma
range
rate
real
register
reliable
required
sgn
should
sigma
size
specify
sqrt
tags
this
time
type
update
use
valued
var
vec
version
w
weight
ws
z

AlternateLearningRate
Args
Base
Box
CPU
CUDAContext
CompositeCosineLRPolicy
CompositeCosineLearningRate
CompositeCyclicalLRPolicy
CompositeCyclicalLearningRate
CompositeLearningRate
CompositeLearningRateItem
ConstantThenLinearWarmupLearningRate
ConstantWarmupLearningRate
CopyFromCPU
CosineLearningRate
CyclicalLearningRate
DCHECK
Defining
DeviceOption
Example
ExpLearningRate
FLT
FixedLearningRate
GE
GT
GateLearningRate
GetRepeatedArgument
GetSingleArgument
HillLearningRate
Input
InvLearningRate
LE
LR
Learning
LearningRate
LearningRateFunctor
LearningRateOp
LearningRateOpFloatCPU
LinearWarmupLearningRate
MAX
Must
NE
NULL
OPERATOR
Operator
OperatorDef
OperatorStorage
Optional
Output
PieceWarmupLearningRate
PolyLearningRate
Required
Resize
Self
SlopeLearningRate
StepLearningRate
THROW
Tensor
TensorShape
The
True
Unknown
Usage
With
Write
`
`active
`alter`
`base
`compositeCosine`
`compositeCyclical`
`composite`
`constant
`constantThenLinearWarmup`
`constantWarmup`
`cosine
`cosine`
`cyclic`
`cyclical
`end
`exp`
`fixed`
`gamma`
`gate`
`hill`
`inactive
`inv`
`iterations`
`linear
`linearWarmup`
`lr
`m
`max
`min
`multiplier
`multiplier`
`n
`num
`period`
`policy`
`power`
`start
`step`
`stepsize`
`sub
`t
active
additional
allowed
alter
and
applied
are
arg
args
arguments
as
back
base
be
boolean
both
change
composite
compositeCosine
compositeCyclical
const
constant
constantThenLinearWarmup
constantWarmup
constraint
context
controlled
controls
cosine
cpu
create
createLearningRateFunctor
cuda
cyclical
decay
decay`
decreasing
def
default
defaults
description
device
dyn
empty
end
enforcement
example
exp
exponential
first
first`
fixed
following
format
forward
function
functor
gamma
gate
gradient
high
hill
how
improvements
in
inactive
index
inference
input
inv
iter
iter`
iter``
iterations
iters
iters`
label
learning
least
linear
linearWarmup
list
look
low
lr
lr`
m
make
max
maximum
min
momentum
more
mult
mult`
multiplier
multiplier`
must
n
name
needed
net
new
no
not
nullptr
number
one
only
operator
option
options
out
output
outputs
over
pair
part
period
period`
piece
pieceWarmup
plus
policies
policy
poly
positive
power
prefix
push
range
rate
rates
register
required
reset
sampling
scale
second
set
should
shrink
shrink`
size
size`
slope
specify
start
starting
step
stepsize
str
strategy
stringstream
sub
subpolicy
tensor
they
third
this
those
threshold
time
train
training
type
usage
use
used
uses
warmup
which
will
with

Concretely
ConstEigenVectorArrayMap
EigenVectorArrayMap
GRAD
GetSingleArgument
Grad
Input
LR
Lr
MEAN
MOMENTUM
MeanSquares
Momentum
New
OPERATOR
OUTPUT
Operator
OperatorDef
Output
OutputGrad
OutputMeanSquares
OutputMomentum
RMSProp
ResizeLike
Returns
RmsProp
RmsPropOp
RmsPropUpdate
SQUARES
Self
Update
Workspace
allow
computes
const
context
cpu
cs
csc
decay
def
device
do
edu
epsilon
estimate
gVec
given
grad
gradient
http
inplace
input
lec
lecture
lr
mean
mom
momVec
momentum
ms
msVec
n
new
ng
nmom
nmomVec
nms
nmsVec
not
o
operator
output
outputs
pdf
register
rmsprop
should
slides
sqrt
square
squares
tags
this
toronto
trait
update
use
vec
ws
www
~tijmen

ALPHA
Alpha
Cannot
GFtrl
GFtrlOp
GFtrlParams
GRAD
GetSingleArgument
Grad
HasArgument
Input
InputDim
InputSize
NZ
OPERATOR
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputDim
OutputNZ
OutputVar
ResizeLike
Self
VAR
Var
Workspace
Z
allow
alpha
alphaInv
and
argument
be
beta
both
compute
const
context
coordinate
cpu
def
device
dim
do
features
gftrl
gradient
idx
inplace
input
inv
j
lambda
learning
n
new
nn
nodes
norm
not
nw
nz
one
op
operator
output
outputs
override
params
rate
real
register
should
sigma
size
specify
sqrt
tags
this
time
update
use
valued
vec
w
weight
ws
z

Adaptive
Before
Compute
ComputeLearningRate
ComputeNorms
GE
GetDeviceType
GetSingleArgument
Gradient
Implement
In
Input
LARS
Lars
LarsOp
Layer
OPERATOR
Operator
OperatorDef
Output
Parameter
Rate
ReinitializeTensor
Rescaled
Scaling
Self
Sqrt
SumSqr
Tensor
Trust
Upper
Weight
Workspace
adding
and
apply
args
avoid
based
be
bound
change
clipped
clipping
compute
computed
const
context
cpu
cpucontext
dX
decay
def
device
do
doesn
during
fmaxf
fminf
given
gradient
how
hyper
implementation
indicates
its
layer
learning
local
lower
lr
match
math
max
min
minimum
much
n
new
norm
norms
not
numerical
offset
one
operator
outputs
parameter
parameters
preset
rate
register
rescaled
rescaling
should
size
tensor
this
trust
update
upper
use
uses
val
wd
we
weight
will
wise
with
ws

Adaption
EFFGRAD
Effgrad
GRAD
GetSingleArgument
Grad
Gradient
INNERPRODUCT
If
Input
Instead
It
LR
Learning
LearningRateAdaption
LearningRateAdaptionOp
Lr
OPERATOR
OUTPUT
Operator
OperatorDef
Output
OutputLr
Rate
ResizeLike
Self
The
Updated
When
Workspace
adaption
allow
alpha
an
and
apply
args
argument
as
based
be
can
computed
const
context
cosineSimilarity
cpu
def
default
descent
device
df
directly
dlr
do
effective
effgrad
equals
false
fmax
following
function
grad
gradient
hyperparameter
inplace
input
iteration
k
kEps
learning
lr
n
new
nlr
no
normalized
not
objective
one
operation
operator
or
output
outputs
parameters
perform
performing
prove
rate
register
set
simply
sqrt
tags
that
this
update
use
vec
we
whether
ws
y
z

ARG
Concretely
DispatchHelper
Enforce
GE
GRAD
GRADSQSUM
GRAGSQSUM
Given
Grad
Gradients
Gradsqsum
INDICES
Indices
Input
LR
Learning
Lr
MOMENT
Moment
Momentum
NDEBUG
Note
OP
OPERATOR
OUTPUT
Operator
OperatorDef
OperatorStorage
Output
OutputGragsqsum
OutputMoment
OutputParam
PARAM
Param
Parameters
Resize
ResizeLike
SINGLE
SIndex
STORM
Self
Sparse
SparseStorm
SparseStormOp
Storm
StormOp
Sum
TensorTypes
This
Updated
We
Workspace
abs
accumulated
adaptive
algorithm
allow
alpha
an
and
args
arxiv
as
be
beta
blobs
block
bound
c
caffe
calculate
calculation
call
case
computed
computes
const
context
cpu
current
debug
def
denominator
dense
device
difficult
dim
do
due
effective
endif
enforce
example
from
given
grad
gradIn
gradSqSumIn
gradSqSumOut
gradSqSumTmp
gradient
gradients
history
https
hyperparameter
idx
ifndef
implement
in
indices
inplace
input
iteration
j
k
learning
limitation
lr
mi
moment
momentIn
momentOut
momentum
n
new
nlr
norm
not
observed
offsetI
offsetIdx
one
operator
optimization
org
original
out
output
outputs
paper
param
paramIn
paramOut
parameters
place
potentially
pow
previous
rate
re
register
returns
shapes
should
simplied
size
sq
square
squared
storage
storm
sum
tags
that
this
type
update
updated
use
using
vec
w
with
ws

Actual
B
Bottou
Concretely
Default
DispatchHelper
EFFECTIVE
Effective
Enforce
GE
GRAD
GetSingleArgument
Given
Grad
Gradient
INDICES
Input
LR
Lr
NDEBUG
OPERATOR
OUTPUT
Operator
OperatorDef
OperatorStorage
Optionally
Output
OutputEffectiveLR
OutputParam
OutputSeqB
OutputSize
OutputUpdate
PARAM
Param
Parameters
ResizeLike
SEQ
SIndex
Self
Seq
SeqB
Sparse
SparseWngrad
SparseWngradOp
SparseWngradOpInputs
SparseWngradOpOutputs
TensorTypes
This
UPDATE
Updated
Ward
WnGrad
Wngrad
WngradOp
WngradOpInputs
WngradOpOutputs
Workspace
Wu
abs
accumulated
algorithm
allow
an
and
applied
args
arxiv
as
b
be
bin
block
bound
bout
call
case
computed
computes
const
context
cpu
debug
def
dense
device
dim
do
effective
effectiveLROut
endif
enforce
epsilon
from
given
grad
gradIn
gradient
h
history
https
idx
ifndef
implement
in
indices
inplace
input
j
learning
lr
lrout
n
new
nh
nhTmp
norm
not
nw
offsetI
offsetIdx
one
operator
optimization
optional
org
out
output
outputs
param
paramIn
paramOut
parameters
rate
register
returns
runs
seq
seqBIn
seqBOut
seqBTmp
shapes
should
size
storage
tags
that
this
type
update
updateOut
updated
use
vec
w
well
with
wngrad
ws

AtomicIter
AtomicIterOp
AtomicIterOpStats
Blob
BlobDeserializerBase
BlobProto
BlobSerializationOptions
BlobSerializerBase
CPU
CTOR
CUDAContext
ERROR
EVENT
EXPORTED
EnforceCheck
GetMutable
Hogwild
IDEEPFallbackOp
Id
If
IncrementIter
Initializing
Input
InputSize
Iter
IterOp
LOG
Match
More
MutexDeserializer
MutexSerializer
Note
OPERATOR
Operator
OperatorDef
OperatorStorage
Option
Otherwise
Output
OutputIsTensorType
OutputTensor
Overflow
Previous
Run
SGD
STAT
Self
SerializationAcceptor
SerializeBlobProtoAsString
Serializes
Similar
Stores
String
Tensor
TensorCPU
The
This
TypeMeta
Useful
VLOG
Workspace
You
acceptor
access
algorithms
already
always
an
and
are
as
atomic
atomically
base
be
blob
but
c
caffe
call
can
cannot
carried
case
cfg
const
contain
content
context
count
counter
cpu
cuda
def
definition
deprecated
deserialize
deserializer
device
do
during
dyn
emulates
enforce
error
example
exists
explicit
fatal
first
function
gets
gradient
guard
happen
has
have
ideep
identical
in
increment
incremented
inplace
input
integer
iter
iteration
lg
lock
make
max
meta
mkldnn
mutex
name
need
negative
new
no
not
now
number
object
old
operator
options
otherwise
out
output
outputs
place
pointer
produce
produces
proto
ptr
register
requires
resume
right
runs
serialize
serializer
set
sgd
shape
side
simply
singe
size
so
soon
specifically
start
starting
stats
sure
takes
tensor
that
think
this
tracking
training
type
typeMeta
unique
updates
use
used
using
variable
vec
void
want
we
will
with
would
ws
zero

?
Alter
AlternateLearningRate
Box
CompositeCosineLearningRate
CompositeCyclicalLearningRate
CompositeLearningRate
CompositeLearningRateItem
ConstantThenLinearWarmupLearningRate
ConstantWarmup
ConstantWarmupLearningRate
Cosine
CosineLearningRate
Cyclical
CyclicalLearningRate
DCHECK
Exp
ExpLearningRate
Fixed
FixedLearningRate
GT
Gate
GateLearningRate
HashMap
HillLearningRate
Inv
InvLearningRate
LearningRateFunctor
LinearWarmup
LinearWarmupLearningRate
LinkedList
M
PI
PieceWarmupLearningRate
Poly
PolyLearningRate
See
Self
SlopeLearningRate
Step
StepLearningRate
abs
according
active
afterwards
all
alternatate
an
and
arxiv
base
before
begin
bound
bounded
cast
changed
changes
changing
composite
consine
constant
constantThenLinearWarmup
constantWarmup
corresponding
cos
cosine
curr
current
cycle
cyclical
decay
decreasing
double
duration
dyn
end
every
fed
final
first
fixed
floor
following
from
functor
gamma
global
hill
https
inactive
increasing
inv
inverse
invoke
iter
iteration
learning
linear
linearly
log
lower
lr
m
max
min
mult
multiplier
n
new
not
number
org
otherwise
pdf
period
policies
policy
pow
power
produces
ramp
rate
reset
scale
scales
schedule
second
shink
shrink
size
slope
stages
start
static
step
steps
stepsize
stop
sub
that
then
time
trait
u
up
update
upper
use
versa
vice
warmup
with
