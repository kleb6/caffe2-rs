crate::ix!();

//-------------------------------------------[.cpp/pytorch/aten/src/ATen/templates/TensorBody.h]

pub type DefaultIntrusivePtrTag = i32;

pub struct HookReturnVar<T> {
    phantomA: PhantomData<T>,
}

pub struct HookReturnVoid<T> {
    phantomA: PhantomData<T>,
}

pub struct TorchautogradNode {}

//TODO:
pub struct GenericPackedTensorAccessor<T,const N: usize,PtrTraits,Index> {
    phantomA: PhantomData<T>,
    phantomB: PhantomData<PtrTraits>,
    phantomC: PhantomData<Index>,
}

pub struct PackedTensorAccessor64<T,const N: usize,PtrTraits> {
    phantomA: PhantomData<T>,
    phantomB: PhantomData<PtrTraits>,
}

pub struct PackedTensorAccessor32<T,const N: usize,PtrTraits> {
    phantomA: PhantomData<T>,
    phantomB: PhantomData<PtrTraits>,
}

//TODO:
pub struct TensorAccessor<T,const N: usize> {
    phantomA: PhantomData<T>, 
}

pub struct IntrusivePtr<A,Tag = DefaultIntrusivePtrTag> { 
    phantomA: PhantomData<A>, 
    phantomB: PhantomData<Tag>, 
}

pub struct WeakIntrusivePtr<A,Tag = DefaultIntrusivePtrTag> { 
    phantomA: PhantomData<A>, 
    phantomB: PhantomData<Tag>, 
}

#[inline] pub fn variable_excluded_from_dispatch() -> bool {
    
    todo!();
        /*
            #ifdef C10_MOBILE
      // Please read the comment in `VariableFallbackKernel.cpp` about the background of this change.
      return true;
    #else
      TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!tls_local_dispatch_key_set().excluded_.has(DispatchKey::Autograd));
      return tls_local_dispatch_key_set().excluded_.isSupersetOf(autograd_dispatch_keyset);
    #endif
        */
}

#[derive(Default)]
pub struct UnsafeBorrow {

}

/**
  | Tensor is a "generic" object holding a pointer
  | to the underlying TensorImpl object, which has
  | an embedded reference count. In this way,
  | Tensor is similar to boost::intrusive_ptr.
  |
  | For example:
  |
  | void func(Tensor a) {
  |   Tensor b = a;
  |   ...
  | }
  |
  | In this example, when we say Tensor b = a, we
  | are creating a new object that points to the
  | same underlying TensorImpl, and bumps its
  | reference count. When b goes out of scope, the
  | destructor decrements the reference count by
  | calling release() on the TensorImpl it points
  | to.
  |
  | The existing constructors, operator overloads,
  | etc. take care to implement the correct
  | semantics.
  |
  | Note that Tensor can also be NULL, i.e. it is
  | not associated with any underlying TensorImpl,
  | and special care must be taken to handle this.
  */
pub struct Tensor {
    impl_: IntrusivePtr<TensorImpl,UndefinedTensorImpl>,
}

impl Clone for Tensor {

    fn clone(&self) -> Self {
        todo!();
    }
}

impl Default for Tensor {

    fn default() -> Self {
        todo!("can do some form of default initialization");
    }
}

impl Tensor {


    /**
      | Create a Tensor with a +0 reference
      | count. Special care must be taken to avoid
      | decrementing this reference count at
      | destruction time.
      |
      | Intended to support MaybeOwnedTraits<Tensor>.
      */
    pub fn new_from_unsafe_borrow(
        _0:  UnsafeBorrow,
        rhs: &Tensor) -> Self {
    
        todo!();
        /*
            : impl_(intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(rhs.impl_.get()))
        */
    }

    /**
      | This constructor should not be used by end
      | users and is an implementation detail invoked
      | by autogenerated code.
      |
      */
    pub fn new(tensor_impl: IntrusivePtr<TensorImpl,UndefinedTensorImpl>) -> Self {
    
        todo!();
        /*


            : impl_(move(tensor_impl)) 
        if (impl_.get() == nullptr) {
          throw runtime_error("TensorImpl with nullptr is not supported");
        }
        */
    }
 
    /**
      | Creates a new wrapper from
      | TensorImpl. Intentionally a free method
      | because it should be used with care. Checks
      | necessary invariants
      |
      */
    pub fn wrap_tensor_impl(tensor_impl: IntrusivePtr<TensorImpl,UndefinedTensorImpl>) -> Tensor {
        
        todo!();
        /*
            Tensor r(move(tensor_impl));
        r.enforce_invariants();
        return r;
        */
    }
    
    pub fn dim(&self) -> i64 {
        
        todo!();
        /*
            return impl_->dim();
        */
    }
    
    pub fn storage_offset(&self) -> i64 {
        
        todo!();
        /*
            return impl_->storage_offset();
        */
    }
    
    pub fn contiguous(&self, memory_format: Option<MemoryFormat>) -> Tensor {

        let memory_format: MemoryFormat = memory_format.unwrap_or(MemoryFormat::Contiguous);

        todo!();
        /*
            if (is_contiguous(memory_format)) {
          return *this;
        } else {
          return __dispatch_contiguous(memory_format);
        }
        */
    }
    
    pub fn conj(&self) -> Tensor {
        
        todo!();
        /*
            if (!this->is_complex()) {
          return *this;
        } else {
          if (this->is_sparse()) {
            return this->conj_physical();
          }
          return this->_conj();
        }
        */
    }

    pub fn is_complex(&self) -> bool {
        
        todo!();
        /*
            return isComplexType(this->scalar_type());
        */
    }
    
    pub fn is_floating_point(&self) -> bool {
        
        todo!();
        /*
            return isFloatingType(this->scalar_type());
        */
    }
    
    pub fn is_signed(&self) -> bool {
        
        todo!();
        /*
            return isSignedType(this->scalar_type());
        */
    }
    
    pub fn size(&self, dim: i64) -> i64 {
        
        todo!();
        /*
            // false is passed to maybe_wrap_dim so behavior is identical to array access (but with wrapping)
        dim = maybe_wrap_dim(dim, this->dim(), false);
        return sizes()[dim];
        */
    }
    
    pub fn stride(&self, dim: i64) -> i64 {
        
        todo!();
        /*
            // false is passed to maybe_wrap_dim so behavior is identical to array access (but with wrapping)
        dim = maybe_wrap_dim(dim, this->dim(), false);
        return strides()[dim];
        */
    }
    
    pub fn unsafe_get_tensor_impl(&self) -> *mut TensorImpl {
        
        todo!();
        /*
            return impl_.get();
        */
    }
    
    pub fn unsafe_release_tensor_impl(&mut self) -> *mut TensorImpl {
        
        todo!();
        /*
            return impl_.release();
        */
    }
    
    pub fn get_intrusive_ptr(&self) -> &IntrusivePtr<TensorImpl,UndefinedTensorImpl> {
        
        todo!();
        /*
            return impl_;
        */
    }
    
    pub fn unsafe_release_intrusive_ptr(&mut self) -> IntrusivePtr<TensorImpl,UndefinedTensorImpl> {
        
        todo!();
        /*
            return move(impl_);
        */
    }
    
    pub fn defined(&self) -> bool {
        
        todo!();
        /*
            return impl_;
        */
    }
    
    pub fn reset(&mut self)  {
        
        todo!();
        /*
            impl_.reset();
        */
    }

    /**
      | The following overloads are very intruiging.
      | Consider the following program:
      |
      |    x[1] = 3;
      |
      | We would expect that the first entry of x is
      | written to 3.  But how can we actually
      | achieve this?  x[1] evaluates to a tensor...
      |
      | The answer is, using a ref-qualifier.  x[1]
      | is an rvalue, which cannot be (profitably)
      | assigned to in the traditional sense, so we
      | overload assignment to mean, "Actually, copy
      | 3 into the tensor data."  This is done with
      | an rvalue-reference ref-qualified overload
      | (the methods with && at the end of their
      | type.)
      |
      | There's one more fly in the ointment: We also
      | want
      |
      |    Tensor x = y;
      |
      | to work, and we want it NOT to copy.  So we
      | need a traditional operator= overload.  But
      | we MUST specify a mutable lvalue
      | ref-qualifier, to disambiguate the
      | traditional overload from the
      | rvalue-reference ref-qualified overload.
      | Otherwise, it will be ambiguous, because
      | a non ref-qualified method is eligible for
      | all situations.
      */
    pub fn assign_from_ref(&mut self, x: &Tensor) -> &mut Tensor {
        
        todo!();
        /*
            impl_ = x.impl_;
        return *this;
        */
    }
    
    pub fn assign_from(&mut self, x: Tensor) -> &mut Tensor {
        
        todo!();
        /*
            impl_ = move(x.impl_);
        return *this;
        */
    }
    
    pub fn is_same(&self, other: &Tensor) -> bool {
        
        todo!();
        /*
            return impl_ == other.impl_;
        */
    }
    
    pub fn use_count(&self) -> usize {
        
        todo!();
        /*
            return impl_.use_count();
        */
    }
    
    pub fn weak_use_count(&self) -> usize {
        
        todo!();
        /*
            return impl_.weak_use_count();
        */
    }
    
    pub fn to_string(&self) -> String {
        
        todo!();
        /*
        
        */
    }
    
    pub fn sizes(&self) -> &[i32] {
        
        todo!();
        /*
            return impl_->sizes();
        */
    }
    
    pub fn strides(&self) -> &[i32] {
        
        todo!();
        /*
            return impl_->strides();
        */
    }

    /// See get_opt_names in ATen/NamedTensor.h
    /// for docs.
    ///
    pub fn opt_names(&self) -> Option<&[Dimname]> {
        
        todo!();
        /*
            return get_opt_names(unsafeGetTensorImpl());
        */
    }

    /// See get_names in ATen/NamedTensor.h for docs.
    ///
    pub fn names(&self) -> &[Dimname] {
        
        todo!();
        /*
            return get_names(unsafeGetTensorImpl());
        */
    }
    
    pub fn ndimension(&self) -> i64 {
        
        todo!();
        /*
            return dim();
        */
    }
    
    pub fn is_contiguous(&self, memory_format: Option<MemoryFormat>) -> bool {

        let memory_format: MemoryFormat = memory_format.unwrap_or(MemoryFormat::Contiguous);

        todo!();
        /*
            return impl_->is_contiguous(memory_format);
        */
    }
    
    pub fn is_non_overlapping_and_dense(&self) -> bool {
        
        todo!();
        /*
            return impl_->is_non_overlapping_and_dense();
        */
    }
    
    pub fn suggest_memory_format(&self, channels_last_strides_exact_match: Option<bool>) -> MemoryFormat 
    {
        let channels_last_strides_exact_match: bool = channels_last_strides_exact_match.unwrap_or(false);

        todo!();
        /*
            // Setting channels_last_strides_exact_match to true forces function to
        // check 0,1 - sized dimension strides.
        if (!is_mkldnn() && !is_sparse()) {
          if (impl_->is_strides_like_channels_last()) {
            if (!channels_last_strides_exact_match ||
                get_channels_last_strides_2d(sizes()) == strides()) {
              return MemoryFormat::ChannelsLast;
            }
          }
          else if (impl_->is_strides_like_channels_last_3d()) {
            if (!channels_last_strides_exact_match ||
                get_channels_last_strides_3d(sizes()) == strides()) {
              return MemoryFormat::ChannelsLast3d;
            }
          }
        }
        return MemoryFormat::Contiguous;
        */
    }

    /**
      | Total bytes consumed by the "view" of
      | elements of the array.  Does not include size
      | of metadata.
      |
      | The number reported here does not necessarily
      | correspond to the true physical memory
      | consumed by a tensor; instead, it reports the
      | memory the tensor would take *if* it were
      | contiguous.
      |
      | Defined to be numel() * itemsize()
      */
    pub fn nbytes(&self) -> usize {
        
        todo!();
        /*
            TORCH_CHECK(layout () != kSparse,
                    "nbytes is not defined for sparse tensors.  If you want the size of the constituent " \
                    "tensors, add the nbytes of the indices and values.  If you want the size of the  " \
                    "equivalent dense tensor, multiply numel() by element_size()");
        return impl_->numel() * impl_->itemsize();
        */
    }
    
    pub fn numel(&self) -> i64 {
        
        todo!();
        /*
            return impl_->numel();
        */
    }

    /**
      | Length of one array element in bytes.
      | This is the traditional Numpy naming.
      |
      */
    pub fn itemsize(&self) -> usize {
        
        todo!();
        /*
            return impl_->itemsize();
        */
    }

    /**
      | Same as itemsize(). This is the PyTorch
      | naming.
      |
      */
    pub fn element_size(&self) -> i64 {
        
        todo!();
        /*
            return static_cast<i64>(impl_->itemsize());
        */
    }

    #[deprecated = "Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device()."]
    pub fn ty(&self) -> &mut DeprecatedTypeProperties {
        
        todo!();
        /*
            return globalDeprecatedTypePropertiesRegistry().getDeprecatedTypeProperties(
            dispatchKeyToBackend(legacyExtractDispatchKey(key_set())),
            scalar_type());
        */
    }
    
    pub fn key_set(&self) -> DispatchKeySet {
        
        todo!();
        /*
            return impl_->key_set();
        */
    }
    
    pub fn scalar_type(&self) -> ScalarType {
        
        todo!();
        /*
            return typeMetaToScalarType(impl_->dtype());
        */
    }
    
    pub fn has_storage(&self) -> bool {
        
        todo!();
        /*
            return defined() && impl_->has_storage();
        */
    }
    
    pub fn storage(&self) -> &Storage {
        
        todo!();
        /*
            return impl_->storage();
        */
    }
    
    pub fn is_alias_of(&self, other: &Tensor) -> bool {
        
        todo!();
        /*
            return impl_->storage().is_alias_of(other.storage());
        */
    }
    
    pub fn to_type(&self, t: ScalarType) -> Tensor {
        
        todo!();
        /*
        
        */
    }
    
    pub fn to_backend(&self, b: Backend) -> Tensor {
        
        todo!();
        /*
        
        */
    }

    #[deprecated = "Tensor.is_variable() is deprecated; everything is a variable now. (If you want to assert that variable has been appropriately handled already, use variable_excluded_from_dispatch())"]
    pub fn is_variable(&self) -> bool {
        
        todo!();
        /*
            return !variable_excluded_from_dispatch();
        */
    }
    
    #[inline] pub fn is_conj(&self) -> bool {
        
        todo!();
        /*
            return impl_->is_conj();
        */
    }

    /**
      | sets the conjugate bit of a tensor.
      |
      | NOTE: Conjugate bit is supposed to be
      | a read-only field. Only change this, if you
      | are extremely sure that's what you
      | want. Changing this might lead to incorrect
      | behavior since conjugation is a lazy
      | operation and we rely on this bit to
      | determine if a conjugation needs to be
      | materialized.
      |
      */
    #[inline] pub fn set_conj(&self, conjugate: bool)  {
        
        todo!();
        /*
            impl_->_set_conj(conjugate);
        */
    }

    /// Returns a `Tensor`'s layout.
    pub fn layout(&self) -> Layout {
        
        todo!();
        /*
            return impl_->layout();
        */
    }

    /// Returns a `Tensor`'s dtype (`TypeMeta`).
    pub fn dtype(&self) -> TypeMeta {
        
        todo!();
        /*
            return impl_->dtype();
        */
    }

    /// Returns a `Tensor`'s device.
    #[inline] pub fn device(&self) -> Device {
        
        todo!();
        /*
            return impl_->device();
        */
    }

    /// Returns a `Tensor`'s device index.
    pub fn get_device(&self) -> i64 {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->get_device();
        */
    }

    /// Returns if a `Tensor` has CPU backend.
    pub fn is_cpu(&self) -> bool {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->is_cpu();
        */
    }
    
    /**
      | Returns if a `Tensor` has CUDA backend.
      |
      */
    pub fn is_cuda(&self) -> bool {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->is_cuda();
        */
    }

    /// Returns if a `Tensor` has XPU backend.
    pub fn is_xpu(&self) -> bool {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->is_xpu();
        */
    }

    /// Returns if a `Tensor` has XLA backend.
    pub fn is_xla(&self) -> bool {
        
        todo!();
        /*
            return impl_->is_xla();
        */
    }

    /// Returns if a `Tensor` has HIP backend.
    pub fn is_hip(&self) -> bool {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->is_hip();
        */
    }

    /// Returns if a `Tensor` has sparse backend.
    pub fn is_sparse(&self) -> bool {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->is_sparse();
        */
    }

    /// Returns is a `Tensor` has a sparse CSR backend.
    pub fn is_sparse_csr(&self) -> bool {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->is_sparse_csr();
        */
    }

    /// Returns if a `Tensor` is mkldnn tensor.
    pub fn is_mkldnn(&self) -> bool {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->is_mkldnn();
        */
    }

    /// Returns if a `Tensor` is mlc tensor.
    pub fn is_mlc(&self) -> bool {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->is_mlc();
        */
    }

    /**
      | Returns if a `Tensor` is vulkan tensor.
      |
      */
    pub fn is_vulkan(&self) -> bool {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->is_vulkan();
        */
    }

    /**
      | Returns if a `Tensor` is metal tensor.
      |
      */
    pub fn is_metal(&self) -> bool {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->is_metal();
        */
    }

    /**
      | Returns if a `Tensor` has quantized
      | backend.
      |
      */
    pub fn is_quantized(&self) -> bool {
        
        todo!();
        /*
            // NB: this is not a native function to avoid dispatching overhead.
        return impl_->is_quantized();
        */
    }

    /**
      | Returns if a `Tensor` is a meta tensor.
      | Meta tensors can also have other
      | designations.
      |
      */
    pub fn is_meta(&self) -> bool {
        
        todo!();
        /*
            return impl_->is_meta();
        */
    }

    /**
      | Returns if a `Tensor` is an inference
      | tensor.
      |
      */
    pub fn is_inference(&self) -> bool {
        
        todo!();
        /*
            return impl_->is_inference();
        */
    }

    /**
      | If a tensor is a quantized tensor, returns
      | its quantizer
      |
      | TODO: it's not in native_functions.yaml
      | yet as it's not exposed to python
      |
      */
    pub fn quantizer(&self) -> QuantizerPtr {
        
        todo!();
        /*
        
        */
    }

    /// Returns if a `Tensor` has any dimension names
    pub fn has_names(&self) -> bool {
        
        todo!();
        /*
            // If a user is using unnamed tensors, then we can short-circuit right here.
        // Otherwise, has_names attempts to retrieve names.
        if (!impl_->has_named_tensor_meta()) {
          return false;
        }
        return has_names(unsafeGetTensorImpl());
        */
    }

    /// Returns a `Tensor`'s dimension names data
    /// structure
    ///
    pub fn get_named_tensor_meta(&self) -> *const dyn NamedTensorMetaInterface {
        
        todo!();
        /*
            return static_cast<NamedTensorMetaInterface*>(impl_->named_tensor_meta());
        */
    }
    
    pub fn get_named_tensor_meta_mut(&mut self) -> *mut dyn NamedTensorMetaInterface {
        
        todo!();
        /*
            return static_cast<dyn NamedTensorMetaInterface*>(impl_->named_tensor_meta());
        */
    }

    /**
      | Returns the `TensorOptions` corresponding
      | to this `Tensor`. Defined in TensorOptions.h.
      |
      */
    pub fn options(&self) -> TensorOptions {
        
        todo!();
        /*
        
        */
    }
    
    pub fn data_ptr(&self)  {
        
        todo!();
        /*
            return this->unsafeGetTensorImpl()->data();
        */
    }
    
    pub fn data_ptr_mut<T>(&self) -> *mut T {
    
        todo!();
        /*
        
        */
    }

    pub fn item<T>(&self) -> T {
    
        todo!();
        /*
        
        */
    }

    /**
      | Purposely not defined here to avoid
      | inlining
      |
      */
    pub fn print(&self)  {
        
        todo!();
        /*
        
        */
    }

    /**
      | Return a `TensorAccessor` for CPU `Tensor`s.
      | You have to specify scalar type and dimension.
      |
      */
    pub fn accessor<T, const N: usize>(&mut self) -> TensorAccessor<T,N> {
    
        todo!();
        /*
            static_assert(N > 0, "accessor is used for indexing tensor, for scalars use *data_ptr<T>()");
        TORCH_CHECK(dim() == N, "TensorAccessor expected ", N, " dims but tensor has ", dim());
        return TensorAccessor<T,N>(data_ptr<T>(),sizes().data(),strides().data());
        */
    }

    /**
      | Return a `GenericPackedTensorAccessor` for
      | CUDA `Tensor`s. You have to specify scalar type
      | and dimension.
      |
      | You can optionally specify RestrictPtrTraits as
      | a template parameter to cast the data pointer
      | to a __restrict__ pointer.
      |
      | In order to use this, your CUDA kernel has to
      | take a corresponding
      | GenericPackedTensorAccessor as an argument.
      */
    pub fn generic_packed_accessor<T, const N: usize, PtrTraits /* = DefaultPtrTraits */, Index /* = i64 */>(&mut self) 
        -> GenericPackedTensorAccessor<T,N,PtrTraits,Index> {
    
        todo!();
        /*
            static_assert(N > 0, "accessor is used for indexing tensor, for scalars use *data_ptr<T>()");
        TORCH_CHECK(dim() == N, "TensorAccessor expected ", N, " dims but tensor has ", dim());
        return GenericPackedTensorAccessor<T,N,PtrTraits,Index>(static_cast<typename PtrTraits<T>::PtrType>(data_ptr<T>()),sizes().data(),strides().data());
        */
    }
    
    pub fn packed_accessor32<T, const N: usize, PtrTraits /* = DefaultPtrTraits */>(&mut self) -> PackedTensorAccessor32<T,N,PtrTraits> {
    
        todo!();
        /*
            return generic_packed_accessor<T,N,PtrTraits,i32>();
        */
    }
    
    pub fn packed_accessor64<T, const N: usize, PtrTraits /* = DefaultPtrTraits */>(&mut self) -> PackedTensorAccessor64<T,N,PtrTraits> {
    
        todo!();
        /*
            return generic_packed_accessor<T,N,PtrTraits,i64>();
        */
    }

    lazy_static!{
        /*
        Tensor operator~() const;
          Tensor operator-() const;
          Tensor& operator+=(const Tensor & other);
          Tensor& operator+=(Scalar other);
          Tensor& operator-=(const Tensor & other);
          Tensor& operator-=(Scalar other);
          Tensor& operator*=(const Tensor & other);
          Tensor& operator*=(Scalar other);
          Tensor& operator/=(const Tensor & other);
          Tensor& operator/=(Scalar other);
          Tensor& operator&=(const Tensor & other);
          Tensor& operator|=(const Tensor & other);
          Tensor& operator^=(const Tensor & other);
          Tensor operator[](Scalar index) const;
          Tensor operator[](Tensor index) const;
          Tensor operator[](i64 index) const;
        */
    }
    
    pub fn cpu(&self) -> Tensor {
        
        todo!();
        /*
        
        */
    }
    
    pub fn cuda(&self) -> Tensor {
        
        todo!();
        /*
        
        */
    }
    
    pub fn hip(&self) -> Tensor {
        
        todo!();
        /*
        
        */
    }
    
    pub fn vulkan(&self) -> Tensor {
        
        todo!();
        /*
        
        */
    }
    
    pub fn metal(&self) -> Tensor {
        
        todo!();
        /*
        
        */
    }

    // ~~~~~ Autograd API ~~~~~

    /// \fn bool is_leaf() const;
    ///
    /// All Tensors that have `requires_grad()` which is ``false`` will be leaf Tensors by convention.
    ///
    /// For Tensors that have `requires_grad()` which is ``true``, they will be leaf Tensors if they were
    /// created by the user. This means that they are not the result of an operation and so
    /// `grad_fn()` is `nullptr`.
    ///
    /// Only leaf Tensors will have their `grad()` populated during a call to `backward()`.
    /// To get `grad()` populated for non-leaf Tensors, you can use `retain_grad()`.
    ///
    /// Example:
    /// @code
    /// auto a = Torchrand(10, Torchrequires_grad());
    /// cout << a.is_leaf() << endl; // prints `true`
    ///
    /// auto b = Torchrand(10, Torchrequires_grad()).to(TorchkCUDA);
    /// cout << b.is_leaf() << endl; // prints `false`
    /// // b was created by the operation that cast a cpu Tensor into a cuda Tensor
    ///
    /// auto c = Torchrand(10, Torchrequires_grad()) + 2;
    /// cout << c.is_leaf() << endl; // prints `false`
    /// // c was created by the addition operation
    ///
    /// auto d = Torchrand(10).cuda();
    /// cout << d.is_leaf() << endl; // prints `true`
    /// // d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)
    ///
    /// auto e = Torchrand(10).cuda().requires_grad_();
    /// cout << e.is_leaf() << endl; // prints `true`
    /// // e requires gradients and has no operations creating it
    ///
    /// auto f = Torchrand(10, Torchdevice(TorchkCUDA).requires_grad(true));
    /// cout << f.is_leaf() << endl; // prints `true`
    /// // f requires grad, has no operation creating it
    /// @endcode

    /**
      | \fn void backward(const Tensor & gradient={}, optional<bool> retain_graph=nullopt, bool create_graph=false, optional<TensorList> inputs=nullopt) const;
      |
      | Computes the gradient of current tensor
      | with respect to graph leaves.
      | 
      | The graph is differentiated using the
      | chain rule. If the tensor is non-scalar
      | (i.e. its data has more than one element)
      | and requires gradient, the function
      | additionally requires specifying
      | ``gradient``.
      | 
      | It should be a tensor of matching type
      | and location, that contains the gradient
      | of the differentiated function w.r.t.
      | this Tensor.
      | 
      | This function accumulates gradients
      | in the leaves - you might need to zero
      | them before calling it.
      | 
      | -----------
      | @param gradient
      | 
      | Gradient w.r.t. the tensor. If it is
      | a tensor, it will be automatically converted
      | to a Tensor that does not require grad
      | unless ``create_graph`` is True.
      | 
      | None values can be specified for scalar
      | Tensors or ones that don't require grad.
      | If a None value would be acceptable then
      | this argument is optional.
      | ----------
      | @param retain_graph
      | 
      | If ``false``, the graph used to compute
      | the grads will be freed. Note that in
      | nearly all cases setting this option
      | to True is not needed and often can be
      | worked around in a much more efficient
      | way. Defaults to the value of ``create_graph``.
      | ----------
      | @param create_graph
      | 
      | If ``true``, graph of the derivative
      | will be constructed, allowing to compute
      | higher order derivative products.
      | Defaults to ``false``.
      | ----------
      | @param inputs
      | 
      | Inputs w.r.t. which the gradient will
      | be accumulated into ``Tensor::grad``.
      | All other Tensors will be ignored. If
      | not provided, the gradient is accumulated
      | into all the leaf Tensors that were used
      | to compute the current tensor. All the
      | provided inputs must be leaf Tensors.
      |
      */
    pub fn backward(&self, 
        gradient:     Option<&Tensor>,
        retain_graph: Option<bool>,
        create_graph: Option<bool>,
        inputs:       Option<TensorList>)  {

        let gradient:     &Tensor = gradient.unwrap_or(&Tensor::default());
        let create_graph: bool    = create_graph.unwrap_or(false);

        todo!();
        /*
            // NB: Adding this wrapper to _backward here because we'd like our
        // 'backwards' api to accept the 'inputs' argument optionally. Since code gen
        // currently does not support optional of TensorList our approach is to replace
        // backward in native_functions.yaml with _backward and call it here instead.
        if (inputs.has_value()) {
          TORCH_CHECK(inputs.value().size() > 0, "'inputs' argument to backward cannot be empty")
          this->_backward(inputs.value(), gradient, retain_graph, create_graph);
        } else {
          this->_backward({}, gradient, retain_graph, create_graph);
        }
        */
    }

    /**
      | \fn Tensor detach() const;
      |
      | Returns a new Tensor, detached from the current graph.
      | The result will never require gradient.
      |
      | \fn Tensor & detach_() const;
      |
      | Detaches the Tensor from the graph that created it, making it a leaf.
      | Views cannot be detached in-place.
      |
      | \fn void retain_grad() const;
      |
      | Enables this Tensor to have their :attr:`grad` populated during
      | :func:`backward`. This is a no-op for leaf tensors.
      |
      | \fn bool retains_grad() const;
      |
      | Is ``true`` if this Tensor is non-leaf and its :attr:`grad` is enabled to be
      | populated during :func:`backward`, ``false`` otherwise.
      */
    pub fn set_requires_grad(&self, requires_grad: bool) -> &Tensor {
        
        todo!();
        /*
            impl_->set_requires_grad(requires_grad);
        return *this;
        */
    }
    
    pub fn requires_grad(&self) -> bool {
        
        todo!();
        /*
            return impl_->requires_grad();
        */
    }

    /**
      | Return a mutable reference to the
      | gradient. This is conventionally used as
      | `t.grad() = x` to set a gradient to
      | a completely new tensor.
      |
      | Note that this function work with a non-const
      | Tensor and is not thread safe.
      */
    pub fn mutable_grad(&self) -> &mut Tensor {
        
        todo!();
        /*
            return impl_->mutable_grad();
        */
    }

    /**
      | This function returns an undefined tensor by
      | default and returns a defined tensor the
      | first time a call to `backward()` computes
      | gradients for this Tensor.
      |
      | The attribute will then contain the
      | gradients computed and future calls to
      | `backward()` will accumulate (add)
      | gradients into it.
      */
    pub fn grad(&self) -> &Tensor {
        
        todo!();
        /*
            const Tensor& maybe_grad = impl_->grad();
        if (!is_leaf() && !retains_grad() && !maybe_grad.defined()) {
          TORCH_WARN(
            "The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "
            "attribute won't be populated during autograd.backward(). If you indeed want the .grad "
            "field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. "
            "If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor "
            "instead. See github.com/pytorch/pytorch/pull/30531 for more informations.");
        }
        return maybe_grad;
        */
    }

    /**
      | The Forward AD API functions below are low
      | level and are not to be used by end users who
      |   should use the API provided in
      |   torch/csrc/autograd.h
      |
      | This function returns the forward gradient for
      | this Tensor at the given level.
      */
    pub fn fw_grad(&self, level: u64) -> &Tensor {
        
        todo!();
        /*
            return impl_->_fw_grad(level, *this);
        */
    }

    /**
      | This function can be used to set the value
      | of the forward grad.
      |
      | Note that the given new_grad might not be used
      |   directly if it has different metadata
      |   (size/stride/storage offset) compared to
      |   this Tensor. In that case, new_grad content
      |   will be copied into a new Tensor
      |
      */
    pub fn set_fw_grad(&self, 
        new_grad:      &Tensor,
        level:         u64,
        is_inplace_op: bool)  {
        
        todo!();
        /*
            impl_->_set_fw_grad(new_grad, *this, level, is_inplace_op);
        */
    }

    /**
      | Special C++ only overloads for std()-like
      | functions (See gh-40287)
      |
      | These are needed because int -> bool conversion
      | takes precedence over int -> IntArrayRef
      |
      | So, for example std(0) would select the
      | std(unbiased=False) overload
      */
    pub fn var(&self, dim: i32) -> Tensor {
        
        todo!();
        /*
            return var(IntArrayRef{dim});
        */
    }
    
    pub fn std(&self, dim: i32) -> Tensor {
        
        todo!();
        /*
            return std(IntArrayRef{dim});
        */
    }

    /**
      | We changed .dtype() to return a TypeMeta in
      | #12766. Ideally, we want the kDouble and
      | its friends to be TypeMeta's, but that
      | hasn't happened yet.
      |
      | Before that change, we make this method to
      | maintain BC for C++ usage like
      | `x.to(y.dtype)`.
      |
      | TODO: remove following two after kDouble
      | and its friends are TypeMeta's.
      */
    #[inline] pub fn to(&self, 
        type_meta:    TypeMeta,
        non_blocking: Option<bool>,
        copy_:        Option<bool>) -> Tensor {

        let non_blocking: bool = non_blocking.unwrap_or(false);
        let copy_:        bool = copy_.unwrap_or(false);

        todo!();
        /*
            return this->to(/*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy);
        */
    }
    
    #[inline] pub fn to_with_device(&self, 
        device:       Device,
        type_meta:    TypeMeta,
        non_blocking: Option<bool>,
        copy_:        Option<bool>) -> Tensor {

        let non_blocking: bool = non_blocking.unwrap_or(false);
        let copy_:        bool = copy_.unwrap_or(false);

        todo!();
        /*
            return this->to(device, /*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy);
        */
    }
    
    pub fn m<F, Args, R>(&self, 
        func:   F,
        params: Args) -> R {
    
        todo!();
        /*
            return func(*this, forward<Args>(params)...);
        */
    }

    /**
      | NOTE: This is similar to the legacy
      | `.data()` function on `Variable`, and is
      |   intended to be used from functions that need
      |   to access the `Variable`'s equivalent
      |   `Tensor` (i.e. `Tensor` that shares the same
      |   storage and tensor metadata with the
      |   `Variable`).
      |
      | One notable difference with the legacy
      |   `.data()` function is that changes to the
      |   returned `Tensor`'s tensor metadata
      |   (e.g. sizes / strides / storage
      |   / storage_offset) will not update the
      |   original `Variable`, due to the fact that
      |   this function shallow-copies the
      |   `Variable`'s underlying TensorImpl.
      |
      */
    pub fn tensor_data(&self) -> Tensor {
        
        todo!();
        /*
        
        */
    }

    /**
      | NOTE: `var.variable_data()` in C++ has the
      | same semantics as `tensor.data` in Python,
      | which create a new `Variable` that shares the
      | same storage and tensor metadata with the
      | original `Variable`, but with a completely new
      | autograd history.
      |
      | NOTE: If we change the tensor metadata
      | (e.g. sizes / strides / storage
      | / storage_offset) of a variable created from
      | `var.variable_data()`, those changes will not
      | update the original variable `var`. In
      | `.variable_data()`, we set
      | `allow_tensor_metadata_change_` to false to
      | make such changes explicitly illegal, in order
      | to prevent users from changing metadata of
      | `var.variable_data()` and expecting the
      | original variable `var` to also be updated.
      */
    pub fn variable_data(&self) -> Tensor {
        
        todo!();
        /*
        
        */
    }

    // Gradient Node and Edges
    //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    /**
      | Gets the gradient function of the
      | `Variable`. If this is a leaf variable,
      | the pointer returned will be null.
      |
      | For View Variables:
      |
      | Gets the up-to-date grad_fn. If the shared
      | data or base was modified, we re-create
      | the grad_fn to express the up-to-date view
      | relationship between this and the base
      | Variable.
      */
    pub fn grad_fn(&self) -> &Arc<TorchautogradNode> {
        
        todo!();
        /*
        
        */
    }

    // Hooks
    //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    lazy_static!{
        /*
        template <typename T>
          using hook_return_void_t = enable_if_t<is_void<typename result_of<T&(Tensor)>::type>::value, unsigned>;

          template <typename T>
          using hook_return_var_t = enable_if_t<is_same<typename result_of<T&(Tensor)>::type, Tensor>::value, unsigned>;
        */
    }

    /**
      | Registers a backward hook.
      |
      | The hook will be called every time
      | a gradient with respect to the Tensor is
      | computed.
      |
      | The hook should have one of the following
      | signature:
      |
      | ``` hook(Tensor grad) -> Tensor ``` ```
      | hook(Tensor grad) -> void ``` The hook
      | should not modify its argument, but it can
      | optionally return a new gradient which
      | will be used in place of `grad`.
      |
      | This function returns the index of the
      | hook in the list which can be used to
      | remove hook.
      |
      | Example:
      | @code
      | auto v = Torchtensor({0., 0., 0.}, Torchrequires_grad());
      | auto h = v.register_hook([](TorchTensor grad){ return grad * 2; }); // double the gradient
      | v.backward(Torchtensor({1., 2., 3.}));
      | // This prints:
      | // ```
      | //  2
      | //  4
      | //  6
      | // [ CPUFloatType{3} ]
      | // ```
      | cout << v.grad() << endl;
      | v.remove_hook(h);  // removes the hook
      | @endcode
      */
    pub fn register_hook_return_void<T>(&self, hook: T) -> HookReturnVoid<T> {
    
        todo!();
        /*
        
        */
    }
    
    pub fn register_hook_return_var<T>(&self, hook: T) -> HookReturnVar<T> {
    
        todo!();
        /*
        
        */
    }
    
    pub fn register_hook_return_u32(&self, hook: fn(_0: &Tensor) -> Tensor) -> u32 {
        
        todo!();
        /*
        
        */
    }

    /// Remove hook at given position
    pub fn remove_hook(&self, pos: u32)  {
        
        todo!();
        /*
        
        */
    }

    // Variable methods
    //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    pub fn is_leaf(&self) -> bool {
        
        todo!();
        /*
        
        */
    }
    
    pub fn output_nr(&self) -> i64 {
        
        todo!();
        /*
        
        */
    }
    
    pub fn set_data(&self, new_data: &Tensor)  {
        
        todo!();
        /*
        
        */
    }
    
    pub fn version(&self) -> i64 {
        
        todo!();
        /*
        
        */
    }
    
    pub fn retain_grad(&self)  {
        
        todo!();
        /*
        
        */
    }
    
    pub fn retains_grad(&self) -> bool {
        
        todo!();
        /*
        
        */
    }
    
    pub fn requires_grad_with(&self, requires_grad: Option<bool>) -> &Tensor {

        let requires_grad: bool = requires_grad.unwrap_or(true);

        todo!();
        /*
        
        */
    }

    // View Variables
    //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    /**
      | Returns true if this `Variable` is a
      | view of another `Variable`.
      |
      */
    pub fn is_view(&self) -> bool {
        
        todo!();
        /*
        
        */
    }

    /**
      | Returns the `Variable` that this `Variable`
      | is a view of. If this `Variable` is not
      | a view, throw a `runtime_error`.
      |
      */
    pub fn base(&self) -> &Tensor {
        
        todo!();
        /*
        
        */
    }

    // Miscellaneous
    //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
    pub fn name(&self) -> &String {
        
        todo!();
        /*
        
        */
    }
    
    pub fn enforce_invariants(&mut self)  {
        
        todo!();
        /*
        
        */
    }
}

#[inline] pub fn get_device(self_: &Tensor) -> i64 {
    
    todo!();
        /*
            return self.get_device();
        */
}

lazy_static!{
    /*
    template <typename T>
    auto Tensor::register_hook(T&& hook) const -> Tensor::hook_return_void_t<T> {
      // Return the grad argument in case of a hook with void return type to have an
      // function with Tensor return type
      static_assert(is_same<decltype(hook(Tensor())), void>::value,
                    "Expected hook to return void");
      return _register_hook([fn=forward<T>(hook)](const Tensor& grad) {
        fn(grad);
        return Tensor();
      });
    }
    */
}

lazy_static!{
    /*
    template <typename T>
    auto Tensor::register_hook(T&& hook) const -> Tensor::hook_return_var_t<T> {
      return _register_hook(forward<T>(hook));
    }
    */
}

/**
  | Helper creator for Tensor class which doesn't
  | requires the users to pass in an intrusive_ptr
  | instead it just converts the argument passed to
  | requested intrusive_ptr type.
  |
  */
pub fn make_tensor<T, Args>(args: Args) -> Tensor {

    todo!();
        /*
            return Tensor(make_intrusive<T>(forward<Args>(args)...));
        */
}

#[inline] pub fn legacy_extract_dispatch_key(t: &Tensor) -> DispatchKey {
    
    todo!();
        /*
            return legacyExtractDispatchKey(t.key_set());
        */
}

pub struct MaybeOwnedTraitsTensor {

}

pub mod maybe_owned_traits_tensor {
    use super::*;

    pub type OwnedType  = Tensor;
    pub type BorrowType = Tensor;
}

impl MaybeOwnedTraitsTensor {

    pub fn create_borrow(from: &maybe_owned_traits_tensor::OwnedType) -> maybe_owned_traits_tensor::BorrowType {
        
        todo!();
        /*
            // NOTE: this can be implemented without the special
        // unsafe_borrow_t Tensor constructor as
        //
        // return borrow_type(intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(from.unsafeGetTensorImpl()));
        //
        // but that hurts inlining due to the nullptr check in the
        // Tensor(intrusive_ptr<...>) constructor. We already know
        // that from.impl_ isn't null because from is a valid Tensor, so
        // we needn't do the check again. (using __builtin_assume can
        // avoid this, but wouldn't be portable to MSVC.)
        return borrow_type(borrow_type::unsafe_borrow_t{}, from);
        */
    }
    
    pub fn assign_borrow(
        lhs: &mut maybe_owned_traits_tensor::BorrowType,
        rhs: &maybe_owned_traits_tensor::BorrowType)  {
        
        todo!();
        /*
            lhs.unsafeReleaseTensorImpl();
        // See above note: this can be implemented with public API
        // similarly to createBorrow(), but that would hurt inlining.
        lhs = borrow_type(borrow_type::unsafe_borrow_t{}, rhs);
        */
    }
    
    pub fn destroy_borrow(to_destroy: &mut maybe_owned_traits_tensor::BorrowType)  {
        
        todo!();
        /*
            toDestroy.unsafeReleaseTensorImpl(); // "leak" it, but it was already +0.
        */
    }
    
    pub fn reference_from_borrow(borrow: &maybe_owned_traits_tensor::BorrowType) -> &maybe_owned_traits_tensor::OwnedType {
        
        todo!();
        /*
            return borrow;
        */
    }
    
    pub fn pointer_from_borrow(borrow: &maybe_owned_traits_tensor::BorrowType) -> *const maybe_owned_traits_tensor::OwnedType {
        
        todo!();
        /*
            return &borrow;
        */
    }
    
    pub fn debug_borrow_is_valid(borrow: &maybe_owned_traits_tensor::BorrowType) -> bool {
        
        todo!();
        /*
            return true;
        */
    }
}


pub struct ExclusivelyOwnedTraitsTensor {

}

pub mod exclusively_owned_traits_tensor {

    use super::*;

    pub type ReprType         = Tensor;
    pub type PointerType      = *mut Tensor;
    pub type ConstPointerType = *const Tensor;
}

impl ExclusivelyOwnedTraitsTensor {

    
    pub fn null_repr() -> exclusively_owned_traits_tensor::ReprType {
        
        todo!();
        /*
            return Tensor();
        */
    }
    
    
    pub fn create_in_place<Args>(args: Args) -> exclusively_owned_traits_tensor::ReprType {
    
        todo!();
        /*
            return Tensor(forward<Args>(args)...);
        */
    }
    
    pub fn move_to_repr(x: Tensor) -> exclusively_owned_traits_tensor::ReprType {
        
        todo!();
        /*
            return move(x);
        */
    }
    
    pub fn destroy_owned(x: &mut Tensor)  {
        
        todo!();
        /*
            TensorImpl*const toDestroy = x.unsafeReleaseTensorImpl();
        TORCH_INTERNAL_ASSERT_DEBUG_ONLY(toDestroy != nullptr, "Tensor somehow got null TensorImpl?");
        // May be 0 because UndefinedTensorImpl doesn't get its refcount
        // incremented.
        TORCH_INTERNAL_ASSERT_DEBUG_ONLY(
            toDestroy->refcount_ == 1 || (toDestroy->refcount_ == 0 && toDestroy == UndefinedTensorImpl::singleton()),
            "ExclusivelyOwned<Tensor> destroyed with refcount ", toDestroy->refcount_, ", expected 1!");
        TORCH_INTERNAL_ASSERT_DEBUG_ONLY(
            toDestroy->weakcount_ == 1 || (toDestroy->weakcount_ == 0 && toDestroy == UndefinedTensorImpl::singleton()),
            "ExclusivelyOwned<Tensor> destroyed with weakcount ", toDestroy->weakcount_, ", expected 1!");
        if (toDestroy != UndefinedTensorImpl::singleton()) {
    #ifndef NDEBUG
          // Needed to pass the debug assertions in ~intrusive_ptr_target.
          toDestroy->refcount_ = 0;
          toDestroy->weakcount_ = 0;
    #endif
          toDestroy->release_resources();
          delete toDestroy;
        }
        */
    }
    
    pub fn take(x: &mut Tensor) -> Tensor {
        
        todo!();
        /*
            return move(x);
        */
    }
    
    pub fn get_impl_mut(x: &mut exclusively_owned_traits_tensor::ReprType) -> exclusively_owned_traits_tensor::PointerType {
        
        todo!();
        /*
            return &x;
        */
    }
    
    pub fn get_impl(x: &exclusively_owned_traits_tensor::ReprType) -> exclusively_owned_traits_tensor::ConstPointerType {
        
        todo!();
        /*
            return &x;
        */
    }
}

#[inline] pub fn borrow_from_optional_tensor(opt: &Option<Tensor>) -> MaybeOwned<Tensor> {
    
    todo!();
        /*
            return opt.has_value()
        ? MaybeOwned<Tensor>::borrowed(*opt)
        : MaybeOwned<Tensor>::owned(in_place);
        */
}

impl Tensor {
    
    /**
      | Should be used if *this can reasonably be
      | expected to be contiguous and performance is
      | important.
      |
      | Compared to contiguous, it saves a reference
      | count increment/decrement if *this is
      | already contiguous, at the cost in all cases
      | of an extra pointer of stack usage, an extra
      | branch to access, and an extra branch at
      | destruction time.
      |
      */
    #[inline] pub fn expect_contiguous(&mut self, memory_format: Option<MemoryFormat>) -> MaybeOwned<Tensor> {

        let memory_format: MemoryFormat = memory_format.unwrap_or(MemoryFormat::Contiguous);
        
        todo!();
        /*
        if (is_contiguous(memory_format)) {
          return MaybeOwned<Tensor>::borrowed(*this);
        } else {
          return MaybeOwned<Tensor>::owned(__dispatch_contiguous(memory_format));
        }
        */
    }
}
