crate::ix!();

//-------------------------------------------[.cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h]

/**
  | Implement this interface and register
  | your instance with the dispatcher to
  | get notified when operators are registered
  | or deregistered with the dispatcher.
  | 
  | NB: registration events only occur
  | when a 'def' occurs; we don't trigger
  | on 'impl' or 'fallback' calls.
  |
  */
pub trait OpRegistrationListener:
OnOperatorRegistered
+ OnOperatorDeregistered {}

pub trait OnOperatorRegistered {
    
    fn on_operator_registered(&mut self, op: &OperatorHandle);
}

pub trait OnOperatorDeregistered {

    fn on_operator_deregistered(&mut self, op: &OperatorHandle);
}

pub struct OperatorDef {

    op: OperatorEntry,

    // These refer to the number of outstanding
    // RegistrationHandleRAII for this operator.
    // def_count reflects only def() registrations
    // (in the new world, this should only ever be
    // 1, but old style registrations may register
    // the schema multiple times, which will
    // increase this count).  
    //
    // def_and_impl_count reflects the number of
    // combined def() and impl() registrations.  
    //
    // When the last def() gets unregistered, we
    // must immediately call the Deregistered
    // listeners, but we must not actually delete
    // the handle as there are other outstanding
    // RAII destructors which will try to destruct
    // and they had better still have a working
    // operator handle in this case
    //
    def_count:          usize, // default = 0
    def_and_impl_count: usize, // default = 0
}

impl OperatorDef {
    
    pub fn new(op_name: OperatorName) -> Self {
    
        todo!();
        /*
        : op(move(op_name)),

        
        */
    }
}

/**
  | Top-level dispatch interface for dispatching
  | via the dynamic dispatcher.
  | 
  | Most end users shouldn't use this directly;
  | if you're trying to register ops look
  | in op_registration
  |
  */
pub struct Dispatcher {

    operators:                LinkedList<OperatorDef>,
    operator_lookup_table:    LeftRight<SkaFlatHashMap<OperatorName,OperatorHandle>>,

    /**
      | Map from namespace to debug string (saying,
      | e.g., where the library was defined)
      |
      */
    libraries:                SkaFlatHashMap<String,String>,

    backend_fallback_kernels: Array<AnnotatedKernel,DispatchKey_NumDispatchKeys>,
    listeners:                Box<RegistrationListenerList>,
    mutex:                    Mutex,
}

impl Dispatcher {

    /**
      | Implementation note: this class abstracts
      | over the fact that we have per-operator
      | dispatch tables.
      |
      | This could be easily adjusted to have
      | a single global hash table.
      */
    pub fn real_singleton() -> &mut Dispatcher {
        
        todo!();
        /*
        
        */
    }
    
    #[inline] pub fn singleton() -> &mut Dispatcher {
        
        todo!();
        /*
            #if !defined C10_MOBILE
        // Implemented inline so that steady-state code needn't incur
        // function-call overhead. We can't just inline `realSingleton`
        // because the function-local static would get duplicated across
        // all DSOs that include & use this header, leading to multiple
        // singleton instances.
        static Dispatcher& s = realSingleton();
        return s;
    #else
        // For C10_MOBILE, we should never inline a static function that
        // has a static member, since the generated code calls
        // __cxa_guard_acquire and __cxa_guard_release which help
        // implement exactly once semantics for the initialization of the
        // static Dispatcher& s above (for the non-mobile case). That
        // additional code when duplicated across all operator stubs
        // for every backend results in a lot of additional code
        // being generated by the compiler.
        return realSingleton();
    #endif
        */
    }

    // ------------------------------------------------------------------------
    //
    // Accessing operators by schema
    //
    // ------------------------------------------------------------------------

    /**
      | Looks for an operator schema with the
      | given name and overload name and returns
      | it if it is registered WITH A SCHEMA.
      | 
      | Returns nullopt otherwise.
      |
      */
    pub fn find_schema(&mut self, operator_name: &OperatorName) -> Option<OperatorHandle> {
        
        todo!();
        /*
        
        */
    }

    /**
      | Variant of findSchema that results
      | in less code generated at the call site.
      | 
      | It (1) takes const char* pointer rather
      | than OperatorName (so we skip generating
      | string constructor calls at the call
      | site), and (2) it raises an exception
      | if the operator is not found (so we skip
      | generating exception raising code
      | at the call site)
      | 
      | Irritatingly, we still have to generate
      | the handful of instructions for dealing
      | with an exception being thrown during
      | static initialization (e.g. __cxa_guard_abort).
      | If we could annotate this method noexcept
      | we could avoid this code too, but as the
      | name of the function suggests, it does
      | throw exceptions.
      |
      */
    pub fn find_schema_or_throw(&mut self, 
        name:          *const u8,
        overload_name: *const u8) -> OperatorHandle {
        
        todo!();
        /*
        
        */
    }

    /**
      | Like findSchema, but also returns
      | 
      | OperatorHandle even if there is no schema
      |
      */
    pub fn find_op(&mut self, operator_name: &OperatorName) -> Option<OperatorHandle> {
        
        todo!();
        /*
        
        */
    }

    /**
      | Returns a list of all operator names
      | present in the operatorLookupTable_
      |
      */
    pub fn get_all_op_names(&mut self) -> Vec<OperatorName> {
        
        todo!();
        /*
        
        */
    }

    // ------------------------------------------------------------------------
    //
    // Invoking operators
    //
    // ------------------------------------------------------------------------
    lazy_static!{
        /*
        template<class Return, class... Args>
          Return call(const TypedOperatorHandle<Return (Args...)>& op, Args... args) const;

          template<class Return, class... Args>
          static Return callWithDispatchKeySlowPath(const TypedOperatorHandle<Return (Args...)>& op, bool pre_sampled, DispatchKeySet dispatchKeySet, const KernelFunction& kernel, Args... args);

          // Like call, but intended for use in a redispatch in kernels that have explicitly performed the DispatchKey update calculatulation.
          // This will take the DispatchKeySet completely as is and dispatch to the kernel of the corresponding highest priority key in the set.
          // Note that this version of redispatch treats the inputted DispatchKeySet *as is*, and does NOT mask out the highest priority key.
          // See Note [Plumbing Keys Through The Dispatcher]
          template<class Return, class... Args>
          Return redispatch(const TypedOperatorHandle<Return (Args...)>& op, DispatchKeySet currentDispatchKeySet, Args... args) const;
        */
    }

    /**
      | Invoke an operator via the boxed calling
      | convention using an IValue stack
      |
      */
    pub fn call_boxed(&self, 
        op:    &OperatorHandle,
        stack: *mut Stack)  {
        
        todo!();
        /*
        
        */
    }

    /** 
     * TODO: This will only be useful if we write
     *  a backend fallback that plumbs dispatch
     *  keys (currently there are none)
     *
     * See Note [Plumbing Keys Through The
     * Dispatcher]
     */
    pub fn redispatch_boxed(&self, 
        op:               &OperatorHandle,
        dispatch_key_set: DispatchKeySet,
        stack:            *mut Stack)  {
        
        todo!();
        /*
        
        */
    }

    // ------------------------------------------------------------------------
    //
    // Performing registrations (NON user public; use op_registration)
    //
    // ------------------------------------------------------------------------

    /**
      * Register a new operator schema.
      * 
      * If a schema with the same operator name
      * and overload name already exists, this
      * function will check that both schemas
      * are exactly identical.
      *
      */
    pub fn register_def(&mut self, 
        schema: FunctionSchema,
        debug:  String) -> RegistrationHandleRAII {
        
        todo!();
        /*
        
        */
    }

    /**
      | Register a kernel to the dispatch table
      | for an operator.
      | 
      | If dispatch_key is nullopt, then this
      | registers a fallback kernel.
      | 
      | -----------
      | @return
      | 
      | A RAII object that manages the lifetime
      | of the registration.
      | 
      | Once that object is destructed, the
      | kernel will be deregistered.
      | 
      | NB: steals the inferred function schema,
      | as we may need to hold on to it for a bit
      | until the real schema turns up
      |
      */
    pub fn register_impl(&mut self, 
        op_name:                  OperatorName,
        dispatch_key:             Option<DispatchKey>,
        kernel:                   KernelFunction,
        cpp_signature:            Option<CppSignature>,
        inferred_function_schema: Box<FunctionSchema>,
        debug:                    String) -> RegistrationHandleRAII {
        
        todo!();
        /*
        
        */
    }

    /**
      | Register a new operator by name.
      |
      */
    pub fn register_name(&mut self, op_name: OperatorName) -> RegistrationHandleRAII {
        
        todo!();
        /*
        
        */
    }

    /**
      | Register a fallback kernel for a backend.
      | 
      | If an operator is called but there is
      | no concrete kernel for the dispatch
      | key of the given operator arguments,
      | it will check if there is such a fallback
      | kernel for the given dispatch key and,
      | if yes, call that one.
      |
      */
    pub fn register_fallback(&mut self, 
        dispatch_key: DispatchKey,
        kernel:       KernelFunction,
        debug:        String) -> RegistrationHandleRAII {
        
        todo!();
        /*
        
        */
    }

    /**
      | Use to register whenever we had a TORCH_LIBRARY
      | declaration in the frontend
      | 
      | API. These invocations are only permitted
      | once per program, so we raise an error
      | if this is called again for the same namespace.
      |
      */
    pub fn register_library(&mut self, 
        ns:    String,
        debug: String) -> RegistrationHandleRAII {
        
        todo!();
        /*
        
        */
    }

    // ------------------------------------------------------------------------
    //
    // Listeners on registrations
    //
    // ------------------------------------------------------------------------

    /**
      | Add a listener that gets called whenever
      | a new op is registered or an existing
      | op is deregistered. Immediately after
      | registering, this listener gets called
      | for all previously registered ops,
      | so it can be used to keep track of ops registered
      | with this dispatcher.
      |
      */
    pub fn add_registration_listener(&mut self, listener: Box<OpRegistrationListener>) -> RegistrationHandleRAII {
        
        todo!();
        /*
        
        */
    }
    
    pub fn check_invariants(&self)  {
        
        todo!();
        /*
        
        */
    }

    // ------------------------------------------------------------------------
    //
    // Assertions
    //
    // ------------------------------------------------------------------------

    /**
      | For testing purposes.
      | 
      | Returns a list of all operators that
      | were created through calls to registerImpl(),
      | without any corresponding calls to
      | registerDef().
      | 
      | After static initialization is done
      | this is almost certainly a bug, as the
      | created OperatorHandle won't have
      | any schema associated with it and users
      | calling the op through the dispatcher
      | won't be able to access it
      | 
      | -----------
      | @note
      | 
      | we cannot enforce this invariant "as
      | we go" during static initialization,
      | due to undefined static initialization
      | order- we have no guarantees over the
      | order in which .def() and .impl() calls
      | are registered in the dispatcher at
      | static initialization time. So this
      | function should only be called after
      | static initialization.
      |
      */
    pub fn find_dangling_impls(&self) -> Vec<OperatorHandle> {
        
        todo!();
        /*
        
        */
    }
    
    pub fn sequence_number_for_running_record_function(dispatch_key: DispatchKey) -> i64 {
        
        todo!();
        /*
        
        */
    }
    
    pub fn run_record_function(
        guard:        &mut RecordFunction,
        op:           &OperatorHandle,
        dispatch_key: DispatchKey)  {
        
        todo!();
        /*
        
        */
    }
    
    pub fn run_record_function(
        guard:        &mut RecordFunction,
        op:           &OperatorHandle,
        dispatch_key: DispatchKey,
        stack:        TorchJitStack)  {
        
        todo!();
        /*
        
        */
    }
    
    pub fn run_record_function(
        guard:        &mut RecordFunction,
        op:           &OperatorHandle,
        dispatch_key: DispatchKey,
        stack:        &TorchJitStack)  {
        
        todo!();
        /*
        
        */
    }
    
    pub fn find_or_register_schema(&mut self, schema: FunctionSchema) -> OperatorHandle {
        
        todo!();
        /*
        
        */
    }
    
    pub fn find_or_register_name(&mut self, op_name: &OperatorName) -> OperatorHandle {
        
        todo!();
        /*
        
        */
    }
    
    pub fn deregister_def(&mut self, 
        op:      &OperatorHandle,
        op_name: &OperatorName)  {
        
        todo!();
        /*
        
        */
    }
    
    pub fn deregister_impl(&mut self, 
        op:            &OperatorHandle,
        op_name:       &OperatorName,
        dispatch_key:  Option<DispatchKey>,
        kernel_handle: AnnotatedKernelIterator)  {
        
        todo!();
        /*
        
        */
    }
    
    pub fn deregister_name(&mut self, 
        op:      &OperatorHandle,
        op_name: &OperatorName)  {
        
        todo!();
        /*
        
        */
    }
    
    pub fn deregister_fallback(&mut self, dispatch_key: DispatchKey)  {
        
        todo!();
        /*
        
        */
    }
    
    pub fn deregister_library(&mut self, ns: &String)  {
        
        todo!();
        /*
        
        */
    }
    
    pub fn cleanup(&mut self, 
        op:      &OperatorHandle,
        op_name: &OperatorName)  {
        
        todo!();
        /*
        
        */
    }
    
    pub fn check_schema_compatibility(&mut self, 
        op:     &OperatorHandle,
        schema: &FunctionSchema,
        debug:  &String)  {
        
        todo!();
        /*
        
        */
    }
}

/**
  | This is a handle to an operator schema
  | registered with the dispatcher.
  | 
  | This handle can be used to register kernels
  | with the dispatcher or to lookup a kernel
  | for a certain set of arguments.
  |
  */
pub struct OperatorHandle {

    /**
      | Storing a direct pointer to the OperatorDef
      | even though we already have the iterator
      | saves an instruction in the critical
      | dispatch path. The iterator is effectively
      | a pointer-to-list-node, and (at least
      | in libstdc++'s implementation) the
      | element is at an offset 16 bytes from
      | that, because the prev/next pointers
      | come first in the list node struct. So,
      | an add instruction would be necessary
      | to convert from the iterator to an OperatorDef*.
      |
      */
    operator_def:      *mut Dispatcher_OperatorDef,

    /**
      | We need to store this iterator in order
      | to make
      | 
      | Dispatcher::cleanup() fast -- it runs
      | a lot on program termination (and presuambly
      | library unloading).
      |
      */
    operator_iterator: DispatcherOperatorDefIterator,
}

impl OperatorHandle {

    pub fn operator_name(&self) -> &OperatorName {
        
        todo!();
        /*
            return operatorDef_->op.operator_name();
        */
    }
    
    pub fn has_schema(&self) -> bool {
        
        todo!();
        /*
            return operatorDef_->op.hasSchema();
        */
    }
    
    pub fn schema(&self) -> &FunctionSchema {
        
        todo!();
        /*
            return operatorDef_->op.schema();
        */
    }
    
    pub fn debug(&self) -> &String {
        
        todo!();
        /*
            return operatorDef_->op.debug();
        */
    }
    
    pub fn dump_state(&self) -> String {
        
        todo!();
        /*
            return operatorDef_->op.dumpState();
        */
    }
    
    pub fn dump_computed_table(&self) -> String {
        
        todo!();
        /*
            return operatorDef_->op.dumpComputedTable();
        */
    }
    
    pub fn check_invariants(&self)  {
        
        todo!();
        /*
            return operatorDef_->op.checkInvariants();
        */
    }
    
    
    pub fn typed<FuncType>(&self) -> TypedOperatorHandle<FuncType> {
    
        todo!();
        /*
            // NB: This assert is not 100% sound: you can retrieve a typed() operator
        // handle prior to ANY C++ signature being registered on the operator
        // and the check will say everything is OK (at which point you can then
        // smuggle in a kernel that is typed incorrectly).  For everything
        // in core library this won't happen, because all the static registrations
        // will be done by the time a typed() handle is acquired.
    #if !defined C10_MOBILE
        operatorDef_->op.assertSignatureIsCorrect<FuncType>();
    #endif
        return TypedOperatorHandle<FuncType>(operatorIterator_);
        */
    }
    
    pub fn call_boxed(&self, stack: *mut Stack)  {
        
        todo!();
        /*
            Dispatcher::singleton().callBoxed(*this, stack);
        */
    }
    
    pub fn redispatch_boxed(&self, 
        ks:    DispatchKeySet,
        stack: *mut Stack)  {
        
        todo!();
        /*
            Dispatcher::singleton().redispatchBoxed(*this, ks, stack);
        */
    }
    
    pub fn new(operator_iterator: DispatcherOperatorDefIterator) -> Self {
    
        todo!();
        /*


            : operatorDef_(&*operatorIterator), operatorIterator_(operatorIterator)
        */
    }
}

/**
  | This is a handle to an operator schema
  | registered with the dispatcher.
  | 
  | It holds the same information as an OperatorHandle,
  | but it is templated on the operator arguments
  | and allows calling the operator in an
  | unboxed way.
  |
  */
lazy_static!{
    /*
    template<class Return, class... Args>
    class TypedOperatorHandle<Return (Args...)> final : public OperatorHandle {

      TypedOperatorHandle(TypedOperatorHandle&&) noexcept = default;
      TypedOperatorHandle& operator=(TypedOperatorHandle&&) noexcept = default;
      TypedOperatorHandle(const TypedOperatorHandle&) = default;
      TypedOperatorHandle& operator=(const TypedOperatorHandle&) = default;

      // See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&
      C10_ALWAYS_INLINE Return call(Args... args) const {
        return Dispatcher::singleton().call<Return, Args...>(*this, forward<Args>(args)...);
      }

      // See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&
      C10_ALWAYS_INLINE Return redispatch(DispatchKeySet currentDispatchKeySet, Args... args) const {
        return Dispatcher::singleton().redispatch<Return, Args...>(*this, currentDispatchKeySet, forward<Args>(args)...);
      }


      explicit TypedOperatorHandle(list<Dispatcher::OperatorDef>::iterator operatorIterator)
      : OperatorHandle(operatorIterator) {}
      friend class OperatorHandle;
    };

    namespace detail {
    template <class... Args> inline void unused_arg_(const Args&...) {}

    // CaptureKernelCall is intended to capture return values from Dispatcher
    // unboxed kernel calls. A record function may request to get outputs from the
    // kernel calls. For boxed kernels, it's straightforward, the returned values
    // are in the stack object. The stack can be passed to record functions. For
    // unboxed kernels, we need to handle different kinds of return values, cache
    // them temporarily, then release the values for the actual function call
    // return.
    template <typename ReturnType>
    struct CaptureKernelCall {
      template <typename F, typename... Args>
      CaptureKernelCall(
          const F& kernel,
          const TypedOperatorHandle<ReturnType(Args...)>& op,
          const DispatchKeySet& dispatchKeySet,
          Args&&... args)
          // Calls the kernel and capture the result in output_.
          : output_{kernel.template call<ReturnType, Args...>(
                op,
                dispatchKeySet,
                forward<Args>(args)...)} {}
      // Wraps the return values in a Stack.
      Stack getOutputs() {
        Stack stack;
        push_outputs<ReturnType, false>::copy(output_, &stack);
        return stack;
      }
      // Since we are returning the output_, we don't expect the output_ to be used
      // afterward. Copy elision and RVO do not apply to class data members. Using
      // move semantic to avoid copies when possible.
      ReturnType release() && {
        return move(output_);
      }

     
      ReturnType output_;
    };

    // Handle the lvalue reference differently since it should not be moved.
    template <>
    inline Tensor& CaptureKernelCall<Tensor&>::release() && {
      return output_;
    }

    // Handle case where the kernel returns void.
    template <>
    struct CaptureKernelCall<void> {
      template <typename F, typename... Args>
      CaptureKernelCall(
          const F& kernel,
          const TypedOperatorHandle<void(Args...)>& op,
          const DispatchKeySet& dispatchKeySet,
          Args&&... args) {
        // Calling the kernel and no need to capture void.
        kernel.template call<void, Args...>(
            op, dispatchKeySet, forward<Args>(args)...);
      }
      Stack getOutputs() {
        return Stack();
      }
      void release() && {}
    };

    } // namespace detail

    // See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&
    template<class Return, class... Args>
    inline Return Dispatcher::callWithDispatchKeySlowPath(const TypedOperatorHandle<Return(Args...)>& op, bool pre_sampled, DispatchKeySet dispatchKeySet, const KernelFunction& kernel, Args... args) {
        // Check if we need to run callbacks registered with RecordFunction
        // If true and callbacks need inputs, we box the arguments and pass
        // them into the callbacks and also into the kernel call

        // Note: for perf reasons we wouldn't want to pass arguments into
        // the function call or prematurely box them
      RecordFunction guard(RecordScope::FUNCTION, pre_sampled);
      if (C10_UNLIKELY(guard.isActive())) {
        auto dispatchKey = dispatchKeySet.highestPriorityTypeId();
        if (op.operatorDef_->op.isObserved()) {
          if (guard.needsInputs()) {
            runRecordFunction(guard, op, dispatchKey, boxArgs(args...));
          } else {
            runRecordFunction(guard, op, dispatchKey);
          }
          if (C10_UNLIKELY(guard.needsOutputs())) {
            // Calls the kernel and capture the output temporarily to pass to
            // RecordFunction.
            CaptureKernelCall<Return> captureKernelCall(
                kernel, op, dispatchKeySet, forward<Args>(args)...);
            guard.setOutputs(captureKernelCall.getOutputs());
            // Releases the captured output to return to caller.
            return move(captureKernelCall).release();
          }
        }
      }
      // keeping the guard alive while executing the kernel
      return kernel.template call<Return, Args...>(op, dispatchKeySet, forward<Args>(args)...);
    }

    // See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&
    template<class Return, class... Args>
    C10_DISPATCHER_INLINE_UNLESS_MOBILE Return Dispatcher::call(const TypedOperatorHandle<Return(Args...)>& op, Args... args) const {
      unused_arg_(args...);  // workaround for a false-positive warning about unused parameters in gcc 5
      auto dispatchKeySet = op.operatorDef_->op.dispatchKeyExtractor()
        .template getDispatchKeySetUnboxed<Args...>(args...);
      TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!isAliasDispatchKey(dispatchKeySet.highestPriorityTypeId()));
      const KernelFunction& kernel = op.operatorDef_->op.lookup(dispatchKeySet.highestPriorityTypeId());
    #ifndef PYTORCH_DISABLE_PER_OP_PROFILING
      // By default, when there're no high-frequency or non-sampled callbacks,
      // RecordFunction is pre-sampled as a perf optimization;
      // shouldRunRecordFunction checks whether RecordFunction should be executed,
      // and sets pre_sampled boolean argument value to whether pre-sampling was used -
      // this boolean is passed into RecordFunction to adjust the sampling rates of
      // the callbacks
      bool pre_sampled = false;
      if (C10_UNLIKELY(shouldRunRecordFunction(&pre_sampled))) {
        return callWithDispatchKeySlowPath<Return, Args...>(op, pre_sampled, dispatchKeySet, kernel, forward<Args>(args)...);
      }
    #endif  // PYTORCH_DISABLE_PER_OP_PROFILING
      return kernel.template call<Return, Args...>(op, dispatchKeySet, forward<Args>(args)...);
    }

    // See [Note: Argument forwarding in the dispatcher] for why Args doesn't use &&
    template<class Return, class... Args>
    inline Return Dispatcher::redispatch(const TypedOperatorHandle<Return (Args...)>& op, DispatchKeySet currentDispatchKeySet, Args... args) const {
      unused_arg_(args...);  // workaround for a false-positive warning about unused parameters in gcc 5
      // do not use RecordFunction on redispatch
      const KernelFunction& kernel = op.operatorDef_->op.lookup(currentDispatchKeySet.highestPriorityTypeId());
      return kernel.template call<Return, Args...>(op, currentDispatchKeySet, forward<Args>(args)...);
    }
    */
}

impl Dispatcher {
    
    #[inline] pub fn call_boxed(&self, 
        op:    &OperatorHandle,
        stack: *mut Stack)  {
        
        todo!();
        /*
            // note: this doesn't need the mutex because write operations on the list keep iterators intact.
      const auto& entry = op.operatorDef_->op;
      auto dispatchKeySet = entry.dispatchKeyExtractor().getDispatchKeySetBoxed(stack);
      const auto& kernel = entry.lookup(dispatchKeySet.highestPriorityTypeId());
    #ifndef PYTORCH_DISABLE_PER_OP_PROFILING
      bool pre_sampled = false;
      if (C10_UNLIKELY(shouldRunRecordFunction(&pre_sampled))) {
        // using already existing stack to record function execution in observers
        RecordFunction guard(RecordScope::FUNCTION, pre_sampled);
        if (C10_UNLIKELY(guard.isActive())) {
          auto dispatchKey = dispatchKeySet.highestPriorityTypeId();
          if (entry.isObserved()) {
            if (guard.needsInputs()) {
              runRecordFunction(guard, op, dispatchKey, *stack);
            } else {
              runRecordFunction(guard, op, dispatchKey);
            }
          }
        }
        // keeping the guard alive while executing the kernel
        kernel.callBoxed(op, dispatchKeySet, stack);
        // track outputs
        if (C10_UNLIKELY(
                guard.isActive() && entry.isObserved() && guard.needsOutputs())) {
          guard.setOutputs(*stack);
        }
        return;
      }
    #endif  // PYTORCH_DISABLE_PER_OP_PROFILING
      kernel.callBoxed(op, dispatchKeySet, stack);
        */
    }
    
    #[inline] pub fn redispatch_boxed(&self, 
        op:               &OperatorHandle,
        dispatch_key_set: DispatchKeySet,
        stack:            *mut Stack)  {
        
        todo!();
        /*
            // note: this doesn't need the mutex because write operations on the list keep iterators intact.
      const auto& entry = op.operatorDef_->op;
      const auto& kernel = entry.lookup(dispatchKeySet.highestPriorityTypeId());
      return kernel.callBoxed(op, dispatchKeySet, stack);
        */
    }
}

//-------------------------------------------[.cpp/pytorch/aten/src/ATen/core/dispatch/Dispatcher.cpp]

pub struct RegistrationListenerList {
    listeners: LinkedList<Box<OpRegistrationListener>>,
}

impl RegistrationListenerList {
    
    pub fn add_listener(&mut self, listener: Box<OpRegistrationListener>) -> fn() -> () {
        
        todo!();
        /*
            listeners_.push_back(move(listener));
        auto delete_it = --listeners_.end();
        return [this, delete_it] {
            listeners_.erase(delete_it);
        };
        */
    }
    
    pub fn call_on_operator_registered(&mut self, op: &OperatorHandle)  {
        
        todo!();
        /*
            for (auto& listener : listeners_) {
          listener->onOperatorRegistered(op);
        }
        */
    }
    
    pub fn call_on_operator_deregistered(&mut self, op: &OperatorHandle)  {
        
        todo!();
        /*
            for (auto& listener : listeners_) {
          listener->onOperatorDeregistered(op);
        }
        */
    }
}

impl Dispatcher {
    
    pub fn new() -> Self {
    
        todo!();
        /*


            : operators_()
    , operatorLookupTable_()
    , backendFallbackKernels_()
    , listeners_(make_unique<RegistrationListenerList>())
    , mutex_()
        */
    }
    
    pub fn real_singleton(&mut self) -> &mut Dispatcher {
        
        todo!();
        /*
            static Dispatcher _singleton;
      return _singleton;
        */
    }
    
    pub fn find_op(&mut self, overload_name: &OperatorName) -> Option<OperatorHandle> {
        
        todo!();
        /*
            return operatorLookupTable_.read([&] (const ska::flat_hash_map<OperatorName, OperatorHandle>& operatorLookupTable) -> optional<OperatorHandle> {
        auto found = operatorLookupTable.find(overload_name);
        if (found == operatorLookupTable.end()) {
          return nullopt;
        }
        return found->second;
      });
        */
    }
    
    pub fn find_schema(&mut self, overload_name: &OperatorName) -> Option<OperatorHandle> {
        
        todo!();
        /*
            auto it = findOp(overload_name);
      if (it.has_value()) {
        if (it->hasSchema()) {
          return it;
        } else {
          return nullopt;
        }
      } else {
        return it;
      }
        */
    }
    
    pub fn find_schema_or_throw(&mut self, 
        name:          *const u8,
        overload_name: *const u8) -> OperatorHandle {
        
        todo!();
        /*
            auto it = findSchema({name, overload_name});
      if (!it.has_value()) {
        // Check if we have ANYTHING; if that's the case, that means you're
        // missing schema
        auto it2 = findOp({name, overload_name});
        if (!it2.has_value()) {
          TORCH_CHECK(false, "Could not find schema for ", name, ".", overload_name);
        } else {
          TORCH_CHECK(false, "Could not find schema for ", name, ".", overload_name,
            " but we found an implementation; did you forget to def() the operator?");
        }
      }
      return it.value();
        */
    }
    
    pub fn get_all_op_names(&mut self) -> Vec<OperatorName> {
        
        todo!();
        /*
            return operatorLookupTable_.read([&] (const ska::flat_hash_map<OperatorName, OperatorHandle>& operatorLookupTable) -> vector<OperatorName> {
        vector<OperatorName> allOpNames;
        for (const auto& op : operatorLookupTable) {
            allOpNames.push_back(op.first);
        }
        return allOpNames;
      });
        */
    }
    
    /**
      | Postcondition: caller is responsible
      | for disposing of registration when
      | they are done
      |
      */
    pub fn find_or_register_name(&mut self, op_name: &OperatorName) -> OperatorHandle {
        
        todo!();
        /*
            const auto found = findOp(op_name);
      if (found != nullopt) {
        return *found;
      }

      operators_.emplace_back(OperatorName(op_name));
      OperatorHandle handle(--operators_.end());
      operatorLookupTable_.write([&] (ska::flat_hash_map<OperatorName, OperatorHandle>& operatorLookupTable) {
        operatorLookupTable.emplace(op_name, handle);
      });

      return handle;
        */
    }
    
    pub fn register_library(&mut self, 
        ns:    String,
        debug: String) -> RegistrationHandleRAII {
        
        todo!();
        /*
            lock_guard<mutex> lock(mutex_);
      auto found = libraries_.find(ns);
      TORCH_CHECK(
        found == libraries_.end(),
        "Only a single TORCH_LIBRARY can be used to register the namespace ", ns,
        "; please put all of your definitions in a single TORCH_LIBRARY block.  "
        "If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL "
        "(which can be duplicated).  If you really intended to define operators for a "
        "single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to "
        "explicitly indicate this.  "
        "Previous registration of TORCH_LIBRARY was ",
        found->second, "; latest registration was ", debug
      );
      libraries_.emplace(ns, move(debug));
      return RegistrationHandleRAII([this, ns] {
        deregisterLibrary_(ns);
      });
        */
    }
    
    pub fn deregister_library(&mut self, ns: &String)  {
        
        todo!();
        /*
            // we need a lock to avoid concurrent writes
      lock_guard<mutex> lock(mutex_);
      libraries_.erase(ns);
        */
    }
    
    pub fn register_def(&mut self, 
        schema: FunctionSchema,
        debug:  String) -> RegistrationHandleRAII {
        
        todo!();
        /*
            // we need a lock to avoid concurrent writes
      lock_guard<mutex> lock(mutex_);

      OperatorName op_name = schema.operator_name();
      auto op = findOrRegisterName_(op_name);

      TORCH_CHECK(op.operatorDef_->def_count == 0, "Tried to register an operator (", schema, ") with the same name and overload name multiple times.",
                                                        " Each overload's schema should only be registered with a single call to def().",
                                                        " Duplicate registration: ", debug, ". Original registration: ", op.operatorDef_->op.debug());
      op.operatorDef_->op.registerSchema(move(schema), move(debug));
      listeners_->callOnOperatorRegistered(op);

      // NB: do not increment the counts until AFTER error checking
      ++op.operatorDef_->def_count;
      ++op.operatorDef_->def_and_impl_count;

      return RegistrationHandleRAII([this, op, op_name] {
        deregisterDef_(op, op_name);
      });
        */
    }
    
    pub fn deregister_def(&mut self, 
        op:      &OperatorHandle,
        op_name: &OperatorName)  {
        
        todo!();
        /*
            // we need a lock to avoid concurrent writes
      lock_guard<mutex> lock(mutex_);

      TORCH_INTERNAL_ASSERT(op.schema().operator_name() == op_name);

      // reduce def_count and actually deregister if no references left
      TORCH_INTERNAL_ASSERT(op.operatorDef_->def_count > 0);
      TORCH_INTERNAL_ASSERT(op.operatorDef_->def_and_impl_count > 0);

      --op.operatorDef_->def_count;
      --op.operatorDef_->def_and_impl_count;
      if (0 == op.operatorDef_->def_count) {
        // note: call listeners *before* operator is removed, i.e. dispatcher is still valid for removed op
        // TODO: check that listeners are not relying on prepareForDeregistration()
        // invariant
        listeners_->callOnOperatorDeregistered(op);
        op.operatorDef_->op.deregisterSchema();
      }

      cleanup(op, op_name);
        */
    }
    
    pub fn register_impl(&mut self, 
        op_name:                  OperatorName,
        dispatch_key:             Option<DispatchKey>,
        kernel:                   KernelFunction,
        cpp_signature:            Option<CppSignature>,
        inferred_function_schema: Box<FunctionSchema>,
        debug:                    String) -> RegistrationHandleRAII {
        
        todo!();
        /*
            lock_guard<mutex> lock(mutex_);

      auto op = findOrRegisterName_(op_name);

      auto handle = op.operatorDef_->op.registerKernel(
        *this,
        dispatch_key,
        move(kernel),
        move(cpp_signature),
        move(inferred_function_schema),
        move(debug)
      );

      ++op.operatorDef_->def_and_impl_count;

      return RegistrationHandleRAII([this, op, op_name, dispatch_key, handle] {
        deregisterImpl_(op, op_name, dispatch_key, handle);
      });
        */
    }
    
    pub fn deregister_impl(&mut self, 
        op:           &OperatorHandle,
        op_name:      &OperatorName,
        dispatch_key: Option<DispatchKey>,
        handle:       AnnotatedKernelIterator)  {
        
        todo!();
        /*
            lock_guard<mutex> lock(mutex_);

      op.operatorDef_->op.deregisterKernel_(*this, dispatch_key, handle);

      TORCH_INTERNAL_ASSERT(op.operator_name() == op_name);

      TORCH_INTERNAL_ASSERT(op.operatorDef_->def_and_impl_count > 0);
      --op.operatorDef_->def_and_impl_count;

      cleanup(op, op_name);
        */
    }
    
    pub fn register_name(&mut self, op_name: OperatorName) -> RegistrationHandleRAII {
        
        todo!();
        /*
            lock_guard<mutex> lock(mutex_);
      auto op = findOrRegisterName_(op_name);
      ++op.operatorDef_->def_and_impl_count;
      return RegistrationHandleRAII(
          [this, op, op_name] { deregisterName_(op, op_name); });
        */
    }
    
    pub fn deregister_name(&mut self, 
        op:      &OperatorHandle,
        op_name: &OperatorName)  {
        
        todo!();
        /*
            lock_guard<mutex> lock(mutex_);
      TORCH_INTERNAL_ASSERT(op.operator_name() == op_name);
      TORCH_INTERNAL_ASSERT(op.operatorDef_->def_and_impl_count > 0);
      --op.operatorDef_->def_and_impl_count;
      cleanup(op, op_name);
        */
    }
    
    /**
      | Test if the operator entry is completely
      | dead, and if so remove it completely
      |
      */
    pub fn cleanup(&mut self, 
        op:      &OperatorHandle,
        op_name: &OperatorName)  {
        
        todo!();
        /*
            if (0 == op.operatorDef_->def_and_impl_count) {
        // NOTE: Making this call fast is the only reason OperatorHandle
        // stores operatorIterator_!
        operators_.erase(op.operatorIterator_);
        operatorLookupTable_.write([&] (ska::flat_hash_map<OperatorName, OperatorHandle>& operatorLookupTable) {
          operatorLookupTable.erase(op_name);
        });
      }
        */
    }
    
    pub fn register_fallback(&mut self, 
        dispatch_key: DispatchKey,
        kernel:       KernelFunction,
        debug:        String) -> RegistrationHandleRAII {
        
        todo!();
        /*
            lock_guard<mutex> lock(mutex_);

      TORCH_CHECK(
        !backendFallbackKernels_[static_cast<u8>(dispatchKey)].kernel.isValid(),
        "Tried to register multiple backend fallbacks for the same dispatch key ", dispatchKey, "; previous registration ",
        backendFallbackKernels_[static_cast<u8>(dispatchKey)].debug, ", new registration ", debug
      );
      // NB: inferred function schema is always nullptr for fallbacks, as fallbacks
      // cannot be unobxed
      backendFallbackKernels_[static_cast<u8>(dispatchKey)] = AnnotatedKernel(move(kernel), nullptr, move(debug));

      for (auto& op : operators_) {
        op.op.updateFallback(*this, dispatchKey);
      }

      return RegistrationHandleRAII([this, dispatchKey] {
        deregisterFallback_(dispatchKey);
      });
        */
    }
    
    pub fn deregister_fallback(&mut self, dispatch_key: DispatchKey)  {
        
        todo!();
        /*
            lock_guard<mutex> lock(mutex_);

      backendFallbackKernels_[static_cast<u8>(dispatchKey)] = {};

      for (auto& op : operators_) {
        op.op.updateFallback(*this, dispatchKey);
      }
        */
    }
    
    pub fn add_registration_listener(&mut self, listener: Box<OpRegistrationListener>) -> RegistrationHandleRAII {
        
        todo!();
        /*
            lock_guard<mutex> lock(mutex_);

      for (auto iter = operators_.begin(); iter != operators_.end(); ++iter) {
        if (iter->def_count > 0) {
          listener->onOperatorRegistered(OperatorHandle(iter));
        }
      }

      auto removeListener = listeners_->addListener(move(listener));
      return RegistrationHandleRAII([this, removeListener] {
          lock_guard<mutex> lock(mutex_);
          removeListener();
      });
        */
    }
    
    pub fn check_invariants(&self)  {
        
        todo!();
        /*
            for (const auto& op : operators_) {
        op.op.checkInvariants();
      }
        */
    }
    
    pub fn find_dangling_impls(&self) -> Vec<OperatorHandle> {
        
        todo!();
        /*
            return operatorLookupTable_.read([&] (const ska::flat_hash_map<OperatorName, OperatorHandle>& operatorLookupTable) -> vector<OperatorHandle> {
        vector<OperatorHandle> opsWithDanglingImpls;
        for (const auto& op : operatorLookupTable) {
          if (!op.second.hasSchema()) {
            opsWithDanglingImpls.push_back(op.second);
          }
        }
        return opsWithDanglingImpls;
      });
        */
    }
    
    pub fn sequence_number_for_running_record_function(&mut self, dispatch_key: DispatchKey) -> i64 {
        
        todo!();
        /*
            i64 seq_num = -1;
      // Setting sequence number in the Autograd case to associate
      // the forward range with the coresponding Autograd's node
      if (isIncludedInAlias(dispatchKey, DispatchKey::Autograd) && GradMode::is_enabled()) {
        seq_num = sequence_number::peek();
      }
      return seq_num;
        */
    }
    
    pub fn run_record_function(&mut self, 
        guard:        &mut RecordFunction,
        op:           &OperatorHandle,
        dispatch_key: DispatchKey,
        stack:        &TorchJitStack)  {
        
        todo!();
        /*
            guard.before(op, stack, sequenceNumberForRunningRecordFunction(dispatchKey));
        */
    }
    
    pub fn run_record_function(&mut self, 
        guard:        &mut RecordFunction,
        op:           &OperatorHandle,
        dispatch_key: DispatchKey,
        stack:        TorchJitStack)  {
        
        todo!();
        /*
            guard.before(op, move(stack), sequenceNumberForRunningRecordFunction(dispatchKey));
        */
    }
    
    pub fn run_record_function(&mut self, 
        guard:        &mut RecordFunction,
        op:           &OperatorHandle,
        dispatch_key: DispatchKey)  {
        
        todo!();
        /*
            // Setting sequence number in the Autograd case to associate
      // the forward range with the coresponding Autograd's node
      guard.before(op, sequenceNumberForRunningRecordFunction(dispatchKey));
        */
    }
}
